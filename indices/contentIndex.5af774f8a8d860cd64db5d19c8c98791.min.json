{"/":{"title":"Welcome!","content":"\n## Introduction\n\nHi there! Seems like you stumbled upon my archive of course notes! \n\nOver the years, I've found that the most effective way for me to learn and process information was to teach it. Since this isn't always practical or possible, I've ended up creating these production-quality notes as an alternative.\n\nAlthough many courses at Berkeley (especially CS courses) have excellent materials and often already have a full set of course notes, I've found that many students- myself included- often struggle to process information when it's that dense. My hope is that these notes can serve as a secondary, lighter perspective on things. \n\nHere, you'll find a wide variety of content from basic concepts, practice problems, and example algorithm walkthroughs. Since they were all made at various times over 4 years, the quality and style may be wildly different from page to page. I intend to go through these notes and resolve any inconsistencies over (a long period of) time.\n\n## Index\n\nHere are the courses that I currently have notes available for, and their statuses:\n\n - [CS 61B: Data Structures and Algorithms](cs61b/): full guide available for all course content, based on the Spring 2020 offering\n\n\nHere are courses that I am currently working on publishing notes for:\n - CS 70: Discrete Math notes available, Probability notes are nonexistent due to how good the Data 140 textbook is\n - CS 61A: creating an incomplete guide (will not cover all course content, since)\n - CS 186: currently under review\n - Data 102: complete, needs review\n - CS 168: complete, needs review\n - CS 188: mostly complete, needs review\n - CS 162: mostly complete, needs review\n - MCB C61: textbook companion available, needs introduction\n\nHere are some future notes that I may work on at some point:\n - Comprehensive psychology major notes (Psych 101, 114, 124, C126, C127, 131, 140, C143, 150)\n - Other CS courses that I have incomplete notes for (61C, 170, 161)\n\n## Basic Principles\n\nHere are some principles that I try to follow when creating notes. I'll probably make a blog post at some point to go over this in more detail, but for now this outline should be enough to show what I hope to accomplish.\n\n1. **Content is more fun when it's important:** Answer the question \"why should I care about this?\" before actually spending time on whatever topic is at hand. If answering it is a struggle, then it's probably not important enough to need to remember in the future.\n2. **Make it interactive:** It's way easier to concentrate on something if it's directly applicable to a problem, question, or situation at hand. Interject conceptual notes with illustrated examples and practice problems whenever possible.\n3. **Notes are rarely self-contained:** It's impossible to fully cover most topics on a single page, and topics may be deeply related to content from other courses. Link to external resources or further learning opportunities whenever possible, just in case it becomes necessary to research the topic further in the future.\n4. **Type a lot of stuff really fast:** For this verbose style of note-taking to be effective for me, I need to be able to completely put down thoughts on the page before I lose them. If you're thinking of doing this on your own, I'd recommend getting good at touch typing, and hitting up [monkeytype](https://monkeytype.com/) for some practice. I'm going against all the research that suggests handwriting is more effective than typing, because the purpose of my notes is not for memorizing or even remembering any of the content, but rather to create a complete repository of knowledge that I and others can easily search in the future.\n\n## About this website\n\nMy notes are hosted on [Netlify](https://www.netlify.com/) and are built on my custom [Amethyst theme](https://github.com/64bitpandas/amethyst) for [Hugo](https://https://gohugo.io/). You can view the source code [here](https://github.com/64bitpandas/notes).\n\nAll of the notes here are formatted in Markdown, and the majority was created using [Obsidian](https://obsidian.md/). These notes are a small fraction of my Obsidian vault; I intend to publish other small bits of it in various places such as my [blog](https://blog.bencuan.me), [devlog](https://devlog.bencuan.me), or [mastodon](https://hachyderm.io/@bencuan) if you're curious.\n\nIf you're interested in contributing, take a look at the [contribution guide](/contributing.md).\n\n## Contact me\n\nWant to chat with me about these notes, or something else? You can find my contact info [here](https://bencuan.me/contact).","lastmodified":"2023-01-08T06:32:50.399928165Z","tags":null},"/contributing":{"title":"Contributing","content":"\nThanks for your interest in contributing to my notes! There's a lot of room for improvement, and I don't have the time to fix everything. If there's something that you'd like to add, please do so!\n\n## Making Requests\n\nI use GitHub for issue tracking. Please make an [issue](https://github.com/64bitpandas/notes/issues) before editing anything or creating a pull request, so I can comment on it before you start working on a feature.\n\n## Editing Content\n\nThis website is built with [Hugo](https://gohugo.io/) using the [Amethyst theme](https://amethyst.bencuan.me/). \n\nFirst, fork the main repository (github.com/64bitpandas/notes).\n\nInstructions on how to format new files and set up a live development server can be found [here](https://amethyst.bencuan.me/setup/editing/). \nA guide on how to use the various features (tabs, callouts, links...) can also be found on the Amethyst website.\n\nIf you have something you want to contribute but it's in another format (like a Google Doc), and you are unable to convert it yourself, create an issue linking the unformatted content and I will take a look.\n\n## Attributions and Academic Integrity\n\nAlthough some of my older notes may be missing citations or attributions to the content they reference, I'm doing my best to link back to the original source moving forwards. If you use public course content (such as screenshots from slides, code snippets from a project skeleton, or practice problems from a past exam), please add a link to where you found it!\n\n**Do not under any circumstances publish private course content** (that can't be found elsewhere for free online, from an official source). This includes homework/project/discussion solutions, textbooks, and readers.\n\nIf you are an instructor or TA for a course that I have notes for and find something that shouldn't be published, please send me an email at [contact@bencuan.me](mailto:contact@bencuan.me) so I can remove it and scrub it from the git history.\n\n\n## Credits\nHere's a list of individuals who have made meaningful contributions to these notes over the years! If you make a pull request, feel free to add your name here as well.\n - [Arin Chang](https://github.com/arinchang)\n - [Zachary Zollman](https://github.com/zacharyzollman)","lastmodified":"2023-01-08T06:32:50.399928165Z","tags":null},"/cs186/":{"title":"Welcome to CS186!","content":"\n# Welcome to my CS186 Guide!\n\nThis is a **non-comprehensive** guide to databases written with an intention to supplement learning and reviewing of Berkeley's [CS186](https://cs186berkeley.net) material. Main topics include:\n\n* [SQL syntax](\u003ccs186/00 SQL Basics\u003e)\n* [How to improve popular sorting and hashing algorithms to work well with limited memory](\u003ccs186/04 Sorting and Hashing\u003e)\n* [B+ trees](02%20B+%20Trees.md B+ Trees\u003e) and other advanced indexing structures\n* [Join algorithms](\u003ccs186/05 Iterators and Joins\u003e)\n* [Query optimization](\u003ccs186/07 Query Optimization\u003e)\n* [Parallel query processing](\u003ccs186/09 Parallel Query Processing\u003e)\n* [Crash Recovery (AERIES)](\u003ccs186/10 Recovery\u003e)\n* [Database transactions and Concurrency](\u003ccs186/08 Transactions\u003e)\n* [Entity-Relationship Diagrams](\u003ccs186/12 ER Diagrams\u003e)\n\nI recognize that the course notes for 186 can be *very* dense sometimes, and don't cover 100% of the information you need to do well on projects and exams. While this also doesn't covery everything, I try to focus on content that the notes don't.\n\n**This isn't a replacement for lectures and other course content.** You probably need to look at those first, and come here if something isn't sticking!\n\n\u003e [!important] Please read this first!\n\u003e \n\u003e [What is an I/O and why should I care?](io)\n\n## Disclaimer\n\nAlthough I am a 186 TA, these notes are not official course content. They have not been reviewed or approved by an instructor, and there may be inaccurate or missing information. Please don't bother the other course staff with questions about the content here- contact me instead (email or office hours).\n\n## Concept Maps\n\n### Database Implementation\n![implementation](concept-implementation.png)\n\n### Database Design\n![design](concept-design.png)\n\n### ACID\n![acid](concept-acid.png)\n\n## Prerequisites\n\nCS 186 projects are done in Java. Knowledge of [CS61B](/cs61b) concepts are assumed. Specifically, you should understand:\n - OOP fundamentals such as classes, inheritance, and objects and implement them in Java\n - Binary trees, their runtime proprties, and implementation of efficient search and insert algorithms\n - Basic hashing and sorting algorithms\n - Use an IDE (preferably IntelliJ or VSCode) and its debugger to step through code and create breakpoints\n\nIn addition, knowledge from the last part of [CS61C](https://cs61c.org) is assumed and will be very useful for the first part of 186. This includes:\n - Knowing how computers store memory, the different types of memory (disk, RAM, cache), and why we have them\n - How data is stored on disk (files, pages, records)\n\nUnsure about prerequisite content? You can review my [CS61B notes](/cs61b) if needed. I'll cover the main points from 61C at the start of [Disks, Buffers, Files](cs186/01%20Disks,%20Buffers,%20Files.md).\n\n## How to contribute\n\nSee the [contributing guide](/contributing) for more details!\n\nTwo particular additions that need to be made are entries for Functional Dependencies and NoSQL. I don't have notes written for this since these topics were not covered when I took the course.\n\n\n\n","lastmodified":"2023-01-08T06:32:50.431928902Z","tags":null},"/cs186/00-SQL-Basics":{"title":"SQL Basics","content":"## Relevant Materials\n - [Note 1](https://notes.bencuan.me/cs186/coursenotes/n01-SQLPart1.pdf)\n - [Note 2](https://notes.bencuan.me/cs186/coursenotes/n02-SQLPart2.pdf)\n - [Discussion 1](https://docs.google.com/presentation/d/1PZ7R8iKSm3gHUapi9l-WAlv_TWKQ-1VvAF98ZnEHW-o/edit)\n\n## What is SQL?\n\n**S**tructured **Q**uery **L**anguage (/ˈsiːkwəl/) is a highly standardized syntax for performing operations on a **database**.\n\n### Terminology\nSQL databases are a set of named **relations**, or tables, that describe the relationship between **attributes**. You can think of attributes as the columns of the table, and each record (also known as a **tuple**) being one row in the table.\n\nRelations are composed of:\n - A **schema**, which is the description of the table. Schemas are fixed, with unique attribute names and atomic types (integer, text, etc.).\n - The **instance**, which is the set of data that satisfy the schema. Instances can frequently change (whenever a row is added or updated), as long as any additions are consistent with the schema.\n\nAs an example for using this terminology: if we add a column to the table, we can say that \"we updated the schema of the relation to include an additional attribute\".\n\n### A note on looking up SQL things online\nThe SQL syntax and features we cover in this course are known as \"standard SQL\", whose specifications can be found [here](https://blog.ansi.org/2018/10/sql-standard-iso-iec-9075-2016-ansi-x3-135/). \n\nIn the wild, there are many implementations of SQL (like SQLite, MySQL, MariaDB), which may have an extended feature set or slightly different syntax. We will generally stay away from these extended features, and you do not need to know them for now.\n\nIf you're struggling to find relevant information, I would recommend prepending \"sqlite\" to the front of your search query (e.g. \"sqlite how to create a table\"). We use SQLite in Project 1, and for most things it's safe to assume that the syntax will be what we're looking for. Read more about SQL vs SQLite [here](https://cs186.gitbook.io/project/assignments/proj1/sql-vs-sqlite).\n\n### Running SQL queries\nYou will install sqlite3 in [Project 1](https://cs186.gitbook.io/project/assignments/proj1/getting-started). Once you've installed it, you can run `sqlite3` in your terminal to start an interactive session, or run `sqlite3 database.db` to read from a file named `database.db`.  Once in the session, you can also run `.read file.sql` to run queries written in the file `file.sql`, or run `.schema` to get schema information for the current database.\n\nIn a pinch, the [CS61A online interpreter](https://code.cs61a.org/) also works great, and has a built-in visualizer that's especially useful for exploring aggregation.\n\n### Pedagogy note\nDon't get too hung up on the syntax, or remembering how to use every small feature of SQL. The language itself is only a small part of this course- for most of our time, we'll discuss how to actually *implement* the features you use here.\n\nIf you anticipate needing SQL in future work, [here's a nice reference](https://www.w3schools.com/sql/sql_ref_keywords.asp) for common keywords and how to use them.\n\n## Keys                                                                                                \n\nWhen defining schemas, it's often important to be able to guarantee that rows are unique so that we can catch duplicate information. For example, if the school had a database of all enrolled students, we wouldn't want two students to have the same SID!\n\n### Primary Keys\n\nPrimary keys are unique, non-null, and can be used to identify an entry. Many times, adding `PRIMARY KEY (id)` during [table creation](#Create%20Table) is good enough.\n\nIn other cases, like the boat reservation tracking example in lecture, we would need more than one attribute (`PRIMARY KEY (sid, bid, day)`) to guarantee uniqueness, since a boat can be reserved many different times.\n\n### Foreign Keys\n\nForeign keys reference other tables' primary keys. Building onto the boat example, the boat ID would be the same as the ID’s in a table of boats, so we could add `FOREIGN KEY (bid) REFERENCES Boats` to the reserves table instead of copying everything over.\n\nThe main purpose of foreign keys is to maintain the uniqueness and non-null constraint of an attribute, since it *has* to match the primary key of another table.\n\nBelow is a screenshot from lecture that puts some code to the example.\n![lecture screenshot](SQL%20Basics/Untitled.png)\n\n## Writing Queries\n\nEnough with the long complicated words, let's write some SQL!\n\n### Create Table\nFirst, let's create a table:\n```sql\nCREATE TABLE clubs(\n\tname TEXT,\n\talias TEXT,\n\tmembers INTEGER,\n\tPRIMARY KEY (name)\n) AS\nSELECT \"Computer Science Mentors\", \"CSM\", 500 UNION\nSELECT \"Open Computing Facility\", \"OCF\", 50;\n```\n\n[Here's a good list of common data types that you can use.](https://www.digitalocean.com/community/tutorials/sql-data-types) For the purposes of this class we will mostly focus on INT, BOOLEAN, TEXT, CHAR(n), VARCHAR(n), and BYTE. More about what these do in the [next section](01%20Disks,%20Buffers,%20Files.md).\n\n### Insert Values\nThere's more clubs to add! Let's add one after the table has already been created:\n```sql\nINSERT INTO clubs VALUES\n\t(\"Eta Kappa Nu\", \"HKN\", 100),\n\t(\"Computer Science Association\", \"CSUA\", 200);\n```\n\n\n### Basic Querying\nBelow is the basic structure of a query:\n```sql\nSELECT name AS clubname\nFROM clubs\nWHERE members \u003e= 100 AND members \u003c= 300 \nORDER BY clubname\nLIMIT 3;\n```\nHere, we:\n - SELECT the column `name` FROM the table `clubs`, and rename it to `clubname`;\n - Keep only the clubs WHERE the number of members is between 100 and 300;\n - Sort the entries by name (ascending by default),\n - and keep only the top 3 entries (by alphabetical order).\n\n### Aggregation\nAggregation can seem tricky, but the core idea is simple: **crunch similar rows into one row, and keep one particularly interersting value.**\n\nThere are three parts to this:\n1. **GROUP BY:** Specify the column containing the similar values. All rows with the same value in this column will be combined into one row. \n2. **HAVING:** Specify how you want to filter (this is optional.) The syntax is basically the same as `WHERE`, except instead of a column name, we use an aggregate on a column name (such as `MAX(members)` or `COUNT(*)`).\n3. Modifications to **SELECT:** Ensure that all of the selected columns (except the one(s) passed into GROUP BY) are aggregates (MAX, MIN, COUNT, SUM...). \n\nIt's possible to GROUP BY multiple columns. This will group together every combination of values in those two columns. For example, if we did `GROUP BY name, alias`, and two clubs `Open Computing Facility` and `Original Cat Friends` had the same alias `OCF`, they would represent two separate groups.\n\n{{\u003c tabs \"qc\" \u003e}}\n{{\u003c tab \"Quick Check\" \u003e}}\nWhat is the difference between `WHERE` and `HAVING`? \n{{\u003c /tab \u003e}}\n{{\u003c tab \"Answer\" \u003e}}\nIn short, `WHERE` operates on individual rows, and `HAVING` operates on groups. \n\nWhenever you want to do something that requires the `GROUP BY` to have been done first, like filter by `MAX(members) \u003e 100`, it needs to be in the `HAVING` clause.\n{{\u003c /tab \u003e}}\n{{\u003c /tabs \u003e}}\n\n### Practice Problems\n\nStill unsure about querying and aggregation? [Here are some of my old 61A discussion slides](\u003chttps://notes.bencuan.me/cs186/coursenotes/61A%20Discussion%2012.pdf\u003e) that have some practice problems (back when we still taught SQL). All of the tables referenced are already preloaded for you in [code.cs61a.org](https://code.cs61a.org).\n\n\n## Logical Processing Order\n\nSQL Queries are typically processed in a different order than they're written. Here's the order- try to develop an intuition as to why this order would make more sense to a machine than how queries are usually written:\n\n1. `FROM` (find the table that is being referenced, join if needed)\n2. `WHERE` (filters out rows)\n3. `GROUP BY` (aggregate)\n4. `HAVING` (filters out groups)\n5. `SELECT` (choose columns)\n6. `ORDER BY` (sort)\n7. `LIMIT` (cut off the output)\n\nWhen writing queries, I often like to follow this order as well since each step builds on the previous one.\n\n### A note on aliasing\nOne consequence of Logical Processing Order is that **we cannot use aliases in WHERE, GROUP BY, or HAVING** because they are processed before any alias is defined in SELECT!\n\nFor example, `SELECT name as clubname WHERE clubname = 'Open Computing Facility'` is NOT a valid query in standard SQL.\n\nHowever, since ORDER BY and LIMIT come afterwards, we are allowed to use aliases there.\n\n\n## Joins\n![Untitled](SQL%20Basics/Untitled%201.png)\nIf we have two tables and need to access information from both in a query, we will need to join the two tables together!\n\nFor this section, we will use the following tables as examples:\n![](SQL%20Basics/Pasted%20image%2020230107122706.png)\n\n### Cartesian Product\nBy default, if a join (`SELECT ... FROM a, b...`) is done in SQL without specifying a type, a **cross product** (Cartesian product) is calculated. Every row in table `a` (the left table) is combined with every row in table `b` to create $R_a * R_b$ rows ($R_a$ = number of rows in table `a`). \n\nIn the example below, since `clubs` had $2$ rows and `members` had $4$ rows, we should expect the result to have $3 \\times 4 = 12$ rows. Note that most of these rows are pretty useless, since there is no correlation between the member and the club they were joined with.\n![|500](SQL%20Basics/Pasted%20image%2020230107122726.png)\n\n### Inner Join\n\nAn inner join takes only the rows in which a particular attribute (or list of attributes) can be found in both the left and right tables. \n\n- For example, an inner join on the `sid` attribute could be written out as `SELECT ... FROM a, b WHERE a.sid = b.sid`.\n- Inner join is the **default behavior** for the JOIN operation, which looks like this:\n    \n    ```sql\n    SELECT ...\n    FROM a INNER JOIN b\n    ON a.sid = b.sid\n    ...\n    ```\n\nIn the example below, we only keep the clubs CSM and OCF, and only keep the members that are in those two clubs:\n![](SQL%20Basics/Pasted%20image%2020230107122849.png)\n\n### Natural Join\n\nA `NATURAL JOIN` automatically inner joins tables on whichever attributes share the same name between the two tables.\n\nIf the columns `alias` and `club` were both named the same thing, then `SELECT ... FROM a NATURAL JOIN b ...` is completely equivalent to the example in the inner join section above. Otherwise, the NATURAL JOIN will return an empty table if no column names are shared (even if the contents are the same).\n\n### Outer Join\n\nLeft outer joins return all matched rows (as in an inner join), AND additionally preserves all unmatched rows from the left table. Any non-matching fields will be filled in with null values.\n![](SQL%20Basics/Pasted%20image%2020230107123046.png)\n\nA right outer join is the same as the left outer join, except it preserves unmatched rows from the right table instead. Flipping the table order on a left outer join creates an equivalent right outer join:\n![](SQL%20Basics/Pasted%20image%2020230107123142.png)\n\nA full outer join returns all rows, matched or unmatched, from the tables on both sides of the join clause. \n\n\n## Advanced Mechanics\n\n### String Comparisons\n\nUsing the `LIKE` operator, we can do the following:\n\n- Match any single character: `_`\n- Match zero, one. or multiple characters: `%`\n- Example: `WHERE name LIKE 'B_%'` will match all rows with a name starting with a B and are at least 2 characters long\n\n### Unions and Intersections\n\nThe `UNION` operator combines two queries (like an OR statement).\n\nThe `INTERSECT` operator combines two queries, and discards rows that do not appear in both (like an AND statement).\n\nThe `EXCEPT` operator subtracts one query’s results from another.\n\nUNION, INTERSECT, and EXCEPT operate using **set semantics** (distinct elements) and will keep only one of each unique element in the result.\n\nUsing UNION ALL, INTERSECT ALL, and EXCEPT ALL will manage **cardinalities** and will add or subtract the number of identical elements accordingly.\n\n- UNION ALL = sum of cardinalities\n- INTERSECT ALL = minimum of cardinalities\n- EXCEPT ALL = difference of cardinalities\n\n### Nested Queries\n\nThe  `IN` operator allows subqueries to be made. For example, we can `SELECT ... FROM ... WHERE id IN (SELECT .....)`\n\n- The `EXISTS` keyword can be used in place of `IN` to make **correlated subqueries** where the table in the subquery interacts with the outer query.\n- `ANY` and `ALL` can be used as well: `SELECT * FROM a WHERE a.value \u003e ANY (subquery...)` would only keep rows in `a` that are bigger than the smallest value in the subquery.\n- `ALL` can be used to compute an argmax: for example, if we want a sailor with the highest rating, we can `SELECT * FROM s WHERE s.rating \u003e= ALL(SELECT s2.rating FROM s s2)`\n\n### Views\n\nA view is a **named query.** It can be thought of as a temporary table that can be accessed in future queries to make development simpler.  Unlike tables, it is not computed immediately, and results are cached when they are needed.\n\n```sql\nCREATE VIEW name\nAS SELECT ...\n```\n\nA “view on the fly” can also be created using the `WITH` keyword:\n\n```sql\nWITH name(col1, col2) AS\n(SELECT ...), \nname2(col1, col2) AS (SELECT ...),\nSELECT ...\n```\n\n","lastmodified":"2023-01-08T06:32:50.399928165Z","tags":null},"/cs186/01-Disks-Buffers-Files":{"title":"","content":"## Relevant Materials\n\n - [Note 3](https://notes.bencuan.me/cs186/coursenotes/n03-DisksFiles.pdf)\n - [Discussion 2](https://docs.google.com/presentation/d/10pKMJVZAA44ABJkIJXw9JhGd1nSQvvrf1qzEO770Ap8/edit#slide=id.g11049acc126_0_4)\n\n## Introduction\n\nNow that we've taken a look at how humans can interface with databases using [SQL](\u003c00 SQL Basics\u003e), let's jump all the way down to the bottom and lay the foundations for how we can go from individual bytes to a fully functional database!\n\nBefore reading this section, review the page [What is an I/O and why should I care?](io) To reiterate the most important points from this section,\n - An I/O occurs when a page is transferred between disk and memory (either read or written).\n - Pages are always fetched in whole: it is impossible to read/write half of a page to memory.\n - For the purposes of this class, a \"block\" and a \"page\" on disk are considered the same thing.\n\n\u003e [!summary] Summary\n\u003e \n\u003e  **Disks** are physical devices good at storing a huge amount of data.\u003cbr\u003e\n\u003e **Files** are stored on the disk and represent one table.\u003cbr\u003e\n\u003e **Databases** are collections of one or more tables.\u003cbr\u003e\n\u003e **Pages** are the basic building block of files. A file is generally made up of many pages.\u003cbr\u003e\n\u003e **Records** represent single rows in the table. Many records can be stored on the same page.\n\u003e \n\u003e When records from a database need to be accessed, they are copied from the disk to the **buffer** in memory one page at a time.\n\n## Devices\n\nHere's the hierarchy of physical memory devices that modern computers use. The main tradeoff is **price to speed:** the devices higher up on the chart (like the cache) allow for far faster accesses, but are much more expensive to produce per unit of data compared to slow devices (like hard drives).\n\n![Untitled](Disks,%20Buffers,%20Files/Untitled.png)\n\n\n### (Optional Context) The anatomy of a hard drive\n\n\u003e This section is taken from 61C. It's not required knowledge for 186, but it helps develop an intuition for what types of access patterns are faster than others on the disk. The main takeaway: disks are really slow.\n\nHard drives are magnetic disks that contain tracks of data around a cylinder. \nHDD's are generally good for sequential reading, but bad for random reads.\n\n![Untitled|400](Disks,%20Buffers,%20Files/Untitled%201.png)\n\n**Disk Latency = Queueing Time + Controller Time + Seek Time + Rotation Time + Transfer Time**\n- Queuing Time: amount of time it takes for the job to be taken off the OS queue\n- Controller Time: amount of time it takes for information to be sent to disk controller\n- Seek Time: amount of time it takes for the arm to position itself over the correct track\n- Rotation Time (rotational latency): amount of time it takes for the arm to rotate under the head (average is 1/2 a rotation)\n\n**Disk space management:**\n- provides an API to read and write pages to device\n- Organizes bytes on disk into pages\n- Provides locality for the ‘next’ sequential page\n- Abstracts filesystem and device details\n\n## Files\n\nA **Database file (DB FILE)** is a collection of pages, which each contain a colection of records. Databases can span multiple machines and files in the filesystem (we'll explore this idea more in [Distributed Transactions](\u003c11 Distributed Transactions.md\u003e).\n\nThere are two main types of files: **heap files**, which are **unordered**, and **sorted files**, in which records are sorted on a key. As you could imagine, sorted files add a significant amount of complexity in exchange for possibly faster runtimes. In general, **range selections and lookups are faster in sorted files, while insertions, deletions, and updates are faster in heap files.**\n\n### File Cost Analysis\n\nIn order to make efficient queries, we need a measure of how good or fast a query is. Knowing that queries operate on records, which are stored on pages in a file on disk, we can use the following cost model for analysis:\n- $B$ = number of data blocks (pages) in file\n- $R$ = number of records per page\n- $D$ = average time to read or write disk page (i.e. cost of one I/O)\n\nFor analysis, we will use the following assumptions:\n- We are mostly concerned about the **average** case.\n- The workload is **uniform random.**\n- Inserts and deletes operate on **single records.**\n- Equality selections will have **exactly one match.**\n- Heap files always **insert to the end of the file.**\n- Sorted files are always sorted according to search key.\n- Packed files are compacted after deletions.\n\nAs an exercise, think about what might happen to the runtime if we try to remove each of these assumptions.\n\n| Operation | Heap File | Sorted File | Explanation |\n| --- | --- | --- | --- |\n| Scan all records | $B*D$ | $B*D$ | Full scan = need to access every page in the file |\n| Equality Search | $B/2$ (average) | $\\log_2(B) * D$* | **Heap:** on average, need to go through half the file \u003cbr\u003e **Sorted**: Binary search runtime |\n| Range Search | $B*D$ | $(\\log_2B + P)*D$ | **Heap:** no guarantee on location of elements in desired range \u003cbr\u003e **Sorted:** binary search to find start of range; range is $P$ pages long |\n| Insertion | $2D$ | $(\\log_2B + B) * D$ | **Heap:** read last page, then write page \u003cbr\u003e **Sorted:** find location (binary search), then insert and shift rest of file |\n| Deletion | $(B/2 + 1) * D$ | $(\\log_2 B + B) * D$ | **Heap:** need to find page first, then write it back (hence the +1) \u003cbr\u003e **Sorted:** find location (binary search), then delete and shift rest of file  |\n\n### Heap File Implementation\nThere are two approaches to actually implementing heap files.\n\nThe first is the **linked list implementation**, where we have two linked lists: one of full data pages, and one of pages that still have free space. To insert a value into the file, we can ignore all of the full pages and just traverse the free pages, stopping at the first page that has enough free space to support the insertion.\n\n![ll](Disks,%20Buffers,%20Files/Pasted%20image%2020230107153137.png)\n\nHere's an example of a common question you'll face about files:\n{{\u003c tabs \"q1\" \u003e}}\n{{\u003c tab \"Question 1\" \u003e}}\nSuppose you have a linked list implementation illustrated in the image above (3 full pages, and 3 pages with free space). In the worst case, how many I/Os will it take to insert a record into a free page? Assume there is enough space in an existing page in the file.\n{{\u003c /tab \u003e}}\n{{\u003c tab \"Q1 Answer\" \u003e}}\n**5 I/Os.** Here's the walkthrough- each step incurs one I/O:\n1. Read the header page to find the pointer to the first free page.\n2. Read the first free page, and realize that it doesn't have enough space! Luckily, it has the pointer to the second free page in it.\n3. Read the second free page. The same thing occurs.\n4. Read the third free page. Due to the problem statement we can assume that our data will fit here! So we will update the third page to insert the new data.\n5. Write the updated page back to disk.\n{{\u003c /tab \u003e}}\n{{\u003c /tabs \u003e}}\n\nThe second type of heap file is a **page directory implementation.** Here, instead of a linked list of data pages, we'll store a linked list of header pages:\n![](Disks,%20Buffers,%20Files/Pasted%20image%2020230107153729.png)\nEach header page then contains a list of pointers to data pages, as well as a pointer to the next header page.\n\n{{\u003c tabs \"q2\" \u003e}}\n{{\u003c tab \"Question 2\" \u003e}}\nSuppose you have 5 header pages, and each header page can store pointers to 30 data pages. What's the worst case I/O cost for inserting a record? *Do not* assume that an existing data page can hold the new data, but *do* assume that not all of the header pages are full.\n{{\u003c /tab \u003e}}\n{{\u003c tab \"Q2 Answer\" \u003e}}\n**7 I/Os.** In the worst case, all data pages are full, and all header pages are also full except for the very last one. So, the following must happen:\n - Incur 5 I/Os reading each of the 5 header pages. Since the page directory implementation stores metadata about whether data pages are full or not, we don't have to actually read in the data pages.\n - Create a new data page, and write it to disk, incurring 1 I/O.\n - Update the last header page with a pointer to the new page, incurring 1 I/O.\n{{\u003c /tab \u003e}}\n{{\u003c /tabs \u003e}}\n\n### Sorted File Implementation\nDon't worry too much about this. We'll explore a better way of maintaining sorted order when we discuss index files in [B+ Trees](02%20B+%20Trees.md).\n\n## Records\n\n### Fixed vs. Variable Records\n**Fixed length records** have a constant, known length. An example is integers, which always have 4 bytes.\n- Field types are the same for all records, so just store the type information in memory. Variables can be accessed in the same location every time.\n\n**Variable length records** may change in size depending on the data that is stored. An example is text, which could be 0 or more characters long. Here's how we implement them:\n- Move all variable length fields to the end of the record:\n    ![Untitled](Disks,%20Buffers,%20Files/Untitled%204.png)\n- Create a header in the beginning to point to the end of variable length fields (compute beginning based on presence of other variables).\n\n### Data File Implementations\nSo, how do we actually store the records inside data pages?\n\nFirst, every data page needs a **page header**. This header includes metadata like free space, number of records, pointers, bitmaps, and a slot table for which parts of the file are empty.\n\nIf records are fixed length, we can pack them densely, which maximizes the amount of data we can store on each page.   \n    ![packed|300](Disks,%20Buffers,%20Files/Untitled%202.png)\n - We can easily append new records, but to delete, we would need to rearrange the records that come after the deleted record, which can get expensive.\n\nWe can also have **unpacked** fixed length records:\n\n![unpacked|300](Disks,%20Buffers,%20Files/Untitled%203.png)\n- To do this, we will:\n    - Keep a **bitmap** of free and empty slots in the header (one bit for each slot, rounded up to the nearest byte).\n    - To add, find an empty slot in the bitmap and mark it as filled.\n    - To delete, just flip the bitmap reference to 0. Don't worry about modifying the data itself, since it'll be overwritten eventually.\n\n For variable length records, we use **slotted page records**:\n-  Relocate the page header into the footer. (This will allow for the slot directory to be extended.)\n- In the footer, store pointers to free space containing the length and pointer to the next record.\n- This can be prone to fragmentation (will need to be addressed somehow).\n- This can also be used for fixed length records to handle null records\n\n### Calculating Record Size\n\nRecords consist of atomic values like ints and chars. Typically, records also include pointers (depending on implementation) as well as variable length records.\n\n![Untitled](Disks,%20Buffers,%20Files/Untitled%205.png)\n\nFor a standard record in a linked list, the following is required:\n- $N$ bytes for each variable, where $N$ is its size in the chart above\n- One 4-byte pointer in the header for every variable length record\n- If nullable, each **non-primary key** takes 1 bit in the bitmap. Make sure to round up to the nearest byte.\n- If using a slotted page implementation for variable length records, we’ll need 8 additional bytes in the header (free pointer, slot count) and 8 additional bytes in every record (record length, record pointer).\n\n**The maximum number of records that can be stored in a page is equal to the page size divided by the minimum record size, rounded down to the nearest integer.**\n\n- For slotted page, it would be the floor of (page size - 8 bytes) / (min record size + 8 bytes) due to the additional metadata needed.\n\nThe slot directory in a slotted page implementation has the following items:\n- slot count (4 bytes)\n- free space pointer (4 bytes)\n- (record pointer + record size) tuple for every record (8N bytes)\n\n## Practice Problems\n\n{{\u003c tabs \"q3\" \u003e}}\n{{\u003c tab \"Question 3\" \u003e}}\nSuppose we have the clubs table from the previous section:\n```sql\nCREATE TABLE clubs(\n\tname TEXT PRIMARY KEY,\n\talias TEXT,\n\tmembers INTEGER\n);\n```\n\nWhat is the maximum number of records that can fit on a 1 KiB (1024 byte) page, assuming all fields are not null?\n{{\u003c /tab \u003e}}\n{{\u003c tab \"Q3 Answer\" \u003e}}\n**50 records.**\n\nThe maximum number of records is achieved when each record is as small as possible. This occurs when both of the text variables have a length of 0. So the smallest record contains:\n - 4 byte pointer to `name`,\n - 4 byte pointer to `alias`,\n - 4 byte integer `members`.\n\nIn total, each minimum-size record is 12 bytes long. However, each record also requires a pointer and a record length value to be stored in the footer (4 bytes each), meaning each record effectively takes 20 bytes in the page.\n\nThe slot directory always contains the slot count and free space pointer (4+4 = 8 bytes), so let's subtract 8 bytes from 1024 to get 1016 bytes remaining for use for records.\n\nFinally, let's divide 1016 by 20 to get the number of records that can fit:\n$$\\lfloor 1016/20 \\rfloor = 50$$\n{{\u003c /tab \u003e}}\n{{\u003c /tabs \u003e}}","lastmodified":"2023-01-08T06:32:50.399928165Z","tags":null},"/cs186/02-B+-Trees":{"title":"","content":"## Relevant Materials\n - [Note 4](https://notes.bencuan.me/cs186/coursenotes/n04-B+Trees.pdf)\n - [Discussion 3](https://docs.google.com/presentation/d/1sOlfZVFGWWl4xFW4X22L8uESvvb7SJYER8PEfRy1Wcg/edit): view this for B+ tree algorithm walkthroughs!\n\n## Introduction\nB+ Trees are one type of **index**: a data structure that allows for quick lookups based on a particular key. They are very similar to binary trees, but can have more than two pointers and come with a variety of improvements.\n\n### What is an index exactly?\nWe can index a collection on any **ordered** subset of columns.\n\nIn an ordered index (such as a B+ Tree), the keys are ordered **lexicographically** by the search key columns:\n- First, order by the 1st column.\n- If there are rows with identical values in the 1st column, sort them by the 2nd column.\n- Continue until all columns are processed.\n\n#### Composite Search Keys (optional context)\n\nUsing a **composite search key** is one way to create an index on multiple columns. The composite search key on a set of columns $(k_1, \\cdots, k_n)$ matches a query if:\n- the query is a conjunction of zero or more **equality clauses:**\n    - `k1 = v1 AND k2 = v2 .. k_m = v_m`\n- and at most 1 **range clause:**\n    - either $k_{m+1} \u003c v_{m+1}$ or $k_{m+1} \u003e v_{m+1}$\n\nIntuitively, a composite search key matches a continuous range of rows in the lexicographically sorted table.\n\n## The Representation\n\n![Untitled](B+%20Trees/Untitled.png)\n\nThere are two types of nodes in a B+ Tree.\n - **Inner nodes** make up all but the last layer of the tree, and store Key-Pointer pairs that reference more nodes.\n - **Leaf nodes** make up only the last layer of the tree, and store either records themselves or references to the records.\n\nYou can think about each node being data that is stored on one page. So, when we want to access data in a B+ tree, we'll have to incur one I/O for each node that we read or write. We can assume that the fan-out is small enough such that this will always be true.\n\nThe **height** of a tree $h$ is defined as the number of pointers it takes to get from the root node to a leaf node. For example, it would take 3 I/Os to access a leaf node for a height $2$ tree (read root node, read inner node, read leaf node).\n\nAll entries within each node are sorted. All B+ trees must follow the two invariants below to guarantee nice properties:\n\n### Key Invariant\nFor any value $x$ in an inner node, the subtree that its pointer references must only have values greater than or equal to $x$.\n\n### Occupancy Invariant\nThe **order** of the tree, $d$, is defined such that each interior node is at least partially full:\n$$d \\le E \\le 2d$$\nwhere $E$ is the number of entries in a node. In other words, every node must have at least $d$ entries and at most $2d$ entries.\n\nThe **fan-out** is equal to $2d+1$. (This is the max number of pointers in each node.) The amount of data a B+ tree can store grows exponentially based on the fan-out factor: $f^h$ where $f$ is the fanout and $h$ is the height of the tree.\n\nAt typical capacities (fan-out of 2144), at a height of 2 the tree can already store $2144^3 = 9855401984$ records!\n\n\n## Operations\n\n### Insertion\n\nB+ Tree insertion is guaranteed to maintain the occupancy invariant.\n\nFor the following steps, we will use the example of trying to insert the value $21$ into the tree below:\n\n![Untitled](B+%20Trees/Untitled%201.png)\n\n1. Find the leaf node where the value should go (using binary search):\n    \n    ![Untitled](B+%20Trees/Untitled%202.png)\n    \n2. If the leaf node now has more than $2d$ entries:\n    1. Split the leaf node into two leaf nodes $L_1$ and $L_2$ with $d$ and $d+1$ entries, respectively.\n    \n    ![Untitled](B+%20Trees/Untitled%203.png)\n    \n    b. **Copy** the first value of $L_2$ into the parent node and adjust pointers to include $L_1$ and $L_2$.\n    \n    ![Untitled](B+%20Trees/Untitled%204.png)\n    \n3. If the parent is now overflowed, then recurse and do the algorithm again. But this time, instead of copying the first value, we will **move** it instead (doesn’t stay in the original node).\n\n### Deletion\nIn practice, the occupancy invariant is often not strictly enforced. This is because rearranging the tree is less efficient than having a good-enough approximation. \n\nTherefore, for B+ Tree deletion, just identify the leaf node that contains the desired value to delete, and simply remove it. Nothing else needs to be done.\n\nThis is acceptable in practice because it is usually far more common to insert values into a B+ tree than it is to delete them, and insertions will restore the occupancy invariant quickly.\n\n## The Three Alternatives\nThere are three ways to implement B+ trees that we'll explore in this class. Note that in Project 2, we will create an Alternative 2 B+ tree implementation.\n\n### Alternative 1\nThe first alternative is to store records directly in the leaf nodes. This allows faster access to the data, but is very inflexible in the case that we want to build an index on another column (we'd have to copy over all the data to another B+ tree).\n\n### Alternative 2\nThe second alternative is to store pointers to data pages in the leaf nodes, which then store the records. This solves the inflexibility issue of Alt. 1, but will need to incur an additional I/O \n\n### Alternative 3\nThe third alternative stores a *list* of pointers to matching record locations in the leaf nodes. This allows us to use less data if storing many duplicate entries, but comes at the cost of additional complexity.\n\n### Clustered Indexes\n\n![Untitled](B+%20Trees/Untitled%205.png)\n\nIn a clustered index, data on disk is roughly sorted (clustered) with respect to an index. This benefits accesses using that index, but may hurt performance for other indices.\n- Clustering can only be done with Alt. 2 or 3 B+ trees. We'll often talk about a \"Alt. 2 unclustered B+ tree\" or something similar.\n- Clustered indexes are more expensive to maintain (since we need to periodically update the order of files and reorganize).\n- Clustered indexes also work best when heap files have extra unfilled space to accommodate inserts, resulting in a larger number of pages.\n\n\u003e [!important] I/O Rule for Clustered Indexes\n\u003e \n\u003e In unclustered indexes, it takes about **1 I/O per record** accessed from the B+ tree, since each record is assumed to be in a different page. In clustered indexes, it takes about **1 I/O per page of records**, since neighboring records are assumed to be on the same page.\n\n\n## Optimizations and Improvements\n\n### Bulk Loading\n\nB+ Tree operations are rather inefficient in that we currently always need to start searches from the root. In addition, there is poor cache utilization due to large amounts of random access.\n\nBulkloading a B+ tree creates a slightly different structure, but has some nice guarantees that allow it to be used when constructing new trees from scratch.\n\nIn bulk loading:\n1. Sort the data by a key.\n2. Fill leaf pages up to size $f$ (the **fill factor**).\n3. If the leaf page overflows, then use the insertion split algorithm from a normal B+ tree.\n4. Adjust pointers to reflect new nodes if needed.\n\n### Sibling Pointers\n\nTo aid in tree traversal, we can add previous and next pointers between the child nodes. When stored in sequential order, data nodes can be visited from other data nodes more quickly.\n\n## Practice Problems\n\n{{\u003c tabs \"q1\" \u003e}}\n{{\u003c tab \"Question 1\" \u003e}}\nWhat is the maximum number of data entries an Alternative 1 B+ tree with height $h$ and degree $d$ can hold?\n{{\u003c /tab \u003e}}\n{{\u003c tab \"Q1 Answer\" \u003e}}\n$$(2d) \\times (2d+1)^h$$\nA height $0$ tree would just have a single leaf node. We know that the leaf node can hold up to $2d$ entries, so this gives us a good starting point.\n\nNow, a height $1$ tree would have one root node pointing to $2d + 1$ leaf nodes (due to the fan-out property). Since each leaf node can still hold $2d$ entries, in total therer should be $(2d) \\times (2d + 1)$ entries.\n\nWe can see that this pattern continues- for every additional layer, we add $2d+1$ inner nodes, which each point to $2d+1$ lower nodes. So we need to keep multiplying the result by $2d+1$, yielding the formula above.\n{{\u003c /tab \u003e}}\n{{\u003c /tabs \u003e}}\n\n{{\u003c tabs \"q2\" \u003e}}\n{{\u003c tab \"Question 2a\" \u003e}}\nSuppose we have a Alternative 2 clustered index built on `members` with a height of 5.\nHow many I/Os on average would it take to run the query `SELECT * FROM clubs WHERE members \u003e 60`? Assume the following:\n - 20 leaf pages satisfy this predicate.\n - 100 records satisfy this predicate.\n - $d=4$.\n - Each leaf page has pointers to the previous and next leaf pages.\n - Each data page fits 25 records.\n{{\u003c /tab \u003e}}\n{{\u003c tab \"Q2a Answer\" \u003e}}\n**26 I/Os.**\n\nFirst, we need to find the leaf node corresponding to `members = 60`. It will take $2$ I/Os to find the leaf page, since the height is 2.\n\nOnce we find the first leaf page, we can continue reading the sibling pointers until we reach the end, so no further inner nodes need to be accessed. In total, we will incur $20$ I/Os reading leaf pages.\n\nSince the index is clustered, we can assume that on average, each page of records incurs 1 I/O. There are 100 records that satisfy the predicate, and each page fits 25 records, so it takes $4$ I/Os to read the data.\n\nIn total, $2 + 20 + 4 = 26$  I/Os.\n{{\u003c /tab \u003e}}\n{{\u003c /tabs \u003e}}\n\n{{\u003c tabs \"q2b\" \u003e}}\n{{\u003c tab \"Question 2b\" \u003e}}\nSuppose everything is the same as the previous problem, except that the index is now *unclustered*. How many I/Os will the query incur now?\n{{\u003c /tab \u003e}}\n{{\u003c tab \"Q2b Answer\" \u003e}}\n**122 I/Os.**\n\nIt will take the same number of I/Os to read the inner and leaf pages: 2 and 20 respectively.\n\nHowever, we now need to incur an average of 1 I/O *per record*, rather than per page of records, so it will take about 100 I/Os to read all of the records.\n\nIn total, $2 + 20 + 100 = 122$ I/Os.\n{{\u003c /tab \u003e}}\n{{\u003c /tabs \u003e}}\n","lastmodified":"2023-01-08T06:32:50.399928165Z","tags":null},"/cs186/03-Buffer-Management":{"title":"","content":"","lastmodified":"2023-01-08T06:32:50.399928165Z","tags":null},"/cs186/04-Sorting-and-Hashing":{"title":"","content":"\nWhen dealing with disk operations, traditional sorting algorithms tend to create lots of random accesses and can be quite slow. We’ll explore a few strategies for creating optimized algorithms for sorting databases.\n\n## Single-Pass Streaming\n\nSingle-pass streaming is an approach for mapping inputs to their desired outputs while minimizing memory and disk usage.\n\n**Main idea:** There are two buffers (input and output). Continuously read from the input buffer and convert them into outputs to place in the output buffer. Only write to disk when the output buffer fills,\n\n![Untitled](Sorting%20and%20Hashing/Untitled.png)\n\n**Optimization: double buffering**\n- The main thread runs the function that converts inputs into outputs.\n- A second I/O thread runs simultaneously to handle the filling and draining of input and output buffers.\n- If the main thread is ready for a new buffer to compute, swap buffers between the two threads.\n\n## Two-Way External Merge Sort\n\n**Main idea:** As input buffers are streaming in, sort each input buffer, then merge two input buffers together into one output buffer using merge sort. Repeat until all pages are merged.\n\n![Untitled](Sorting%20and%20Hashing/Untitled%201.png)\n\nFor larger input sets that span multiple pages, several passes are required. In each pass, pages are merged together and double in size.\n\n**I/O Cost Analysis**\n- Suppose we have $N$ pages.\n- In every pass, we read and write each page in file, causing $2N$ IO’s.\n- The number of passes is logarithmic in nature: $\\lceil \\log_2 N \\rceil + 1$\n- Multiplying the number of passes by the cost per pass gives a total cost of $2N \\cdot (\\lceil log_2 N \\rceil + 1)$.\n\n## General External Merge Sort\n\nIn a typical system, we have more than 3 pages available to us at a time. So, we can merge more than two pages at a time.\n\nWe can leverage the full power of our buffer pool to minimize the number of pages created in the 0th pass:\n\nIf we have $B$ pages in our buffer pool, then for every pass after the 0th pass, merge $B-1$ pages into one output buffer. (Since there is no output in pass 0, it can use all $B$ pages.)\n\n**Runtime Analysis**\n\n- The number of passes now becomes $1 + \\lceil \\log_{B-1} \\lceil N / B \\rceil \\rceil$.\n- The total IOs per pass does not change from 2-way merge sort.\n- So, the total number of IOs becomes  $2N \\cdot (1 + \\lceil \\log_{B-1} \\lceil N / B \\rceil \\rceil)$.\n- The number of required passes decreases exponentially with respect to the number of buffer pages.\n\n## Hashing\n\nHashing is best for when we don’t care about the absolute order of elements, but only to group similar elements together. \n\n**Main idea:** use a hash function. Stream values with similar hash values to the same partition. Then, re-hash in memory using a different hash function and write the results to disk.\n\nEssentially, by partitioning the values, we are splitting a large file into many smaller files, each one with at most $B$ pages.\n\nSince these smaller files can each fit into the buffer, we can then use a more specific hash function to split them apart however we wish.\n\n![Untitled](Sorting%20and%20Hashing/Untitled%202.png)\n\n**Recursive partitioning:** if the first division step doesn’t divide into small enough partitions (# pages \u003e B), just keep dividing using different hash functions until the files are the correct size.\n\n- If all of the elements in a partition are the same, then immediately write it to disk instead of recursing as to avoid an infinite loop.\n\nThe number of input buffers read does not necessarily equal the number of output buffers created, since some output partitions could not be completely full.","lastmodified":"2023-01-08T06:32:50.399928165Z","tags":null},"/cs186/05-Iterators-and-Joins":{"title":"","content":"# Iterators and Joins\n\n## Cost Notation\n\nUsed to analyze the size of a database.\n\n$R$ is a table.\n\n$[R]$ is the number of pages needed to store $R$\n\n$p_R$ is the number of records per page of $R$.\n\n$|R|$ is the cardinality of $R$, or the number of records.\n\n- $|R| = p_R \\times [R]$\n\n## Simple Join\n\nIntuitively, joining two tables is essentially a double for loop over the records in each table:\n\n```python\nfor record r in R:\n\tfor record s in S:\n\t\tif join_condition(r, s):\n\t\t\tadd \u003cr, s\u003e to result buffer\n```\n\nwhere `join_condition` is an optional function, also known as $\\theta$, that returns a boolean (true if record should be added to result).\n\nThe cost of a simple join is the cost of scanning $R$ once, added to the cost of scanning $S$ once per tuple in $R$:\n\n$[R] + |R|[S]$\n\n## Page Nested Loop Join\n\nSimple join is inefficient because it requires an I/O for every individual record for both tables.\n\nWe can improve this by operating on the page level rather than the record level: before moving onto the next page, process all of the joins for the records on the current page.\n\n```python\nfor rpage in R:\n\tfor spage in S:\n\t\t\tfor rtuple in rpage:\n\t\t\t\t\tfor stuple in spage:\n\t\t\t\t\t\tif join_condition(rtuple, stuple):\n\t\t\t\t\t\t\tadd \u003cr, s\u003e to result buffer\n```\n\nNow, the cost becomes the cost of scanning $R$ once, then scanning $S$ once per *page* of $R$:\n\n$[R] + ([R] \\times [S])$\n\n## Block Nested Loop Join\n\nTo improve upon loop join even further, let’s take advantage of the fact that we can have $B$ pages in our buffer.\n\nRather than having to load in one page at a time, we can instead load in:\n\n- $1$ page of $S$\n- $1$ output buffer\n- $B-2$ pages of $R$\n\nand then load in each page of $S$ one by one to join to all $B-2$ pages of $R$ before loading in a new set of $B-2$ pages.\n\n```python\nfor rblock of B-2 pages in R:\n\tfor spage in S:\n\t\tfor rtuple in rblock:\n\t\t\tfor stuple in sblock:\n\t\t\t\tadd \u003crtuple, stuple\u003e to result buffer\n```\n\nThe cost now becomes the cost of scanning $R$ once, plus scanning $S$ once per number of blocks:\n\n$[R] + \\lceil [R] / (B-2) \\rceil \\times [S]$ \n\n## Index Nested Loop Join\n\nIn previous version of nested loop join, we’d need to loop through all of the elements in order to join them. \n\nHowever, with the power of B+ trees, we can quickly look up tuples that are equivalent in the two tables when computing an equijoin.\n\n```python\nfor r_row in R:\n\tfor \n```\n\nCost: $[R] + |R| \\times t_S$ where $t_S$ is the cost of finding all matching $S$ tuples\n\n- Alternative 1 B+Tree: cost to traverse root to leave and read all leaves with matching utples\n- Alternative 2/3 B+Tree: cost of retrieving RIDs + cost to fetch actual records\n    - If clustered, 1 IO per page. If not clustered, 1 IO per tuple.\n- If no index, then $t_S = |S|$ which devolves INLJ into SNLJ.\n\n## Sort-Merge Join\n\n**Main idea:** When joining on a comparison (like equality or $\u003c$), sort on the desired indices first, then for every range (group of values with identical indices) check for matches and yield all matches.\n\n![Untitled](Iterators%20and%20Joins/Untitled.png)\n\nThe cost of sort-merge join is the sum of:\n\n- The cost of sorting $R$\n- The cost of sorting $S$\n- The cost of iterating through R once, $[R]$\n- The cost of iterating through S once, $[S]$\n\nOne optimization we can make is to stream both relations directly into the merge part when in the last pass of sorting! This will reduce the IO cost by removing the need to re-read $[R] + [S]$.\n\n- Subtract $2 \\times ([R] + [S])$ IOs\n\n## Hash Join\n\nIf we have an equality predicate, we can use the power of hashing to match identical indices quickly.\n\nNaively, if we load all records in table $R$ into a hash table, we can scan $S$ once and probe the hash table for matches.\n\n- This requires $R$ to be less than $(B-2) \\times H$ where $H$ is the hash fill factor.\n\n## Grace Hash Join\n\nIf the memory requirement of $R \u003c (B-2) * H$ is not satisfied, we will have to partition out $R$ and process each group separately.\n\nEssentially, Grace Hash Join is very similar to the divide-and-conquer approach for hashing in the first place:\n\n![Untitled](Iterators%20and%20Joins/Untitled%201.png)\n\n- In the dividing phase, matching tuples between $R$ and $S$ get put into the same partition.\n- In the conquering phase, build a separate small hash table for each partition in memory, and if it matches, stream the partition into the output buffer.\n\nFull process:\n\n1. Partitioning step: make $B-1$ partitions.\n2. If any partitions are larger than $B-2$ pages, then recursively partition until they reach the desired size.\n3. **Build and probe:** \n    1. Build an in-memory hash table of one table $R$, and stream in tuples of $S$.\n    2. For all matching tuples of $R$ and $S$, stream them to the output buffer.","lastmodified":"2023-01-08T06:32:50.399928165Z","tags":null},"/cs186/06-Relational-Algebra":{"title":"","content":"# Relational Algebra\n\n**Relational algebra** is a language that represents a logical query plan for translating SQL queries into underlying actions.\n\n- perform operations on sets (the “how”)\n- operational description of transformations\n- Related to relational calculus (which describes the result of a computation: the “what”) by Codd’s Theorem: everything that can be represented with relational calculus can be equivalently represented in relational algebra\n\n# Operators\n\n## Unary Operators\n\nUnary operators work on a **single relation.** \n\n- Projection: $\\pi$\n    - retains only desired columns (vertical)\n- Selection: $\\sigma$\n    - retains only a subset of rows (horizontal)\n- Renaming: $\\rho$\n    - rename attributes and relations\n\n## Binary Operators\n\nBinary operators work on **pairs of relations.** \n\n- Union: $\\cup$\n    - Or operator: either in r1 or r2\n    - Equivalent to `UNION` in SQL (doesn’t keep duplicates: `UNION ALL` does)\n- Set difference:  $-$\n    - Tuples in r1, but not in r2\n    - Equivalent to `EXCEPT` in SQL\n- Cross product: $\\times$\n    - Joins r1 with all r2\n\nThe schemas for both relations must be identical for union and set difference.\n\n## Compound Operators\n\nCompound operators are macros (shorthand) for several unary or binary operators together.\n\n- Intersection: $\\cap$\n    - And operator: both in r1 and r2\n- Joins: $\\bowtie$, $\\Join_\\theta$\n    - Combine relations that satisfy predicates (combination of cross product, selection)\n    - Theta join ($\\Join_{\\theta}$): join on any logical expression $\\theta$\n    - Natural join ($\\Join$): equi-join on all matching column names\n        - $R \\Join S = \\pi_{unique cols} \\sigma_{matching cols equal}(R \\times S)$\n\n## Extended Relational Algebra\n\n- Group by: $\\gamma$\n    - Usage: $\\gamma_{age, AVG(rating),COUNT(*)\u003e2}(S)$ = `GROUP BY age, AVG(rating) HAVING COUNT(*)\u003e2`\n\n## Examples\n\n$\\pi_{sname,age}(S)$: `SELECT sname, age FROM s`\n\n$\\sigma_{rating\u003e8}(S)$: `SELECT * FROM s WHERE rating\u003e8`\n\n$\\rho(Temp1(1 \\to sid1, 4 \\to sid2), R \\times S)$: combines every tuple in $R$ with every tuple in $S$, then renames the 1st and 4th columns to `sid1` and `sid2` respectively","lastmodified":"2023-01-08T06:32:50.399928165Z","tags":null},"/cs186/07-Query-Optimization":{"title":"","content":"# Query Optimization\n\n**Query optimization** is the bridge between a declarative language (like SQL, where you describe “what” you want) and an imperative language (like Java, which describes how the answer is actually computed).\n\n## System R Optimizers\n\n![Untitled](Query%20Optimization/Untitled.png)\n\nThe query parser first checks for correctness and authorization (user permissions to access the table). It then generates a parse tree out of the query. This step is usually fairly straightforward.\n\nNext, the query rewriter converts queries into smaller query blocks, and flattens the views.\n\nOnce the query is rewritten, it gets passed into the query optimizer. The primary goal of a query optimizer is to translate a simple query plan into a better query plan. A cost-based query optimizer processes one query block at a time (e.g. select, project, join, group by, order by). \n\n- For each block, consider:\n    - all relevant access methods\n    - All left-deep join trees: right branches are always simple FROM clauses\n    \n    ![Untitled](Query%20Optimization/Untitled%201.png)\n    \n- Typically, we don’t care about exact performance; the main purpose of query optimization is to prune out extremely inefficient options.\n\n## The Components of a Query Optimizer\n\nThere are three main problems:\n\n1. **Plan space:** for a given query,  what plans are considered?\n    1. Two query plans are **physically equivalent** if they result in the same content and the same physical properties (primarily sort order and hash grouping).\n2. **Cost estimation:** how do we estimate how much a plan will cost?\n    1. In System R, cost is represented as a single number: #IOs + CPU-factor*#tuples (where CPU factor is the proportion of time the system spends actually computing the query). We typically just care about the IO cost. \n    2. For each term, determine its **selectivity** $S$ (size of output / size of input).  Lower selectivity value is better (filters more items).\n        1. Result cardinality is equal to the max number of tuples multiplied by the product of all selectivities.\n        2. If searching for `col = value`,  $S = 1/N(l)$ where $N(l)$ is the number of unique values in the table.\n        3. If searching for `col1 = col2`, then $S = 1/max(N(l_1), N(l_2))$.\n        4. If searching for `col \u003e value`, then $S = (max(l) - value) / max(l) - min(l) + 1).$\n        5. If we are missing the needed stats to compute any of the above, assume that $S = 1/10$. \n        6. If searching for a disjunction (or) $Q_1 \\lor Q_2$, $S = S_1 + S_2 - (S_1 \\times S_2)$.\n        7. If searching for a conjunction (and) $Q_1 \\land Q2$, $S = S_1 \\times S_2$.\n        8. If searching for a negation (not), $\\lnot Q_1$, $S = 1 - S_1$.\n        9. For joins, simply apply the selectivity query to the cross product of the two tables that are being joined.\n    3. For clustered indexes, the approximate number of IOs is $(P(L) + P(R)) \\times S$ where $P$ is the number of pages and $S$ is selectivity.\n    4. For unclustered indexes, the approximate cost is $(P(L) + T(R))\\times S$ where $T$ is the number of tuples.\n    5. A sequential scan of a file takes $N(R)$ cost.\n3. **Search strategy:** how do we search the plan space?\n\n## Relational Algebra Equivalences\n\nSelections:\n\n- $\\sigma_{c_1 \\land \\cdots \\land c_n}(R) \\equiv \\sigma_{c_1}(\\cdots(\\sigma_{c_n}(R))\\cdots)$ (cascade)\n- $\\sigma_{c1}(\\sigma_{c2}(R)) \\equiv \\sigma_{c2}(\\sigma_{c1}(R))$ (commutative)\n\nProjections:\n\n- Can also cascade like selections:\n    \n    ![Untitled](Query%20Optimization/Untitled%202.png)\n    \n    - Make sure that the table being selected from actually has the desired column to begin with\n\nCross (Cartesian) products:\n\n- $R \\times (S \\times T) \\equiv (R \\times S) \\times T$ (associative)\n- $R \\times S \\equiv S \\times R$ (commutative)\n\nJoins:\n\n- Are sometimes commutative or associative, but not always (depends on the  join condition)\n    - Need to make sure that the same rows are being filtered out at each step\n    - Some join orders require cross products, others don’t (cross products less efficient)\n\n## Common Heuristics\n\n**Selection and projection cascade and pushdown:** apply selections ($\\sigma$) and projections ($\\pi$) as soon as you have the relevant tables. i.e. push them as far to the right as possible\n\n**Avoid Cartesian products:** given a choice, do theta-joins rather than cross products.\n\n**Put the more effective selection onto the outer loop before a join:** reorder joins such that we are joining a smaller table in the outer loop with a larger table in the inner loop, especially if this allows us to filter out more rows in the outer loop\n\n**Materialize inner tables before joins:** rather than calculating selections on the fly, create a temporary filtered table before passing it into a join\n\n- Not effective 100% of the time since it costs IO’s to write and re-read the table\n\n**Use left-deep trees only** (explained in an earlier section).\n\n## Algorithms\n\n**Table access with selections and projections:**\n\n- Heap scan\n- Index scan if available\n\n**Equijoins:**\n\n- Block nested loop join when simple algorithm needed\n- Index nested loop join if one relation is small and the other one is indexed\n- Sort-merge join if equal-size tables, small memory\n- Grace hash join if 1 table is small\n\n**Non equijoins:**\n\n- Block nested loop join\n\n## Statistics and Catalogs\n\nIn order to perform query optimization we need to store metadata about what we’re referencing.\n\n![Untitled](Query%20Optimization/Untitled%203.png)\n\nCatalogs are updated periodically (since they are just estimations, and we can save computation).\n\n## Selinger Query Optimization\n\nThe Selinger query optimization algorithm uses dynamic programming over $n$ passes (where $n$ is the number of relations).\n\nIn each of the passes, we use the fact that left-deep plans can differ in the order of relations, access method for leaf operators, and join methods for join operators to enumerate all of the plans.\n\nMore specifically:\n\n- In the 1st pass, find the best single relation plan for each relation. (what’s the best way to scan a table?)\n    - This includes full scans (# IOs = # pages in table) and index scans (# IOs depends on type of index and any applicable selects, since selections are pushed down)\n- In each $i$ith pass, find the best way to join the result of an $i-1$ relation plan to the $i$th relation. (The $i-1$ plan will always be the outer relation due to the left-deep property.)\n- For each subset of relations, retain only:\n    - Cheapest plan overall for each combination of tables,\n    - Cheapest plan for each interesting order of tuples\n\n**The principle of optimality:** the best overall plan is composed of the best decisions on the subplans. This means that if we find the cheapest way to join a subset of tables, we can add on more tables one by one by finding the cheapest cost for that single join operation.\n\n### Interesting Orders\n\nSo, what are interesting orders? \n\nIf a GROUP BY or ORDER BY clause will be processed later on, it may be useful to take a temporary hit in IO cost in order to avoid needing to resort or regroup the table in a future step.\n\n## Selectivity Estimation Practice\n\nSuppose $R(a,b,c)$ has 1000 tuples and $S(a)$ has 500 tuples. We have the following indexes:\n\n- R.a: 50 unique integers uniformly distributed in $[1, 50]$\n- R.b: 100 unique float values uniformly distributed in $[1, 100]$\n- S.a: 25 unique integers uniformly distributed in $[1, 25]$\n\n`SELECT * FROM R`: Full scan requires iterating through every tuple in $R$, so 1000 tuples are outputted.\n\n`SELECT * FROM R WHERE a = 42`:\n\n- The selectivity is $1/50$ since there are 50 unique values in a and exactly one of them is desired.\n- $1000 \\times 1/50 = 20$ tuples.\n\n`SELECT * FROM R WHERE c = 42`:\n\n- We have no information, so by default $$S=1\n- $1000 \\times 1/10 = 100$ tuples.\n\n`SELECT * FROM R WHERE a \u003c= 25`:\n\n- Exactly 1/2 of all possible values of `a` are less than 25 (exact formula listed above).\n- $1000 \\times 1/2 = 500$ tuples.\n\n`SELECT * FROM R WHERE b \u003c= 25`:\n\n- Using the equation for selectivity of inequalities, (value - low) / (high - low) = (25-1)/(100-1) = 24/99.\n- $\\lfloor 1000 \\times 24/99 \\rfloor = 242$ tuples.\n\n## Access Plans\n\nSuppose we have \n\n![Untitled](Query%20Optimization/Untitled%204.png)","lastmodified":"2023-01-08T06:32:50.399928165Z","tags":null},"/cs186/08-Transactions":{"title":"","content":"# Transactions\n\nTransactions are collections of operations that can be treated like a single unit.\n\n## ACID and Concurrency\n\nWe want all transactions to obey ACID:\n\n- **Atomicity:** either all operations happen, or none of them\n- **Consistency:** database remains in a consistent state with its constraints\n- **Isolation:** it should appear as if we only run 1 transaction at a time (even if they’re actually run concurrently)\n- **Durability:** once a transaction commits, it persists\n\nTransaction operations:\n\n- Commit: indicates a successful transaction, changes should be saved\n- Abort: indicates an unsuccessful transaction, changes should be reverted\n\nConcurrent transitions are better:\n\n- Increase throughput (processor and disk utilization) → more transactions are completed per second\n- Decrease latency → transactions complete more quickly\n\n## Serializability\n\n- A **schedule** is the order in which we execute operations in a set of transactions.\n- A **serial schedule** is a schedule in which every transaction completes without interleaving (finish all parts of one transaction from start to finish before moving to the next)\n- **Two schedules are equivalent if:**\n    - They involve the same transactions\n    - Each transaction’s operations are completed in the same order\n    - The final state after all transactions is the same\n- Isolation = serializable = schedule is equivalent to a serial schedule\n- In order to check serializability, we will check if they are **conflict serializable:**\n    - Two operations are in a schedule conflict if\n        - at least one operation is a write\n        - the operations are on different transactions\n        - the operations work on the same resource\n    - Two operations are conflict equivalent if every conflict is ordered in the same way.\n    - A schedule is conflict serializable if it is conflict equivalent to a serial schedule.\n- **View serializability** refers to conflict serializability in which blind writes (intermediate writes that are overwritten without a read in between) are ignored. Checking view serializability is an NP complete problem.\n\n![Untitled](Transactions/Untitled.png)\n\n## Dependency Graphs\n\n- Each node = one transaction\n- If an operation in transaction $T_i$ conflicts with an operation in $T_j$, and the operation in $T_i$ comes first, then create an edge between the $T_i$ and $T_j$ nodes.\n- To find equivalent schedules, run topological sort on all involved graphs. **All conflict serializable schedules have an acyclic dependency graph.** So if a graph has a cycle, it is not conflict serializable.\n\n![Untitled](Transactions/Untitled%201.png)\n\n# Locking\n\nA transaction may lock a resource in two ways:\n\n- S lock (shared): allows a transaction to read a resource\n    - Multiple transactions can hold S lock on the same resource at the same time\n- X lock (exclusive): allows a transaction write a resource\n    - No other transaction can have any type of lock on the same resource as a transaction with an X lock on it\n    \n    ![Untitled](Transactions/Untitled%202.png)\n    \n\n## Deadlock\n\nDeadlock occurs when there is a cycle of transactions all waiting for each other to release their locks.\n\n### Deadlock Avoidance\n\nDeadlock avoidance: catching deadlocks before they occur\n\n- Wait-die: if $T_i$ wants a lock but $T_j$ holds a conflicting lock:\n    - If $T_i$ higher priority, wait for $T_j$ to release\n    - If $T_i$ lower priority, abort (die)\n- Wound-wait:\n    - If $T_i$ is higher priority, $T_j$ aborts\n    - If $T_i$ is lower priority, it waits for $T_j$ to finish\n- If no explicitly defined priority, we can assign priority by age (current time - start time)\n\n### Deadlock Detection\n\nTo perform deadlock detection, we can draw a **waits-for graph**:\n\n- One note per transaction\n- If $T_i$ holds a lock that conflicts with the lock that $T_j$ wants (i.e. $T_j$ waits for $T_i$), add an edge from $T_j$ to $T_i$\n- Deadlock occurs if there is a cycle in the graph\n\n## Two Phase Locking (2PL)\n\n2PL is one method of enforcing conflict serializability.\n\nThere are two phases:\n\n1. From start until a lock is released, the transaction is only acquiring locks (acquiring step)\n2. From after a lock is released to the end of the transaction, the transaction is only releasing locks (release phase)\n\nTransactions cannot acquire any lock after it has released a lock.\n\nStrict 2PL only allows releasing of locks at the end of the transaction. This avoids cascading aborts (when unrelated transactions are aborted due to lock release schedule).\n\n## Multigranularity Locking\n\n- Tuple-level locking: one lock per tuple (high locking overhead due to a scan requiring many locks)\n- Table-level locking: one lock per table (low concurrency since any update locks the entire table)\n- **Multigranularity Locking:** based on operation, allow for different types of locks\n    - Scans = lock entire table = lower overhead\n    - Update tuple = lock only the affected tuple = higher concurrency\n    - For each lock, transactions must hold **intent locks (IX, IS)** at all higher levels of granularity\n        - Indicate a future requirement to lock at a higher level\n        - Example: If S lock on tuples is requested, we need intent locks on database, table, and page\n        - IX = intent to acquire exclusive lock on lower level\n        - IS = intent to acquire shared lock on lower level\n        - SIX = shared + intent to acquire exclusive lock at lower level. (equivalent to having both S and IX locks - can read entire table and acquire X locks when needed)\n        - In order to acquire S, a transaction must also have the IS lock (same for X)\n\n![Untitled](Transactions/Untitled%203.png)","lastmodified":"2023-01-08T06:32:50.399928165Z","tags":null},"/cs186/09-Parallel-Query-Processing":{"title":"","content":"# Parallel Query Processing\n\nParallelism helps us break down a big problem into small, independent chunks. The idea is that a lot of machines all working on the same problem at the same time will finish the problem more quickly.\n\nThere are two main metrics we want:\n\n- **Speed-up:** if we add more hardware, the same workload should be completed more quickly.\n- **Scale-up:** if the workload increases, we should be able to add a corresponding amount of hardware to make the problem be processed with the same amount of time as before.\n\nThere are two main types of parallelism:\n\n- **Pipelining:** each machine does one component of the calculation, then passes the result on to another machine\n- **Partitioning:** each machine runs the same computations on a different set of data.\n\nNow for types of query parallelism:\n\n- **Inter-query parallelism:** each query runs on a different processor. Requires parallel-aware concurrency control.\n- **Intra-query, inter-operator parallelism:** each operator in one query is done by a differerent machine.\n    - **Pipeline parallelism:** every query operation depends on the previous query’s output\n    - **Bushy tree parallelism:** do two operators at the same time if they don’t depend on each other\n- **Intra-query, intra-operator parallelism:** for joins, each machine scans and processes a chunk of the table, and all results are eventually combined.\n    - **Partition parallelism:** partition the data, and operate on each partition simultaneously since they don’t depend on each other\n\n## Data Partitioning\n\nSo, if we’re going to do intra-operator parallelism, how do we split up the data in the first place?\n\n![Untitled](Parallel%20Query%20Processing/Untitled.png)\n\nRange and hash partitioning are prone to key-skew. Round-robin ensures that all machines receive roughly equal amounts of work regardless of the data distribution.\n\nHowever, with range and hash partitioning, searching is more efficient since we only need to query a subset of machines.\n\n## Parallel Query Operators\n\n### Parallel Hashing\n\n![Untitled](Parallel%20Query%20Processing/Untitled%201.png)\n\nAdd a new partitioning phase to separate data into machines before running the algorithm individually for each machine.\n\n### Parallel Hash Joins\n\n![Untitled](Parallel%20Query%20Processing/Untitled%202.png)\n\nUse hash partitioning on both relations, then perform a normal hash join on each machine independently.\n\n### Symmetric Hash Joins\n\nStreaming hash join algorithm: does not require all tuples to be available.\n\nBasic idea: build two hash tables ($R$ and $S$) at the same time\n\n- When a tuple from $R$ arrives, probe hash table for $S$ and add the tuple into hash table for $R$.\n    - If there are matching tuples in $S$, then add all joined tuples to the output by iterating over the hash table bucket.\n- Do the same for $S$ and $R$.\n\n### Parallel Sorting\n\nPartition the data over machines using **range partitioning.** Then, perform sort-merge join independently on each machine. Since each range is independent, we can join all of them at the end for the final output.\n\n### Parallel Hashing\n\n### Parallel Aggregation\n\nSUM, COUNT, AVERaGE, etc.\n\n- Use hierarchical aggregation: first, local function calculates sum and count for both machines. Then, a global function combines the results from all of the machines.\n\n### Asymmetric Shuffles\n\nSometimes, data is already partitioned the way we want. This would make it redundant to repartition or send data. \n\nFor example, if we wanted to run sort-merge join on R and S, but R is already range partitioned, then we can leave R alone and range-partition S using the same ranges before performing the merge.\n\n### Broadcast Joins\n\nSometimes, one table is tiny and another one is huge. It can be very expensive to partition the huge table, so we can instead send the entire tiny table to each machine containing the huge table and perform the operation locally.\n\nHere are some examples of parallel operations:\n\n- Lookup by key: easy for range/hash, need to broadcast to all machines for RR\n- Insert: need specific machine for range/hash, can go to any machine for RR\n- Unique key: easy for range/hash, need to broadcast request for RR. If response received, overwrite in correct machine. Otherwise, insert anywhere\n- Hashing: just\n\n## Measuring Parallelism\n\n**Network cost:** amount of data we need to send over the network to perform an operation\n\n- Measured in bytes/KB/GB etc.\n- Unlike I/Os, we can send one tuple at a time rather than entire pages\n- Network cost is incurred whenever a tuple needs to be moved to another machine (e.g. due to partitioning). Typically all data originates on 1 of the machines and needs to be distributed.\n\n**Measuring partitioning costs:**\n\n- Given $N$ pages on one machine, we want to partition the data onto $M$ machines. How much data would be transferred if each page is $S$ KB large?\n    - If all data is uniform and hash functions are uniformly distributed, all partition schemes (hash, range, RR) would put $N/M$ pages on each machine. $M-1$ machines need to be written to, so $\\frac{N}{M} \\times (M-1) \\times S$ KB would be written.\n    - In the best case, range and hash partitioning would not need to write anything at all (since everything is on the first machine). So the cost is $0$ KB. Round Robin best case is the same as the above calculation.\n    - In the worst case, range and hash partitioning would need to write all $M$ pages to other machines, so $M \\times S$ KB would be written. Round Robin is guaranteed to always uniformly distribute data so its worst case and best case are the same.","lastmodified":"2023-01-08T06:32:50.399928165Z","tags":null},"/cs186/10-Recovery":{"title":"","content":"# Recovery\n\n**Recovery** is the process of making databases resilient to failure. Specifically, recovery enforces **durability** (a committed transaction remains persistent) and **atomicity** (either all of the operations in a transaction complete, or none of them).\n\n**Assumptions:**\n\n- We use strict two-phase locking for concurrency control.\n- Updates happen in-place: transactions that modify data overwrite entries in the database.\n\n## Steal/No Force\n\n### No Steal Policy\n\nDon’t allow buffer pool frames with uncommitted updates to be replaced or flushed to disk.\n\n- Achieves atomicity\n- Can cause poor performance due to pinned pages limiting buffer replacement\n\n### Force Policy\n\nBefore commit, ensure that every update is forced onto the disk.\n\n- Achieves durability\n- Can cause poor performance due to high random IO\n\nA simplistic attempt at recovery (that doesn’t actually work) would be to combine no-steal and force. This doesn’t work because it doesn’t guarantee atomicity if the DB crashes while writing dirty pages.\n\n### Steal No-Force Policy\n\nSteal No-Force is the most efficient approach, but also complicated:\n\n- No Force: flush dirty pages as little as possible, and only when convenient, before commits. Allow for redoing modifications. (Can complicate durability)\n- Steal: allow buffer pool frames to be replaced whenever convenient. (Can complicate atomicity)\n\n## Logging\n\n![Untitled](Recovery/Untitled.png)\n\n**Main idea:** for every update, record info to allow undoing and redoing.\n\nA log is an ordered list of records.\n\n- Each record contains:\n    - Transaction ID\n    - Page ID\n    - Offset\n    - Length\n    - Old Data\n    - New Data\n    - Additional control info\n- A log also contains a write buffer (tail) in RAM.\n- Each log record has a unique, increasing log sequence number (LSN).\n- The log tracks `flushedLSN` that tells us the most recently flushed log record.\n- Each data page in the database contains a `pageLSN`, a pointer to the most recent log record for an update to that page.\n\n**Write Ahead logging protocol (WAL):**\n\n- Log record must be written for an update before the corresponding data page is written to disk (allows atomicity with UNDO info)\n- All log records must be stored for a transaction before commit (allows durability with REDO info)\n- **Invariant:** before page `i` is flushed to the DB, `pageLSN \u003c= flushedLSN`. i.e. all log records for the page need to be flushed before the page itself can be flushed.\n\n### ARIES Log Records\n\n![Untitled](Recovery/Untitled%201.png)\n\nARIES  (Algorithms for Recovery and Isolation Exploiting Semantics) is a logging algorithm. Some details:\n\n- Every log record contains `prevLSN`, which is the previous log record written by this transaction. This helps us undo a chain of operations.\n- Log records can be several types:\n    - Update, commit, abort\n    - Checkpoint (for log maintinence)\n    - Compensation Log Records (CLRs) for undo actions\n    - End (of commit or abort)\n- Two in-memory tables are created:\n    - Transaction table contains one entry per active transaction. When a transaction commits or aborts, it is removed.\n        - Contains transaction ID, status (running, committing, aborting) and lastLSN columns.\n    - Dirty page table contains one entry per dirty page in the buffer pool.\n        - Contains page ID and recLSN (log record that first caused the page to be dirty).\n        - recLSN stands for “recovery LSN”: the first timestamp in which the page is dirty.\n\n### ARIES Checkpoints\n\n**Main idea:** save the state of the database periodically so we don’t need to process the entire log during recovery.\n\nA checkpoint consists of the following:\n\n- Stop accepting any new transactions\n- Wait until all current transactions complete (commit or abort)\n- Flush log to disk\n- Flush all dirty pages to disk\n- Write a CKPT log record\n- Flush log again\n- (At this point, all commits are written, and all aborts have been rolled back)\n- Resume processing transactions\n\nThis is very slow because the database freezes during checkpoint creation. **Fuzzy checkpointing** attempts to resolve this issue by saving the state of all transactions and page statuses, but does not stop any transactions or flush dirty pages.\n\n- Keep track of transaction states (transaction table) and dirty page table\n- Save the above to disk\n- When recovering, recreate the above from the log, recreate running transactions and dirty pages in memory, then replay the rest of the log.\n\nSpecifically, the following happens:\n\n- Write a BEGIN CKPT to log\n- Flush log to disk\n- Continue normal operation\n- When DPT and transaction tables are written to disk, write END CKPT to log\n\n### ARIES Normal Operation\n\n**Start Transaction:**\n\n- Write START to log\n- Update transactions table with new transaction\n- Set the following values\n    - `prevLSN = lastLSN` in log\n    - `pageLSN = LSN` in buffer pool\n    - `lastLSN = LSN` in transaction table\n    - If `recLSN` is null, then set it to `LSN` (i.e. first time this page is dirty)\n\n**Flush Page:**\n\n- Flush log up to and including `pageLSN`\n- Remove page from DPT and buffer pool\n\n**Fetch page:** \n\n- Create new entry in DPT and buffer pool\n\n**Commit:**\n\n- Write commit record to log\n- Flush log up to entry\n- Update transaction table status to commit\n\n**Abort:**\n\n- Write abort record to log\n- Find first action to undo from `lastLSN`\n- Start to undo changes\n- Write compensation record (CLR) to log, and set is `undoNextLSN` to the next record to undo\n- Follow `prevLSN` like a linked list pointer to find next value(s) to undo\n- When the last action has been undone, change the transaction status to Abort\n\n## Crash Recovery\n\n### Summary\n\n- Start from a checkpoint from the master record\n- Analysis phase: figure out which transactions committed and which ones failed\n- REDO phase: repeat the history of the log and reconstruct the state of the database before the crash.\n- UNDO phase: undo effects of failed transactions.\n\n### Analysis Phase\n\n- Re-establish knowledge of state at checkpoint via transaction table and dirty page table stored in the checkpoint.\n- Scan log forward from the checkpoint.\n    - End record: remove transaction from transaction table\n    - Update record: if page is not in DPT, add it to the DPT and set its `recLSN` to `LSN` (current log record being scanned).\n    - Else: add transaction to transaction table, set `lastLSN` to `LSN`, then change the transaction status on commit or abort.\n- After analysis ends:\n    - For every transaction in the committing state, write a corresponding END record and remove it from the transaction table.\n    - The transaction table now displays which transactions were active at the time of crash. Set all of these to aborting status and write abort records.\n    - The DPT now shows which dirty pages might not have made it to disk.\n    \n\n### REDO Phase\n\n- Scan forward from the log record containing the smallest `recLSN` in the dirty page table.\n    - Checkpoints don’t actually save data, so we have to replay all of the actions that didn’t necessarily make it onto disk. `recLSN` tells us when the log and disk diverges.\n- Cases where we don’t want to redo:\n    - **Main idea:** The point of REDO is to replay transactions. If an operation has already made it to disk, we don’t need to redo it.\n    - Affected page is not in DPT (page was flushed and removed before checkpoint)\n    - Affected page is in DPT, but `recLSN \u003e LSN` (page was flushed and removed from DPT, then referenced again and reinserted into DPT later on)\n    - `pageLSN \u003e= LSN` (page was updated again and flushed after the log record)\n- If we do want to redo:\n    - Reapply the logged action.\n    - Set pageLSN to LSN with no additional logging.\n\n### UNDO Phase\n\n- The transaction in the transaction table after analysis should abort.\n- Do one backwards pass of the entire log, and undo sequentially.\n\n![Untitled](Recovery/Untitled%202.png)\n\n### What happens if DB crashes during crash recovery?\n\n- During analysis: no serious consequences, simply restart from the beginning of crash recovery\n- During REDO: no serious consequences, can detect redos and","lastmodified":"2023-01-08T06:32:50.399928165Z","tags":null},"/cs186/11-Distributed-Transactions":{"title":"","content":"# Distributed Transactions\n\nIf \n\n## 2 Phase Commit\n\n![Untitled](Distributed%20Transactions/Untitled.png)\n\n**Phase 1: voting**\n\n- Coordinator asks participants to vote by sending a PREPARE message to all participants.\n- Participants send VOTE YES or VOTE NO to coordinator.\n- Participants log and flush either a PREPARE or ABORT record to the log, keeping track of the coordinator ID.\n- After the coordinator receives a message from all participants, the coordinator logs and flushes either a COMMIT or ABORT record to log\n\n**Phase 2: results**\n\n- Coordinator tells participants to COMMIT or ABORT.\n- Participants log and flush COMMIT or ABORT to log.\n- Participants send ACK to coordinator.\n- Coordinator logs (but does not need to flush) an END record to the log, to remove it from the transaction table.\n\nAll results must be unanimous in order to commit. Any one node that can’t commit should cause the entire transaction to abort.\n\n### Recovery\n\n- If we have a COMMIT or ABORT log record, we know what to do. (The coordinator should send commit or abort messages periodically until all ACKs are received.)\n- If we have a PREPARE log record, but no commit/abort, then we’re at a participant node that should send a message to the coordinator inquiring about the status of the transaction.\n- If we have no prepare, commit, or abort, then something crashed.\n    - If at a participant node, abort the transaction (did not send YES)\n    - If at a coordinator node, respond to all future votes with ABORT.\n- It is never possible to COMMIT if either the coordinator or participants have written an ABORT.\n- It is never possible to ABORT if any one participant has written COMMIT.\n\n### Optimization\n\n**Presumed abort:** a transaction should abort if we have no log records locally.\n\nWhen a transaction aborts:\n\n- Coordinator cleans up locally- remove transaction from table if it doesn’t have ACKs\n- If participant receives ABORT, do not send ACKs\n- If transaction not in coordinator’s transaction table when participant inquires, ABORT\n- Don’t store participant IDs in abort records\n- Abort records do not need to be flushed\n\n### Blocking\n\nIf a node crashes during the voting (first) phase, any participant that voted yes keeps locks and waits for commit or abort\n\nIf a participant doesn’t recover, coordinator respawns new participant using log records; destroy old participant\n\nIf the coordinator doesn’t recover, 2PC doesn’t work (use 3PC, etc)\n\n### Distributed Deadlock\n\nSuppose multiple machines have running transactions, and each machine has its own waits-for graph.\n\nTo evaluate deadlock, union all of the waits-for graphs and check for cycles.\n\n## 2PC Timing\n\nThe best case time for a transaction to complete using 2PC:\n\nPhase 1:\n\n- Coordinator sends a prepare message (coordinator send time)\n- Participants flush to log (flush time)\n- Participants send Yes message (max of participant send times)\n- Coordinator flushes a commit log (flush time)\n\nPhase 2:\n\n- Coordinator sends commit message (coord send time)\n- Participants flush commit record (flush time)\n- Participants send ACK (max of participant send times)\n- Coordinator flushes END record (flush time)\n\nTime to abort:\n\nPhase 1:\n\n- Coordinator sends prepare message\n- Committing participants flush prepare, aborting participants add abort record\n    - max (times for aborts to send, times for commits to send + flush time)\n\nPhase 2:\n\n- Coordinator sends abort message\n- Under abort optimization, no ACK is required.","lastmodified":"2023-01-08T06:32:50.399928165Z","tags":null},"/cs186/12-ER-Diagrams":{"title":"","content":"# ER Diagrams\n\nProduction databases have a lot of tables with complicated relationships. **Entity Relationship (ER) Diagrams** help us organize databases in a visual manner.\n\n### Steps in Database Design\n\n- Requirement Analysis: what do users need the database to do?\n- Conceptual Design (ER Model): highly level description of DB schemas\n- Logical Design: translate ER model into DBMS data model\n- Schema Refinement: consistency, normalization\n- Physical Design: indices, disk layout\n- Security Design: who accesses what, and how\n\n## Data Models\n\nA data model is a collection of concepts for describing data.\n\nA schema is a description of a particular collection of data using a given data model. \n\nAbstraction:\n\n- Users see views (eg app on smartphone)\n- Logical structure defined by conceptual schema\n- Physical structure stores conceptual schema using files and indices\n\n**Logical Data Independence:** maintain views when logical structure changes\n\n**Physical data independence:** maintain logical structure when physical structure changes\n\n## Entities\n\nEntities are real-world objects that are described with attributes.\n\nAn entity set is a collection of the same type of entities (same attributes).\n\n- Entity sets are described by a key (rectangle) and attributes (ellipses):\n    \n    ![Untitled](ER%20Diagrams/Untitled.png)\n    \n- Primary keys are underlined.\n\n## Constraints\n\nA relationship is an association between multiple entities or entity sets.\n\nA relationship set is a collection of the same type of relationships (diamond).\n\n![Untitled](ER%20Diagrams/Untitled%201.png)\n\n![Untitled](ER%20Diagrams/Untitled%202.png)\n\n![Untitled](ER%20Diagrams/Untitled%203.png)\n\n## Weak Entities\n\nWeak entities can be defined uniquely only with the key of another entity.\n\n- The partial key (dashed underline) is the key in the other entity that must be combined with the owner entity’s key.\n- Must exist in a many-to-one relationship (1 owner entity, many weak entities) with total participation\n- Weak entities and their relationship set are bolded","lastmodified":"2023-01-08T06:32:50.399928165Z","tags":null},"/cs186/io":{"title":"What is an I/O and why should I care?","content":"If you're taking 186, you've probably heard or experienced some variation of the following:\n - This class has a lot of I/O counting on exams\n - I/Os are weird, sometimes you can read like 5 things at the same time but it's still one I/O for some reason\n - I/O counting is tedious and boring and I will never do it after this class, so why do I need to do it??????\n\nWhile I can't guarantee that you'll ever count I/Os after this class, my hope for this article is to justify why it's necessary for understanding key database design concepts, and how the principles can be applied to real-world issues in query optimization. It's like learning how to multiply numbers by hand when we have calculators: although functionally obsolete, we still need to understand the mechanics before we can start taking the shortcut.\n\n\n## The Problem\n\nThe biggest issue that most database solutions solve is that **disks are slow and memory is fast, but we don't have enough memory to store everything we need.**\n\nThink about trying to process hundreds of gigabytes of data on your computer (very common for applications like machine learning), even though you only have something like 16GB of RAM.\n\nIf we want our operations to complete in a reasonable amount of time, we'll want to do as much as possible within RAM. Most of the algorithms you've likely encountered in 61B assume that we have an *infinite* amount of fast memory, and that all operations took the same amount of time, since we only cared about asymptotic runtimes.\n\nHowever, in the real world, reading something from memory could be hundreds of thousands of times faster than reading something from disk. As such, when evaluating the runtime of an algorithm we only care about how long it takes to transfer something from disk into memory so that we can access it. This basic unit of time is what we call the Input-Output cost, or \"an I/O\" for short.\n\n\n## Definition of an I/O\n\n\u003e [!abstract] Summary\n\u003e \n\u003e **An I/O is a single read or write event where one page of data is transferred between memory and disk.**\n\nA **page** is the basic (\"atomic\") unit of information on disk: since it's more efficient than reading one byte at a time, nearly all modern hard drives and SSDs have some hard-coded block size in their firmware, such that any data accessed from them will always be delivered one block at a time. These blocks are then converted into pages, which also have a fixed size, in the operating system.\n\nFor the purposes of this class, **\"block\" and \"page\" can be used interchangably.** In most contexts we will refer to it as a page. However, in general, [there is a difference.](https://stackoverflow.com/questions/22137555/whats-the-difference-between-page-and-block-in-operating-system)\n\nA very important thing to note down is that **there is no such thing as a fractional I/O.** If we need to read 4.1 pages' worth of data, it will really take 5 I/Os since the last page needs to be read in its entirety.\n\n## Applications\n\nThe primary application for evaluating I/O cost is for [query optimization](\u003ccs186/07 Query Optimization\u003e), where we need to decide which operation is the most efficient out of several possibilities. \n\n**Why can't we just use asymptotic runtime?**\n - We're dealing with known, finite input sizes: We may decide to use a different algorithm for a 1000-row table versus a 10000000-row table, even if the algorithm for the former has poorer asymptotic properties.\n - The difference between a $O(n)$ algorithm and an $O(2n)$ algorithm can get extremely noticable when we have millions (or billions) of rows in a table.\n - The runtime depends on the data that we put in. For instance, certain types of joins (like Sort-Merge Join) perform poorly when we have large amounts of duplicate data. Since we already know (or can approximate) what data we have, we can use this knowledge to estimate how much of the data will need to be accessed to complete the operation, which may be dramatically better or worse than its average runtime.\n\n## Practice\n\nHere are some basic I/O problems to test your understanding, before moving onto applying it to more involved algorithms. **For all of the below problems, assume that one page can store 5 (five) integers, and that all available space on a page will be used before a new page is created.**\n\n{{\u003c tabs \"q1\" \u003e}}\n{{\u003c tab \"Q1\" \u003e}}\nAlice has an array `[1, 2, 3, 4, 5]` stored on disk. She changes the `3` into a `6`, then writes the updated array back to disk. How many I/Os did this operation incur? \n{{\u003c /tab \u003e}}\n{{\u003c tab \"Q1 Solution\" \u003e}}\n**2 I/Os**. One to read the entire array (since it's on the same page), and one to write the entire array back.\n{{\u003c /tab \u003e}}\n{{\u003c /tabs \u003e}}\n\n{{\u003c tabs \"q2\" \u003e}}\n{{\u003c tab \"Q2\" \u003e}}\nBob has an array `[10, 7, 9, 8, 6]` stored on disk. He performs an in-place Insertion Sort and reads the result, but does not save it. How many I/Os did this operation incur? \n{{\u003c /tab \u003e}}\n{{\u003c tab \"Q2 Solution\" \u003e}}\n**1 I/O**, which is incurred when Bob reads the entire array from disk. No I/Os are incurred for actually performing the sort, since all of the swapping operations are done in memory which do not count.\n{{\u003c /tab \u003e}}\n{{\u003c /tabs \u003e}}\n\n{{\u003c tabs \"q3\" \u003e}}\n{{\u003c tab \"Q3\" \u003e}}\n**Challenge:** How many I/Os does it take to insert a page at the end of a linked list of $N$ pages? Assume the pointer to the next page is stored within each page (so no additional data is needed).\n{{\u003c /tab \u003e}}\n{{\u003c tab \"Q3 Solution\" \u003e}}\n**N + 2 I/Os.** To find the end of the linked list, we first need to read all $N$ pages. Then, we need to perform 2 writes- one to update the next pointer of the now second-to-last page, and one to write the new page.\n{{\u003c /tab \u003e}}\n{{\u003c /tabs \u003e}}","lastmodified":"2023-01-08T06:32:50.4839301Z","tags":null},"/cs61b/":{"title":"Welcome to CS61B!","content":"\n# Welcome to my CS61B Guide!\n\nThis is a **non-comprehensive** guide to data structures written with an intention to supplement learning and reviewing of Berkeley's [CS61B](https://inst.eecs.berkeley.edu/\\~cs61b) material. Main topics include:\n\n* Object oriented programming basics\n* Abstract data types\n* Asymptotics and runtime analysis\n* Sorting algorithms\n* Search algorithms\n* And some more miscellaneous topics thrown in!\n\nThis guide is written to be as easy to follow and digestible as possible😀I've included lots of diagrams, practice problems, and more intuitive explanations instead of the more straightforward approach most textbooks use. **This isn't a replacement for lectures and other course content.** You probably need to look at those first, and come here if something isn't sticking!\n\n### The 61B Concept Map\n\n![](\u003cimg/assets/image (6).png\u003e)\n\n\n\n## Who is this for?\n\nMostly me; making unnecessarily detailed guides is my goto method of making sure I understand everything😁 But you are welcome to use it as well for reviewing for 61B exams, touching up on data structures knowledge, or whatever you want!\n\n**Basic programming knowledge gained from** [**CS61A**](https://cs61a.org/) **or equivalent is assumed.** [[notes/cs61a|Click here]] for my notes for that!\n\n## How to use this guide\n\nAgain, I will emphasize that this **isn't a textbook.** While I try to be as comprehensive as possible, I'm sure I missed plenty of important concepts or assume you know others. Please [open an issue](https://github.com/64bitpandas/notes/issues) if you think something's wrong!\n\nThis content was ported from my original [61B Notes](https://cs61b.bencuan.me), so you may see some strange formatting here and there. Again, please create an issue if you spot anything overly egregious.\n\n\u003e [!note] Content Note\n\u003e\n\u003e For more difficult topics, I'll put a warning like this at the top of the page with links to prerequisites or supporting topics!\n\n\nThere are also plenty of practice problems to try out! Here's an non-exhaustive list of pages with those if you are mostly interested in them and not the conceptual content.\n\n* [Access Control](oop/access-control.md#practice)\n* [Dynamic Method Selection](oop/dynamic-method-selection.md)\n* [Generic Types](oop/generics.md#generic-subtypes)\n* [Asymptotics Practice](asymptotics/asymptotics-practice.md)\n\n\n\n## How to contribute\n\nSee the [contributing guide](/contributing) for more details!\n\nThe pages that could be most improved (in no particular order) are [Union Find (Disjoint Sets)](abstract-data-types/union-find-disjoint-sets.md), [Stacks and Queues](abstract-data-types/collections/stacks-and-queues.md), [Linked Lists](abstract-data-types/collections/linked-lists.md), [Sets](abstract-data-types/collections/sets.md), [Sorting](algorithms/sorting.md), [Searching](algorithms/searching.md), [Binary Search](algorithms/searching/binary-search.md), [Shortest Paths](algorithms/shortest-paths/), and [Exceptions](misc-topics/exceptions.md). Feel free to add whatever content you like (explanations, examples, practice problems, memes...) to these!\n\nAdditionally, there are plenty of topics (Regex, Testing, Files/Scanners, Ranges, GUI just to name a few) that aren't currently covered in this guide. If you want to add one of these topics, please create an issue first but it will almost certainly be approved.\n\n### Credits\n\n* [Ben Cuan](https://github.com/64bitpandas)\n* [Arin Chang](https://github.com/arinchang)\n","lastmodified":"2023-01-08T06:32:50.4839301Z","tags":null},"/cs61b/abstract-data-types/":{"title":"","content":"","lastmodified":"2023-01-08T06:32:50.4839301Z","tags":null},"/cs61b/abstract-data-types/binary-trees/":{"title":"","content":"\n\u003e [!quote] \u0026nbsp;\n\u003e\n\u003e \"The most important concept in computer science\" - Josh Hug\n\n## Humble Origins\n\nLinked lists are great, but we can do better! Let's try **rearranging the pointers** in an interesting way.\n\nInstead of starting at one end of the list, let's set our first pointer at the **middle** of the list!\n\n![](\u003c../../img/assets/image (69).png\u003e)\n\nNow, let's make new pointers going to the **center** of each **sublist** on the left and right of the center.\n\n![](\u003c../../img/assets/image (70).png\u003e)\n\nLet's do it again!\n\n![](\u003c../../img/assets/image (71).png\u003e)\n\nWould ya look at that, we've got a **tree**! 🌲\n\n![🌲🌲🌲🌲🌲](\u003c../../img/assets/image (73).png\u003e)\n\n## Types of Trees\n\nRight now, we can determine some properties that all trees have.\n\n* All trees have a **root node**.\n* All nodes can point to **child nodes.** Or, if they don't have any children, they are **leaves.**\n\nWe can add more and more constraints to our tree to make them more useful!\n\nFirst, let's add the constraint that **node can only have 2 or fewer children** to create a **binary tree.**\n\nThen, let's **ensure our tree is sorted** to create a **binary search tree.** A tree is sorted if it has these properties:\n\n* Every value in the **left subtree** of a node is **less than** the node's value.\n* Every value in the **right subtree** of a node is **greater than** the node's value.\n* Values are **transitive** - there are **no duplicate values**.\n* The tree is **complete** - it is possible to **compare any two values** in the tree and say that one is **either less than or greater than the other.**\n* The tree is **antisymmetric** - If `p \u003c q` is true and `q \u003c r` is also true, then it must follow that `p \u003c r`.\n\n## Tree Operations\n\nThere are **three important operations** that trees should support: **find, insert, and delete.**\n\n### **Find**\n\nFinding a value in a tree uses the [Binary Search](../../algorithms/searching/binary-search.md) algorithm. \n\n\n### Insert\n\nThe insert algorithm is **very similar to binary search.** Here are the steps to take:\n\n* Search for the item. **If it's found, then do nothing** since the value is already in the tree.\n* If it's not found (search would return null in this case), then create a node and put it where it should be found. If using recursion, this last step is already done- all we need to do is return a new node!\n\nHere's the algorithm:\n\n```java\npublic BST insert(BST T, Key sk) {\n    if (T == null) {\n        // Create new leaf with given key. Different from search\n        return new BST(sk, null, null); \n    }\n    if (sk.equals(T.key)) {\n        return T;\n    } else if (sk \u003c T.key) {\n        T.left = find(T.left, sk); // Different from search\n    } else {\n        T.right = find(T.right, sk); // Different from search\n    }\n}\n```\n\n### Delete\n\nThis one's a bit trickier because we need to make sure that the new tree still **preserves the binary search tree structure.** That means that we might have to shuffle around nodes after the deletion. There are **three cases:**\n\nA) The node to delete is a **leaf**. This is an easy case- just remove that node and you're done!\n\n![Deleting a leaf.](\u003c../../img/assets/image (64).png\u003e)\n\nB) The node to delete has **one child.** In this case, **swap** the node with its child, then **delete the node.**\n\n![Deleting a node with one child.](\u003c../../img/assets/image (65).png\u003e)\n\nC) The node to delete has **two children.** This one's trickier, because we still need to preserve the tree structure! In this case, we have to **traverse the node's children** to find the **next biggest value** and swap that up to replace the old node.\n\n![Deleting a node with two children.](\u003c../../img/assets/image (66).png\u003e)\n\n## Asymptotic Analysis\n\nA binary tree can be **bushy** or **spindly.** These two cases have dramatically different performances!\n\n**Bushy** trees are the **best case.** A tree is bushy if **every parent has exactly 2 children.**\n\nA bushy tree is guaranteed to have a height of $\\Theta(\\log(n))$ which means that the runtimes for adding and searching will also be $\\Theta(\\log(n))$ .\n\n**Spindly** trees are the **worst case.** A tree is spindly if **every parent has exactly 1 child.** This makes the tree essentially just a linked list!\n\nA spindly tree has a height of  $\\Theta(n)$ which means that the runtimes for adding and searching will also be $\\Theta(n)$ .\n\n![](\u003c../../img/assets/image (67).png\u003e)\n\nIn [Balanced BSTs](balanced-search-structures.md), we will explore ways of guaranteeing that a tree is bushy!\n\n## Limits of Trees\n\nWhile trees are extremely versatile and fantastic for a variety of applications, trees have some limitations that make it difficult to use in some situations.\n\n* **All items in a tree need to be comparable.** We can't construct a binary tree out of categorical data, like models of cars, for example.\n* **The data must be hierarchical.** If data can be traversed through in multiple ways, or forms loops, [Graphs](../graphs.md) are probably better.\n* **The best case runtime is** $\\Theta(\\log(n))$ . This might seem good, but other data structures like [Tries](tries.md) and [Hash Tables](../hashing.md) can be as good as **** $\\Theta(1)$ !\n\n## Tree Traversals\n\nCheck out these pages for information on how to go through each element of a tree!\n\n[Depth First Search](../../algorithms/searching/depth-first-search-dfs.md)\n\n[Breadth First Search](../../algorithms/searching/breadth-first-search-bfs.md)\n\n","lastmodified":"2023-01-08T06:32:50.4839301Z","tags":null},"/cs61b/abstract-data-types/binary-trees/balanced-search-structures":{"title":"","content":"\n\u003e [!info] Content Note\n\u003e\n\u003e Please read [Binary Trees](./) before continuing!\n\n\n**Balanced Binary Search Trees** are an even more specific subcategory of binary trees that have an important property: **they are always bushy.**\n\n## B Trees (2-4 Trees)\n\n**The basic idea:** Nodes can hold multiple values now! When nodes have too many values, we will split it.\n\nA **2-4 tree** is named such because each parent can have **2 to 4 children.** Another constraint we will put on is a **limit on the** **number of items allowed in a single node**, so that we can guarantee that searching a single node will always be $\\Theta(n).$\n\n### **Adding Values to a B-Tree**\n\nAdding values to a B Tree can be a bit tricky because we need to make sure all the properties are still followed. Here are some example scenarios:\n\nIf a node already has 2 or more children, place the new value in one of its existing children.\n\n![](\u003c../../img/assets/image (81).png\u003e)\n\nIf a node is full (reaches the limit), we must **split the node** by **moving one value up to the parent** and **creating another child node**. Here, we'll use a limit of **3**.\n\n![](\u003c../../img/assets/image (82).png\u003e)\n\n### Properties of B Trees\n\n* Searching in a single node is **constant runtime** since the limit is a constant.\n* All leaves must be the **same distance** from the root.\n* A non-leaf node with **k** items must have **k+1** children.\n* The height of a B tree is guaranteed to be $\\Theta(\\log(n))$ because it is bushy.\n\n## Red-Black Trees and Tree Rotation\n\n**The basic idea:** Let's try to represent B trees in a **binary tree format.** That means that every parent can only have 2 children! In order to do this, we'll **add an extra color property** to each node.\n\n**Black nodes** are just like any normal binary tree node, but **Red nodes** represent the nodes in B Trees that have **more than one value.** For example, let's convert the B Tree we were working with before into a RB Tree.\n\n![](\u003c../../img/assets/image (83).png\u003e)\n\nIn order to make our lives easier, we'll restrict our Red Black trees into **left leaning red black trees** which can **only have red nodes on the left.**\n\n### **Tree Rotation**\n\nIn order to ensure that adding new nodes won't break the Red Black Tree structure, we will use a concept called **tree rotation** which swaps around nodes. There are two rotations, a **left rotation** and a **right rotation,** which move a child node up to replace its parent. For example, a **left rotation** moves the **right node up and left** to replace the parent.\n\nA \"left rotation on 7\" looks like this:\n\n![](\u003c../../img/assets/image (84).png\u003e)\n\nNotice that the **8** gets moved to be a **right child** of **7** after the rotation! This is necessary to preserve the binary tree structure.\n\nA \"right rotation on 7\" looks like this:\n\n![](\u003c../../img/assets/image (85).png\u003e)\n\nHere, the **6** gets moved to be a **left child** of **7.**\n\nIf you want to see how these rotations can be implemented into the `insert` algorithm, [try the homework](https://inst.eecs.berkeley.edu/\\~cs61b/sp20/materials/hw/hw8/index.html) on implementing a LLRB Tree! Below is a brief outline on how insert works:\n\n* **Always add values to a leaf node as a red node first.** Follow normal sorted binary tree rules.\n* If the link is leaning right, rotate the tree to make it left leaning.\n* If a node already has a red link to the left, temporarily add it to the right also as a red link.\n  * Then, flip the color of all links connected to the node (if previously black, turn red; if previously red, turn black)\n  * Might need to fix right-leaning red nodes that are created as a result\n* If a node has red links to both parent and child, rotate it such that it becomes the above case, and then handle that case like you did before.\n\n### Properties of Red Black Trees\n\nLike B Trees, Red Black Trees have some important properties that allow them to be easily distinguishable.\n\n* Red Black trees have a **one-to-one correspondence** with B trees. That means for every Red Black tree, there is exactly one B Tree that represents the same connections. This also means that a Red Black Tree will have the same runtimes as their corresponding B Trees. (Take a linear algebra course to learn more about isomorphisms 🙂 )\n* **Every node must have the same number of black nodes in between itself and the root.** This might be a bit surprising at first, but remember that their corresponding B Tree is always bushy, and red links mean a multi-value node in a B Tree.\n","lastmodified":"2023-01-08T06:32:50.4839301Z","tags":null},"/cs61b/abstract-data-types/binary-trees/heaps":{"title":"","content":"## What are Heaps?\n\nA heap is a **specific order of storing data,** often in a list. Heaps are very similar to binary trees, but have some differences:\n\n* Unlike trees, heaps **only care about the root node.** Usually, the root node is either the **largest** or **smallest** value in the heap (corresponding with max-heaps and min-heaps), and we don't care too much about the rest.\n* Every element in the heap must be **larger than all its children** (in a max-heap) or **smaller than all its children** (in a min-heap). This is known as the **heap property.**\n\nWhen stored in a list, there is an **important rule** to figure out how to identify parent nodes and their children: **a node's parent has an index equal to half of that node's index.** More specifically, `parentIndex = nodeIndex / 2` where `/` has floor-division properties.\n\n![Converting a heapified list into a min-heap diagram.](\u003c../../img/assets/image (60).png\u003e)\n\n## The Heapify Algorithm\n\nThe most important heap algorithm is **heapify**, which converts any non-heap list into a heap. This algorithm is vital to most heap functions like insert or remove, since these functions often break the heap structure before fixing it with heapify.\n\n**Here's how it works:**\\\n****(This example is an excerpt from my [Sorting Guide](https://docs.google.com/document/d/1dUfzdh5V3okrwFbB9o0PgtEBaLHyCqJFwpQWyQ53IeU/edit). The example provided is a max-heap \\[5,6,2,4,1].)\n\nStart with the element in the middle of the array (which is the root of the heap).\n\n![](\u003c../../img/assets/image (61).png\u003e)\n\nIf the root is smaller than either of its children (larger for a min-heap), swap it with its largest child (smallest for a max-heap).\n\n![](\u003c../../img/assets/image (62).png\u003e)\n\n\n\nIf the root was swapped, recursively call heapify on the new position. Otherwise, stop recursion.\n\nAfter heapify is complete, it should look like this:\n\n![](\u003c../../img/assets/image (63).png\u003e)\n\n## Practical Applications\n\n[Lab 9](https://inst.eecs.berkeley.edu/\\~cs61b/sp20/materials/lab/lab9/index.html) is a fantastic resource for practicing heap implementations and working with the algorithms that are needed to work with heaps (like heapify, insert, remove). Since this lab goes into plenty of detail about how each of these algorithms work, I won't explain them too much here.\n\nHeap sort relies on the heap structure to provide consistent nlogn sorting! I have more information about this on page 11 in my [sorting guide](https://docs.google.com/document/d/1dUfzdh5V3okrwFbB9o0PgtEBaLHyCqJFwpQWyQ53IeU/edit).\n","lastmodified":"2023-01-08T06:32:50.4839301Z","tags":null},"/cs61b/abstract-data-types/binary-trees/tries":{"title":"","content":"## Main Ideas\n\nA **trie** is a specific implementation of a set and is short for **retrieval tree.**\n\nIt only works on sets with a **finite alphabet**, like digits or ASCII characters, for example. The idea is that each node can act like an **array containing all characters in the alphabet** and we can just access the branches super fast by indexing into them!\n\nTries are fantastic for searching to see if a word is contained in a set. Here's an example:\n\n![This trie contains the words 'batcat', 'batman', and 'banana'.](\u003c../../img/assets/image (74).png\u003e)\n\nThis is great because it makes the `add()` and `contains()` functions run in $\\Theta(1)$ time! Additionally, it makes special string operations like prefix matching or autocomplete very efficient.\n\nWe can improve this data structure a lot- for instance, we can condense the leaves to reduce the number of nodes like this:\n\n![](\u003c../../img/assets/image (75).png\u003e)\n\nI won't go into too much detail on how to optimize it further, or how to implement the actual functions efficiently, but hopefully you'll have a good sense of how to do it yourself after learning about concepts like [Hashing and Hash Tables](../hashing.md) or [Sets](../collections/sets.md) etc.\n","lastmodified":"2023-01-08T06:32:50.4839301Z","tags":null},"/cs61b/abstract-data-types/collections/":{"title":"","content":"\n![An overview of all the Collections in Java.](\u003c../../img/assets/image (3).png\u003e)\n\n**Collection** is a Java interface for common abstract data types that store multiple items in them.\n\n## Sub-Interfaces\n\n* **Lists** are indexed sequences with duplication. The two most common types are [**ArrayLists**](arrays.md#array-lists)  and [**Linked Lists**](linked-lists.md).\n* [**Sets**](sets.md)  are non-indexed sequences with no duplication. (That is, every value in a set is unique.)\n* **Maps** are key-value pairs. See [Hashing and Hash Tables](../hashing.md) for a description on one common map implementation, the HashMap. All keys in a map must be unique, but values can be duplicated.\n* [**Stacks and Queues**](stacks-and-queues.md)  are two ordered collections that have two core behaviors:\n  * push(T x): puts x on the top.\n  * pop(): Removes the first item. (See the stacks and queues page for more information.)\n\n## Common Functions\n\n* **Membership tests** `contains()` and `containsAll()` that can determine whether or not an element is in the collection.\n* `size()` to get the number of items in the collection.\n* `isEmpty()` returns true if there is nothing in the collection.\n* `iterator()` returns an Iterator object to go through all the values in the collection.\n* `toArray()` converts the collection to a standard Java array.\n* **Optional** functions that aren't implemented in the interface: `add, addAll, clear, remove, removeAll, retainAll (intersection)`\n  * Throws `UnsupportedOperationException` if not implemented.\n","lastmodified":"2023-01-08T06:32:50.4839301Z","tags":null},"/cs61b/abstract-data-types/collections/arrays":{"title":"","content":"\n\u003e [!info] Content Note\n\u003e\n\u003e This page assumes prior knowledge of Python lists from CS61A or equivalent.\n\nArrays are a very popular data structure that stores an indexed list of data. \n\n\n![An artistic interpretation of a new int\\[5\\] {6, 1, 2, 3, 99};](\u003c../../img/assets/image (37).png\u003e)\n\n## Properties\n\n* **Fixed length:** after instantiation, the length of an array cannot be changed.\n* Every value in array is the **same type** and holds the **same amount of bits** in memory.\n* **Zero-indexed.** That means `arr[0]` returns the first value, and `arr[arr.length]` is out of bounds.\n* **No methods.** Helper methods from other libraries (like `System.arraycopy`) need to be used to manipulate arrays.\n* **Retrieval is independent of size** and takes constant time regardless of how big arrays are.\n\n## Using Arrays in Java\n\n**Instantiation:**\n\n* `int[] a = {1, 2, 3, 4, 5};` assigns values.\n* `int b = new int[3];` creates array of provided length populated with default values.\n\n**Copying**\n\n* Simply assigning `int[] c = b` will copy the **pointer** to array b! Not the values! See [Java Objects](../../oop/objects.md) for a discussion on why this is significant.\n* Use `System.arraycopy(source, start, target, startTarget, amountToCopy)` to **shallow copy** the values (or pointers) in the array. That is, if an array is holding **reference types,** only the pointers will be copied and not the actual values of the reference objects being held.\n* `System.arraycopy(b, 0, x, 3, 2)` is equivalent to `x[3:5] = b[0:2]` in Python.\n\n**Multidimensional Arrays**\n\n* `int[][] 2d = new int[4][4];`or `int[][] 2d = new int[][] {{1}, {2, 3}, {4, 5, 6}};`will create **arrays inside of an array.** This is useful for storing matrices, coordinate maps, or any other multidimensional data!\n\n**Generic Arrays**\n\n* Arrays of generic objects are NOT allowed! Use ArrayLists instead.\n* Or, this workaround can be used:`Type[] items = (Type[]) new Object[length]`\n\n## Array Lists\n\nJava has another built-in type that uses an array under the hood, which is the `ArrayList`. Here's how ArrayLists are different from normal arrays:\n\n* ArrayLists can resize arbitrarily. (They use something similar to the array case study in the [Amortization](../../asymptotics/amortization.md#what-if-we-doubled-the-size-instead-of-adding-one) page.\n* ArrayLists use [Generic Types](../../oop/generics.md) and therefore do not support primitive types like `int`.\n* ArrayLists have all behaviors expected from the [Collections](./) interface.\n","lastmodified":"2023-01-08T06:32:50.4839301Z","tags":null},"/cs61b/abstract-data-types/collections/linked-lists":{"title":"","content":"\n\u003e [!info] Content Note\n\u003e\n\u003e This page assumes prior knowledge of linked lists from CS61A or equivalent. I'll assume you have already worked with basic singly linked lists before.\n\n\nThe linked list is an extremely common recursive data structure that allows storage and access of an arbitrary amount of data.\n\n## Feature List of an Effective Linked List\n\n1. **Rebranding**- represents Node as an individual object rather than having one monolithic List type.\n2. **Bureacracy:** Create an abstraction barrier so that users do not need to know how methods or Nodes work, only how to call them.\n3. **Access Control:** Data cannot be accessed directly to prevent dangerous behavior; only the provided methods are used.\n4. **Nested Class:** Nodes are nested within the List object since other classes do not need it.\n5. **Caching:** The size of the list is incremented every time a node is added, so running size() is O(1) and traversal is not needed.\n6. **Generalizing:** A **sentinel node** represents an empty list and remains the first node of the list. When getFirst() is called, the second node is actually returned (since the first node is always the sentinel).\n7. **Doubly Linked:** Nodes have both first and last pointers for even faster traversal.\n8. **Circular list:** Sentinel last pointer points to the last value in the node, allowing for fast removeLast().\n\n![An illustration of an effective linked list.](\u003c../../img/assets/image (36).png\u003e)\n\n## Method List\n\n| Method                                                               | Description                                      | Optimal Runtime |\n| -------------------------------------------------------------------- | ------------------------------------------------ | --------------- |\n| \u003cp\u003e\u003ccode\u003eaddFirst(T x)\u003c/code\u003e\u003c/p\u003e\u003cp\u003e\u003ccode\u003eaddLast(T x)\u003c/code\u003e\u003c/p\u003e    | Adds a node to the front/back of the list.       | $\\Theta(1)$   |\n| \u003cp\u003e\u003ccode\u003egetFirst()\u003c/code\u003e\u003c/p\u003e\u003cp\u003e\u003ccode\u003egetLast()\u003c/code\u003e\u003c/p\u003e          | Gets the node at the front/back of the list.     | $\\Theta(1)$   |\n| \u003cp\u003e\u003ccode\u003eremoveFirst()\u003c/code\u003e\u003c/p\u003e\u003cp\u003e\u003ccode\u003eremoveLast()\u003c/code\u003e\u003c/p\u003e    | Removes the node at the front/back of the list.  | $\\Theta(1)$   |\n| `size()`                                                             | Returns the number of nodes in the list.         | $\\Theta(1)$   |\n| `contains(T x)`                                                      | Returns true if the list contains element `x`.   | $\\Theta(n)$   |\n| \u003cp\u003e\u003ccode\u003eadd(T x, int pos)\u003c/code\u003e\u003c/p\u003e\u003cp\u003e\u003ccode\u003eremove(T x)\u003c/code\u003e\u003c/p\u003e | Adds/remove an element at an arbitrary location. | $\\Theta(n)$   |\n\n## Limitation: Arbitrary Retrieval\n\nYou may have noticed in the chart above that it takes $\\Theta(n)$  time to retrieve arbitrary values from the list. This will get really slow if the list is large! If arbitrary values need to be accessed frequently, [Arrays](arrays.md) are much better.\n\n## The Java List Interface\n\nJava has a built-in `LinkedList` class so you don't have to implement it yourself! Read up on the [official docs](https://docs.oracle.com/javase/8/docs/api/java/util/LinkedList.html/) to learn more about the specific methods and behaviors provided.\n\n","lastmodified":"2023-01-08T06:32:50.4839301Z","tags":null},"/cs61b/abstract-data-types/collections/sets":{"title":"","content":"\n\u003e [!warning] Warning\n\u003e\n\u003e This page is incomplete. [help make it better!](contributing.md)\n\n## Basics\n\nA Set stores a collection of values with **no duplicates.** Sets have no inherent order, so you can't rely on expecting any value to come before any other value when iterating through them.\n\nSome set functions include:\n\n* `add(T x)`\n* `contains(T x)`\n* `size()`\n\n## ArraySet\n\nAn ArraySet is an array-based solution to a set implementation.\n\n* Objects get added to an array that gets [resized](../../asymptotics/amortization.md) when it's too full.\n* In order to allow for iteration, we can use one of two methods:\n  *   One method is to use **iterators** which work very similarly to Python iterators:\n\n      ```java\n      Iterator\u003cInteger\u003e seer = set.iterator();\n      while (seer.hasNext()) {\n        System.out.println(seer.next());\n      }\n      ```\n  * Another method is to implement the `Iterator` and `Iterable` interface.\n    * Iterator must implement `hasNext()` and `next()` methods\n    * Requires generic type\n    * Iterable must implement `iterator()` method which returns the Iterable object\n    * Allows usage of for/foreach loops\n","lastmodified":"2023-01-08T06:32:50.487930192Z","tags":null},"/cs61b/abstract-data-types/collections/stacks-and-queues":{"title":"","content":"\nStacks and queues are two very common data structures used for a variety of applications from [CPU processes](https://www.tutorialspoint.com/operating\\_system/os\\_processes.htm) to [finding shortest paths using Dijkstra's Algorithm](../../algorithms/shortest-paths/dijkstras-algorithm.md). Fundamentally, they are very similar in structure and **only differ by the order in which items are popped from them**.\n\n## Pushing and Popping\n\n### Pushing\n\nAdding an item to a stack or queue is called **pushing**. This will either put the item on the **top** of a stack or in the **back** of a queue.\n\nYou can think of a stack like a pile of pizza boxes- the one on the top is the first one you have to take off if you need one!\n\n![](\u003c../../img/assets/image (52).png\u003e)\n\nOn the other hand, you can think of a queue like lining up for a ride at Disneyland. The first person who gets in line will get to go first, and the last person who gets in will go last. (Of course, we all know people cut and stuff- see [Priority Queues](stacks-and-queues.md#priority-queues) to see how this is better handled.)\n\n![those lines tho](\u003c../../img/assets/image (54).png\u003e)\n\n### Popping\n\nTaking an item out of a stack or queue is called **popping.**\n\nStacks are **last in, first out (LIFO).** That means the last item that you put in will be the first item that gets popped.\n\nQueues are **first in, first out (FIFO).** That means that the first item that you put in will be the first item that gets popped.\n\n## Priority Queues\n\nLet's say you bought a VIP pass and get to cut to the front of the line for your favorite Disneyland ride! Well, a normal Queue won't be able to model this behavior since it puts everything in the back by default.\n\nA priority queue will solve this design need by introducing a new **priority** **tracking system** for each item in the queue! **If an item has a lower priority number, it will get to go first.**\n\n![gotta grab those fastpasses yEEt 🎟](\u003c../../img/assets/image (53).png\u003e)\n","lastmodified":"2023-01-08T06:32:50.487930192Z","tags":null},"/cs61b/abstract-data-types/comparables-and-comparators":{"title":"","content":"\n## What is it?\n\nA **Comparable** is a **generic type** that allows standardized comparisons between objects.\n\nIn other words, anything that has a `compareTo()` method can be a Comparable!\n\nMany Java libraries already use Comparable without you knowing! Some of the more well-known ones are `Collection` and `String`.\n\n### CompareTo can't return anything you want!\n\nThere are some very specific properties CompareTo needs to have! Usually, we take them for granted but might forget about them when making our own.\n\n* If `x` and `y` are the **same object**, `y.compareTo(x)` must return **0.**\n* `x.compareTo(y)` must return the **negative** of `y.compareTo(x)`. (if one throws an error, the other must too!)\n* If `x` and `y` are the **same object**, `x.compareTo(z)` must **equal** `y.compareTo(z)` **for all z.**\n\n### Defining a Comparable subclass\n\n```java\npublic class MyComparable implements Comparable\u003cMyComparable\u003e {\n    public int foo;\n    ...\n\n    /** Instance method that has nothing to do with comparable */\n    public void doSomething() {\n        ...\n    }\n\n    /** Comparable method used to compare objects of this type */\n    public int compareTo(Object o) {\n        MyComparable mc = (MyComparable) o;\n        return ...\n    }\n}\n```\n\n## **Comparators**\n\nComparators are used instead of higher order functions in order to provide a **callback** function to methods. One example of where it is used commonly is `Collections.sort`. You can pass in a comparator here to change how items are sorted- for example, you could sort `Person` objects by their `height` variable.\n\n**The interface is as follows:**\n\n```java\npublic interface Comparable\u003cT\u003e {\n int compare(T o1, T o2);\n}\n```\n\n### How is it different from Comparables???\n\nComparable is used to compare **itself** to **other objects**; a Comparator compares **two other objects but not itself.**\n\n","lastmodified":"2023-01-08T06:32:50.487930192Z","tags":null},"/cs61b/abstract-data-types/graphs":{"title":"","content":"\n## Introduction\n\nGraphs are simply a collection of **vertices** connected by **edges.** They're very similar to trees, but are much more versatile and don't require hierarchical relationships like trees do.\n\n![A very simple graph.](\u003c../img/assets/image (55).png\u003e)\n\nFor most purposes, we will be working with **simple graphs** that follow two rules:\n\n* There are **no loops** (a connection of a node to itself).\n* There are **no parallel edges** (two edges that connect the same two vertices).\n\n![Don't make these graphs pls. Keep life simple!](\u003c../img/assets/image (56).png\u003e)\n\n## Graph Properties\n\nGraphs can be described by some properties that they could have. Here are the important ones:\n\nA graph can be **directed** if edges are arrows and have a direction, or **undirected** if you can cross edges in any direction.\n\nA graph is **cyclic** if the edges form a loop, or **acyclic** if there are no loops (like in a tree).\n\n![Direction vs. Cycles](\u003c../img/assets/image (57).png\u003e)\n\nGraphs can have **edge labels** if edges are numbered (great for distances). They can also have **vertex weights** if vertices are numbered (great for priorities or costs).\n\n![Edge labels vs. Weights](\u003c../img/assets/image (58).png\u003e)\n\nGraphs are **connected** if all of the vertices are connected with edges, such that you can freely move from one vertex to any other vertex.\n\n![](\u003c../img/assets/image (59).png\u003e)\n\n## Graph Queries\n\nHere are some cool things you can do with graphs:\n\n* Is there a path between two vertices? (s-t path)\n* What is the shortest route between two vertices? (shortest s-t path)\n* Are there cycles? (cycle detection)\n* Can you visit each vertex/edge exactly once? (Euler tour / Hamilton tour)\n* Is a graph connected? (connectivity problem)\n* Is a vertex that disconnects the graph when removed? (single point of failure / biconnectivity)\n* Are two graphs isomorphic?\n* Can a graph be drawn with no crossing edges? (planarity)\n\n## More on Graphs\n\n[Depth First Search (DFS)](../algorithms/searching/depth-first-search-dfs.md), [Breadth First Search (BFS)](../algorithms/searching/breadth-first-search-bfs.md), [Minimum Spanning Trees](../algorithms/minimum-spanning-trees/), [Shortest Paths](../algorithms/shortest-paths/), [Dijkstra's Algorithm](../algorithms/shortest-paths/dijkstras-algorithm.md), [A\\* Search](../algorithms/shortest-paths/a-search.md), [Prim's Algorithm](../algorithms/minimum-spanning-trees/prims-algorithm.md), and [Kruskal's Algorithm](../algorithms/minimum-spanning-trees/kruskals-algorithm.md) all rely on graphs. Graphs are a super useful concept!!!\n","lastmodified":"2023-01-08T06:32:50.487930192Z","tags":null},"/cs61b/abstract-data-types/hashing":{"title":"Hashing and Hash Tables","content":"\n## Data Indexed Sets: Introduction\n\nSo far, we've explored a whole bunch of ways we can store items, but they aren't really optimized for general searching. What if we could get searching in $\\Theta(1)$ time??? Wouldn't that be nice!\n\nLet's try something: **putting all of our data in a massive array.** Let's say that we know all our data falls into the range from 0 to 10,000 and make an array of 10,000 length to hold stuff.\n\n![](\u003c../img/assets/image (86).png\u003e)\n\nHere, it doesn't matter what index each item is stored in- if we want to get \"eecs\" which is stored at key 3, it will be as instantly accessible as \"haas\" which is all the way in 9998.\n\nOf course, this has a **major design flaw** that you can probably see right away. **It takes way too much memory!**\n\n## Hash Codes\n\nLet's figure out a way to get around the issue of space, but still not lose our awesome constant-time property. One way we can do this is to represent each item with a **hash code** and store them into the index with that hash code.\n\nFor instance, let's use the **first letter of a word** as the hash code. We have just turned a nearly infinite space of possibilities into something that can be stored in just **26** **buckets.**\n\n![](\u003c../img/assets/image (87).png\u003e)\n\nWhile this solution is great, it still has another **major drawback**, which can be illustrated with this example:\n\n![](\u003c../img/assets/image (88).png\u003e)\n\nIn the worst case, this just turns back into a **linked list!** That means the runtime just went from O(1) to O(n), and that's no good.\n\n## Good Hash Codes\n\nIf we can somehow create a \"good\" hash code, we can prevent things like the example above from happening because there shouldn't be a clear pattern in what buckets different objects go to. More specifically, a good hash code:\n\n* Ensures that two objects that are **equal** have the **same hash code.**\n* Ensures that **no distinguishable pattern** can be made out of hash codes from different objects.\n* Returns a **wide variety** of hash codes (not just putting everything into a single bucket, for example).\n\nLuckily, Java already handles hash code generation for us using the `hashCode()` function in the Object class. This function returns an **integer** that can be used to create good hash tables.\n\n## Dynamic Resizing\n\nLet's add another feature to our hash table: **dynamic resizing.** This means that the number of buckets will increase proportionally to the number of items in the set.\n\nOne fairly simple way to do this with a numerical hash code is to mod the hash code by the number of buckets to get which bucket an item is stored in. For example, if a item has hash code `129382981` and we have `10` buckets, then we put it in bucket `1`, or `129382981 % 10`.\n\nIn order to do this, we'll choose a **load ratio** at which to resize. This load ratio is calculated as `N/M`, where N is the number of items and M is the number of buckets. For example, a load ratio of 2 will mean the table resizes when, on average, each bucket has 2 items in it.\n\nWhen resizing, we must **recompute all the hash codes** so that we can balance out all of the buckets again.\n\nThis has some cool runtime implications that are closely related to [Amortization](../asymptotics/amortization.md). Like what happened in the dynamically resizing array, resizing hash tables like this is also a $\\Theta(1)$ operation. Nice!\n\n## Java Hash Tables\n\nIn Java, hash tables are used in the data structures `HashSet` and `HashMap` which are the most popular implementation of sets and maps.\n\nThese two implementations provide **fantastic performance** and **don't require values to be comparable** like trees do.\n\nHowever, they have a drawback that must be considered: **objects cannot be modified after they are put into the hash table.** This is because mutating an object will change its hash code, which means that the object will be lost forever since its bucket doesn't match the current hash code!\n\nIf the built-in hash code generator isn't what is needed (like you want two objects to be equal if they have the same size, for instance), you can override the `hashCode()` method. **Be careful when doing this** because `hashCode()` relies on `equals()` to find which bucket objects are in! So, if hashCode is overridden, it is highly recommended to override equals as well to ensure that they are compatible.\n","lastmodified":"2023-01-08T06:32:50.487930192Z","tags":null},"/cs61b/abstract-data-types/union-find-disjoint-sets":{"title":"Union Find (Disjoint Sets)","content":"\n# Union Find (Disjoint Sets)\n\n\u003e [!info] Content Note\n\u003e\n\u003e This is not a complete entry, because I feel like existing course materials already cover this in an extremely intuitive manner. See[ lab 14](https://inst.eecs.berkeley.edu/~cs61b/sp20/materials/lab/lab14/index.html) for an guide on how to implement your own Union Find structure!\n\nThe Union Find data structure is a way of representing a bunch of nodes that are connected to each other in subsets. It's used in [Kruskal's Algorithm](../algorithms/minimum-spanning-trees/kruskals-algorithm.md) among other things.\n\nUnion Find is named as such because it supports two functions, **find** (which returns the group that a value is contained in), and **union** (which connects two values to form a single group).\n\nUnion Find tracks each set with an ID, typically **the value of the root of each set.** In the sections below, we'll discuss how to add an item to a set, as well as figure out which set an existing item is in.\n\n## Union\n\nIn order to **join two values together,** we need to use the **union** function. Let's see what it does visually:\n\n![Calling union(1,2).](\u003c../img/assets/image (78).png\u003e)\n\nThere are lots of ways to represent this behavior. One possible method is to keep an **array of parent values** corresponding to each actual value. In the example above, for instance, we can choose 1 as our parent and make 2 fall under that. Let's see how this might work:\n\n![Parents list.](\u003c../img/assets/image (79).png\u003e)\n\nNow, let's say we call `union(3,2)`. We can just set the parent of 3 to 2, as to create a structure like this:\n\n![union(1,2) followed by union(3,2)](\u003c../img/assets/image (80).png\u003e)\n\nThis looks a lot like a tree!\n\nYou might have noticed that this looks like a **spindly tree** though, which is bad for runtime! Perhaps we can convert it to the equivalent of a bushy tree- the union function can be made much more efficient using tricks such as WeightedQuickUnion and Path Compression. Watch [this playlist](https://www.youtube.com/watch?v=JNa8BRRs8L4\\\u0026list=PL8FaHk7qbOD59HbdZE3x52KOhJJS54BlT\\\u0026index=1) for more information!\n\n## Find\n\nFirst, let's explore how to implement an efficient way to **find which set a value is in.** Using the union function from above, we can do this pretty easily with this simple algorithm:\n\n* If the parent is 0, simply return the value.\n* If the parent is not 0, return the result of calling the function on the parent value.\n\nIf we follow this algorithm on the example in the Union section, we can see that calling `find(3)` will go to `2`, then finally to `1` and return `1`.\n","lastmodified":"2023-01-08T06:32:50.487930192Z","tags":null},"/cs61b/algorithms/":{"title":"","content":"","lastmodified":"2023-01-08T06:32:50.487930192Z","tags":null},"/cs61b/algorithms/minimax":{"title":"Minimax Algorithm","content":"\n## Game Trees\n\nThe Minimax algorithm is often used for making AI's for turn-based games. It relies on the use of a type of **game tree,** which maps out all of the possible moves that players can make.\n\nIn the tree, there are two types of nodes: **maximizing nodes** and **minimizing nodes.** The max-nodes represent **you**- you want to make your position as advantageous as possible (maximizing your score). The min-nodes represent **your opponent-** they want to make you do as poorly as possible (minimizing your score).\n\nThe scores themselves are generated using a **heuristic function** that assesses the current game state and returns a number based on which player has an advantage, and to what extent. **This heuristic is totally up to you to figure out and has very few constraints.** There are a couple rules, however:\n\n* Heuristic functions must return **positive values** if you're doing better than your opponent, and **negative values** if your opponent is doing better.\n* Heuristic functions must return the **maximum value** for a state in which you won, and the **minimum value** for a state in which your opponent won.\n\nIn most games, you and your opponent will take turns, so each layer will alternate node type, like this:\n\n![](\u003c../img/assets/image (99).png\u003e)\n\nIn most games, this tree will spiral out of control because there are far too many nodes to possibly analyze (maybe even an infinite number)! Therefore, we need to set a **depth** to stop searching and compute a heuristic. For example, if the depth is **3**, it'll look something like this:\n\n![](\u003c../img/assets/image (100).png\u003e)\n\nNow that the tree has bottomed out at the heuristic layer, we can start going back up to figure out which move we should make! The rules are simple: **min-nodes take the smallest of the values** while **max-nodes take the largest of the values.** Here's the first layer, for example:\n\n![](\u003c../img/assets/image (101).png\u003e)\n\nHere's the entire tree filled out:\n\n![](\u003c../img/assets/image (102).png\u003e)\n\nAnd here's the minimax algorithm in pseudocode format:\n\n```python\ndef minimax_value(s: MinimaxNode):\n    if is_terminal(s):\n        return s.value\n    elif s.player == Maximizing:\n        return max(minimax_value(c) for c in s.children)\n    elif s.player == Minimizing:\n        return min(minimax_value(c) for c in s.children)\n```\n\n## **Alpha-Beta Pruning**\n\nWe can make our tree **even more efficient** by simply ignoring all of the branches that will lead to results that will **never be chosen.** Here, we'll assume that **both players play optimally** (choose the best move for their particular node).\n\nIn the example above, we can see that the 7 on the right will **never need to be visited** because we **already know that 5 will be chosen.**\n\nIn order to do this, we'll introduce two additional parameters, **alpha** and **beta.** Here are the rules:\n\n* **Alpha** starts out as **negative infinity** and is set by **max nodes** to their current value.\n* **Beta** starts out as **positive infinity** and is set by **min nodes** to their current value.\n* A node **passes its alpha and beta values** onto its children.\n* If **alpha is greater than beta (**$\\alpha \\ge \\beta$**),** the branch will be **pruned** (no longer visited).\n\nHere are the step-by-step instructions on how to process a node:\n\n1. Copy the alpha and beta values from the parent node. (If no parent node exists, then initialize alpha to negative infinity and beta to positive infinity.\n2. For every branch:\n   1. Recursively process the branch.\n   2. Update the current alpha/beta value depending on the value of the branch after processing. (MaxNodes can only update alpha, and MinNodes can only update beta.)\n   3. If $\\alpha \\ge \\beta$**,** then prune the rest of the branches (stop this loop).\n3. Set the value of this node to the biggest (MaxNode) or smallest (MinNode) value seen.\n\nThis is a pretty tough concept to grasp, and that's why I've illustrated how it works below. Read on!\n\n## A Story of Minimax Nodes: An Intuitive Understanding\n\nMinimax is quite difficult to understand just by studying its rules. In order to really know what's going on, we need to know why we have all of these rules and what everything represents. Here's how I think about it:\n\n![](\u003c../img/assets/image (24).png\u003e)\n\n![](\u003c../img/assets/image (25).png\u003e)\n\n![](\u003c../img/assets/image (27).png\u003e)\n\n![](\u003c../img/assets/image (28).png\u003e)\n\n![](\u003c../img/assets/image (29).png\u003e)\n\n![](\u003c../img/assets/image (30).png\u003e)\n\n![](\u003c../img/assets/image (31).png\u003e)\n\n![](\u003c../img/assets/image (33).png\u003e)\n\n_NOTE: The 5's in the above image should all be 7's. This will be corrected soon (tm)._\n\n\n\n## Practice Problems\n\n{{\u003c tabs \"minimax-q1\" \u003e}}\n{{\u003c tab \"Question 1\" \u003e}}\nHere's a tree. Figure out:\n\n* What values each of the nodes report\n* Which branches are pruned\n* The alpha and beta values at each visited node\n\n![](\u003c../img/assets/image (34).png\u003e)\n{{\u003c /tab \u003e}}\n\n{{% tab \"Q1 Answer\" %}}\nHere's my answer! The green arrows denote the order in which the nodes are visited. Note that the branches are pruned every time **alpha is greater than beta.**\n\n![](\u003c../img/assets/image (35).png\u003e)\n{{\u003c /tab \u003e}}\n{{\u003c /tabs \u003e}}\n\nThis was just an ordinary problem and **might not be enough to ensure that you fully understand minimax trees**! Here are some checks you can do to ensure that your understanding is strong:\n\n* Figure out what the tree returns and prunes intuitively _without_ finding any alpha or beta values.\n* Make your own minimax tree problem like the one above and solve it. Are you confident in your answer (since no answer key exists)?\n* Make a minimax tree that's missing some values, and try to find all possible values that fit in there such that the branch will become pruned.\n* Implement the minimax algorithm in Java.\n","lastmodified":"2023-01-08T06:32:50.487930192Z","tags":null},"/cs61b/algorithms/minimum-spanning-trees/":{"title":"","content":"\n## Spanning Tree Definition\n\nA **spanning tree** $T$ is a subgraph of a graph $G$ where $T$:\n\n* Is connected (there's a path to every vertex)\n* Is acyclic (no cycles)\n* Includes every vertex (spanning property)\n\n**Notice:** the first two properties defines a tree structure, and the last property makes the tree spanning.\n\nA **minimum spanning tree** is a spanning tree with minimum total edge weight.\n\nExample: I want to connect an entire town with wiring and would like to find the optimal wiring connection that connects everyone but uses the least wire.\n\n## MST vs. Shortest Path Tree\n\nIn contrast to a shortest path tree, which is essentially the solution tree to running Dijkstra’s with root node = source vertex, a MST has no source. However, it is possible for the MST to be the same as the SPT.\n\nWe can think of the MST as a global property for the entire graph, as opposed to SPT which depends on which node is the source node.\n\nIf the edges of the graph are not unique, there’s a chance that the MST is not unique.\n\n## Cuts Property\n\n* A **cut** is defined as assigning the nodes in a graph into two sets.\n* A **crossing edge** is an edge that connects two nodes that are in different sets\n* The smallest crossing edge is the crossing edge with smallest weight\n\nThe **Cut Property** states that the smallest crossing edge is always going to be in the MST, no matter how the cut is made.\n\n![](\u003c../../img/assets/image (109).png\u003e)","lastmodified":"2023-01-08T06:32:50.487930192Z","tags":null},"/cs61b/algorithms/minimum-spanning-trees/kruskals-algorithm":{"title":"Kruskal's Algorithm","content":"\n\u003e [!info] Content Note\n\u003e\n\u003e Before reading, review [Minimum Spanning Trees](./) and [Union Find (Disjoint Sets)](../../abstract-data-types/union-find-disjoint-sets.md) as they both make Kruskal's algorithm possible!\n\n\n## Conceptual Overview\n\nKruskal's algorithm is another optimal way to construct a **minimum spanning tree**. It's benefits are that it is conceptually very simple, and easy to implement. The idea is that first we sort all the edges of the graph in order of increasing weight. Then, add the smallest edge to the MST we are constructing unless this creates a cycle in the MST. Repeat until V - 1 edges total.\n\n## Detailed Breakdown\n\nIn order to optimally check if adding an edge to our MST creates a cycle, we will use a **WeightedQuickUnion** object. (See [Union Find (Disjoint Sets)](../../abstract-data-types/union-find-disjoint-sets.md) for a recap on what this is.) This is used because checking if a cycle exists using a WeightedUnionFind object boils down to one `isConnected()` call, which we know takes $\\Theta(\\log(N))$.\n\nTo run the algorithm, we start by adding all the edges into a [PriorityQueue](../../abstract-data-types/collections/stacks-and-queues.md). This gives us our edges in sorted order. Now, we iterate through the PriorityQueue by removing the edge with highest priority, checking if adding it forms a cycle, and adding it to our MST if it doesn't form a cycle.\n\nLet's see an example of Kruskal's Algorithm in action!\n\nHere, we start with a simple graph and have sorted all of its edges into a priority queue.\n\n![](\u003c../../img/assets/image (103).png\u003e)\n\nSince the edge **DE** is the shortest, we'll add that to our UnionFind first. In the process, we'll **remove DE from the priority queue.**\n\n![](\u003c../../img/assets/image (104).png\u003e)\n\nWe'll do the same thing with the next shortest path, **DC.**\n\n![](\u003c../../img/assets/image (105).png\u003e)\n\nNow, let's move on to **AB.** Notice that this time, connecting A and B creates another **disjoint set!** Unlike Prim's Algorithm, Kruskal's Algorithm does not guarantee that a solution will form a tree structure until the very end.\n\n![](\u003c../../img/assets/image (106).png\u003e)\n\nNow, let's connect **BC.**\n\n![](\u003c../../img/assets/image (107).png\u003e)\n\nSince **CE** and **BD** would both form cycles if connected, **we are done 😄** Here's the final tree:\n\n![](\u003c../../img/assets/image (108).png\u003e)\n\n## PseudoCode\n\n```java\npublic class Kruskals() {\n\n    public Kruskals() {\n        PQ edges = new PriorityQueue\u003c\u003e();\n        ArrayList\u003cEdge\u003e mst = new ArrayList\u003c\u003e();\n    }\n\n    public void doKruskals(Graph G) {\n        for (e : G.edges()) {\n            PQ.add(e);\n        }\n        WeightedQU uf = new WeightedQU(G.V());\n        Edge e = PQ.removeSmallest();\n        int v = e.from();\n        int w = e.to();\n        if (!uf.isConnected(v, w)) {\n            uf.union(v, w);\n            mst.add(e);\n        }\n\n    }\n}\n```\n\n## Runtime Analysis\n\nLeft as an exercise to the reader 😉\n\n{% hint style=\"info\" %}\nSomeone's been reading too much [LADR](https://www.springer.com/gp/book/9783319110790)...\\\n(The answer is $\\Theta(E\\log(E))$by the way. Try to convince yourself why!)\n{% endhint %}\n","lastmodified":"2023-01-08T06:32:50.487930192Z","tags":null},"/cs61b/algorithms/minimum-spanning-trees/prims-algorithm":{"title":"Prim's Algorithm","content":"\n\u003e [!info] Content Note\n\u003e\n\u003e Before reading, review [Minimum Spanning Trees](./), as that is the foundation of Prim's algorithm!\n\n## Conceptual Overview\n\nPrim's algorithm is an optimal way to construct a **minimum spanning tree**. It basically starts from an arbitrary vertex, then considers all its immediate neighbors and picks the edge with smallest weight to be part of the MST. **Note:** this creates a cut in the graph, where the two nodes in the MST being constructed are in one set, and every other vertex of the graph is in another set.\n\nNow, the edges taken into consideration include all immediate neighbors of every node in the MST. Add the edge that has the smallest weight to the MST. Repeat until every vertex has been visited. The result is an MST for the graph.\n\n## Detailed Breakdown\n\nThe way Prim's algorithm is usually implemented is via [PriorityQueue](../../abstract-data-types/collections/stacks-and-queues.md), `edgeTo` array, and` distTo` array. You will soon see its similarities to [Dijkstra's](../shortest-paths/dijkstras-algorithm.md).\n\nFirst, insert all vertices into the PriorityQueue, storing vertices in order of **distance from MST**. Then, remove vertex with highest priority in the PriorityQueue and relax its edges. In each of these iterations, the distTo and edgeTo arrays will be updated for each vertex v if the **weight of the edge is smaller than the current value in distTo\\[v]**. In other words, only update if the distance from the MST to the vertex is the best seen so far. This is a very important point, and is one of the subtleties that makes Prim's algorithm fundamentally different from Dijkstra's.\n\n## Useful Properties/Invariants\n\nThe MST under construction is **always connected.**\n\n## Pseudocode\n\n```java\npublic class Prims() {\n\n    public Prims() {\n        PQ = new PriorityQueue\u003c\u003e();\n        edgeTo = new Edge[numVertices];\n        distTo = new Dist[numVertices];\n        marked = new boolean[numVertices];\n    }\n\n    public void doPrims() {\n        PQ.add(sourceVertex, 0);\n        for(v : allOtherVertices) {\n            PQ.add(v, INFINITY);\n        }\n        while (!PQ.isEmpty()) {\n            Vertex p = PQ.removeSmallest();\n            marked[p] = true;\n            relax(p);\n        }\n    }\n\n    public void relax(Vertex p) {\n        for (q : p.neighbors()) {\n            if (marked[q]) { continue; }\n            if (q.edgeWeight \u003c distTo[q]) {\n                distTo[q] = q.edgeWeigth;\n                edgeTo[q] = p;\n                PQ.changePriority(q, distTo[q]);\n            }\n        }\n    }\n}\n```\n\nLooking at this pseudocode, the resemblance to Dijkstra's makes them seem nearly identical. But hopefully you've read the conceptual overviews first, and you understand the remarkable subtlety that leads to two very fundamentally different algorithms.\n\n## Runtime Analysis\n\nThis is the same as for Dijkstra's Algorithm.\n\n**Unsimplified:**\n\n$\n\\theta(V * log(V) + V * log(V) + E * log(V))\n$\n\n**Simplified:**\n\n$\n\\theta(E * log(V))\n$\n\n**Explanation:**\n\n* each add operation to PQ takes log(V), and perform this V times\n* each removeFirst operation to PQ takes log(V) and perform this V times\n* each change priority operation to PQ takes log(V), perform this at most as many times as there are edges\n* everything else = O(1)\n* usually, there are more or equal edges compared to the number of vertices.\n\n## Demo\n\n[https://docs.google.com/presentation/d/1GPizbySYMsUhnXSXKvbqV4UhPCvrt750MiqPPgU-eCY/edit#slide=id.g9a60b2f52\\_0\\_0](https://docs.google.com/presentation/d/1GPizbySYMsUhnXSXKvbqV4UhPCvrt750MiqPPgU-eCY/edit#slide=id.g9a60b2f52\\_0\\_0)\n","lastmodified":"2023-01-08T06:32:50.487930192Z","tags":null},"/cs61b/algorithms/searching/":{"title":"","content":"\nThis section will cover some ways to find values in a set.\n\n{{\u003c section \u003e}}","lastmodified":"2023-01-08T06:32:50.487930192Z","tags":null},"/cs61b/algorithms/searching/binary-search":{"title":"","content":"# Binary Search\n\nBinary search is a way of finding a specific node in a tree. It only works on [binary trees](../../abstract-data-types/binary-trees/) due to its helpful sorted property. It simply traverses the tree, moving left if the current node is too large or right if it is too small.\n\nBinary search runs in $\\Theta(\\log(n))$ time for bushy trees, which is also the number of layers in a tree.\n\n## The Algorithm\n\n```java\npublic BST find(BST T, Key sk) {\n    if (T == null) {\n        return null;\n    }\n    if (sk.equals(T.key)) {\n        return T;\n    } else if (sk \u003c T.key) {\n        return find(T.left, sk);\n    } else {\n        return find(T.right, sk);\n    }\n}\n```\n","lastmodified":"2023-01-08T06:32:50.487930192Z","tags":null},"/cs61b/algorithms/searching/breadth-first-search-bfs":{"title":"Breadth First Search (DFS)","content":"\nBreadth First Search (BFS), like [Depth First Search (DFS)](depth-first-search-dfs.md), is a method of **traversing a graph.** BFS simply traverses in a different order, but otherwise is very similar to DFS.\n\nThe main difference is that BFS **visits all children before any subgraphs.** In a tree, we call this **level order.**\n\n![](\u003c../../img/assets/image (110).png\u003e)\n\nFor the example tree above, a level order traversal would go in this order: **D B F A C E G.**\n\n## Step by Step\n\n**Let's see how we might implement BFS.**\n\nSome data structures we will need are:\n\n* A graph to traverse.\n* A queue **Q** to keep track of which nodes need to be processed next.\n* A list of booleans **marked** to keep track of which nodes were already visited.\n* (Optional) **edgeTo** and **distTo** to keep track of information that might be useful for other applications (like [Dijkstra's Algorithm](../shortest-paths/dijkstras-algorithm.md)).\n\nFirst, let's start with a vertex in the graph by marking it and adding it to the queue.\n\n![](\u003c../../img/assets/image (111).png\u003e)\n\nThe next step is to **remove A from the queue** and **add its children** (B and C) **to the queue.** Also, we need to **mark all of the children.**\n\n![](\u003c../../img/assets/image (112).png\u003e)\n\nNext, we'll move onto the **next item on the queue** (B). We'll do the same thing that we did with A: remove B, mark all its children, and add its children to the queue. **Since C is already marked, we do not add it to the queue again.**\n\n![](\u003c../../img/assets/image (113).png\u003e)\n\nNow, we'll move on to the next item on the queue, C, and do the same thing. Again, we won't add C or A because they are both marked.\n\n![](\u003c../../img/assets/image (114).png\u003e)\n\nFinally, we'll visit the two remaining nodes in the queue, D and E. Since all of the nodes are marked now, there aren't any other nodes to visit.\n\n","lastmodified":"2023-01-08T06:32:50.487930192Z","tags":null},"/cs61b/algorithms/searching/depth-first-search-dfs":{"title":"Depth First Search (DFS)","content":"\n## Depth First Traversal\n\nBefore we move on to searching, let's talk about **traversing. Traversal** is the act of **visiting nodes in a specific order.** This can be done either in trees or in graphs.\n\nFor trees in particular, there are **three main ways** to traverse.\n\n![The example tree we will use for traversal illustrations.](\u003c../../img/assets/image (89).png\u003e)\n\nThe first way is **inorder** traversal, which visits **all left children**, then **the node itself,** then **all right children.** The end result should be that the nodes were visited in **sorted order.**\n\nThe second way is **preorder** traversal, which visits **the node itself first,** then **all left children,** then **all right children.** This method is useful for applications such as printing a directory tree structure.\n\nThe third way is **postorder** traversal, which visits **all left children,** then **all right children,** then **finally the node itself.** This method is useful for when operations need to be done on all children before the result can be read in the node, for instance getting the sizes of all items in the folder.\n\nHere are some pseudocodey algorithms for tree traversals.\n\n```java\n// INORDER will print A B C D E F G\nvoid inOrder(Node x) {\n    if (x == null) return;\n    inOrder(x.left);\n    print(x);\n    inOrder(x.right);\n}\n\n// PREORDER will print D B A C F E G\nvoid preOrder(Node x) {\n    if (x == null) return;\n    print(x);\n    preOrder(x.left);\n    preOrder(x.right);\n}\n\n// PREORDER will print A C B E G F D\nvoid postOrder(Node x) {\n    if (x == null) return;\n    preOrder(x.left);\n    preOrder(x.right);\n    print(x);\n}\n```\n\n## Depth First Search in Graphs\n\nGraphs are a little more complicated to traverse due to the fact that they could have **cycles** in them, unlike trees. This means that we need to **keep track of all the nodes already visited** and add to that list whenever we encounter a new node.\n\nDepth First Search is great for determining if everything in a graph is connected.\n\nHere's an outline of how this might go:\n\n* Keep an array of 'marks' (true if node has been visited) and, optionally, an edgeTo array that will automatically keep track of how to get to each connected node from a source node\n* When each vertex is visited:\n  * Mark the vertex\n  * For each adjacent unmarked vertex:\n    * Set edgeTo of that vertex equal to this current vertex\n    * Call the recursive method on that vertex\n\nLike trees, DFS can be done **inorder, preorder, or postorder.** It's nearly identical behavior to trees, with the addition of the marks array.\n","lastmodified":"2023-01-08T06:32:50.487930192Z","tags":null},"/cs61b/algorithms/shortest-paths/":{"title":"","content":"\nWe've seen that Breadth-First Search can help us find the shortest path in an unweighted graph, where the shortest path was just defined to be the fewest number of edges traveled along a path. In the following shortest-paths algorithms, we will discover how we can generalize the breadth-first traversal to find the path with the lowest total cost, where the cost is determined by different weights on the edges.","lastmodified":"2023-01-08T06:32:50.487930192Z","tags":null},"/cs61b/algorithms/shortest-paths/a-search":{"title":"A* Search","content":"\n\u003e [!info] Content Note\n\u003e\n\u003e In order to understand A\\*, you'll need to be comfortable [Dijkstra's Algorithm](dijkstras-algorithm.md) first!\n\n\n## A\\* Algorithm\n\nThe A\\* Search Algorithm is **incredibly similar to Dijkstra's Algorithm** with one addition: a **heuristic function.**\n\nThis heuristic function calculates weights of a path **from a vertex to a goal vertex.** This way, we can help bias our algorithm in the right direction so that it doesn’t make a bunch of bad moves.\n\nThis has an important implication: **not all vertices get visited.** The algorithm only cares about finding the best path to the goal, and not any other vertex (assuming we design our heuristic well).\n\nThe **order** that the vertices get visited is lowest **distance + heuristic**. This is basically the same as Dijkstra's, just with that added heuristic term.\n\n## What's a good heuristic?\n\nHeuristic functions can be really tricky to design, since there isn't much to go off of.\n\n**A good heuristic has these two properties:**\n\n* **Admissible** - heuristic of each vertex returns a cost that is \u003c= the true cost/distance i.e. h(A) \u003c= cost(A, goal)\n* **Consistent** - difference between heuristics of two vertices \u003c= true cost between them i.e. h(A) - h(B) \u003c= cost(A, B)\n\n## **Want more?**\n\n[Here's a cool demo!](https://docs.google.com/presentation/d/177bRUTdCa60fjExdr9eO04NHm0MRfPtCzvEup1iMccM/edit#slide=id.g369665031c\\_0\\_350)\n\n","lastmodified":"2023-01-08T06:32:50.487930192Z","tags":null},"/cs61b/algorithms/shortest-paths/dijkstras-algorithm":{"title":"Dijkstra's Algorithm","content":"\n\u003e [!info] Content Note\n\u003e\n\u003e Before continuing, make sure you're comfortable with [Graphs](../../abstract-data-types/graphs.md), [Stacks and Queues](../../abstract-data-types/collections/stacks-and-queues.md), and [Shortest Paths](./).\n\n\n## One sentence overview\n\nVisit vertices in order of best-known distance from source; on visit, relax every edge from the visited vertex.\n\n## Detailed Breakdown\n\nDjikstras uses a **PriorityQueue** to maintain the path with lowest cost from the starting node to every other node, an **edgeTo** array to keep track of the best known predecessor for each vertex, and a **distTo** array to keep track of the best known distance from the source vertex to every other vertex.\n\n**Relaxing** the edges of a vertex v just refers to the process of updating edgeTo\\[n] for each neighbor n to v.\n\nYou'll see in the pseudocode and diagrams below that succesful relaxation only occurs when the edge connecting the vertex being visited to one of its neighbors yields a smaller total distance than the current shortest path to that neighboring vertex that the algorithm has seen.\n\nNow, here's a demonstration on how it works! Let's start out with this graph:\n\n![](\u003c../../img/assets/image (92).png\u003e)\n\nWe'll start at node A and try to figure out the shortest path from A to each node. Since we have no idea how far each node is, we'll take the conservative guess that everything is infinitely far away ♾😎\n\nThe first thing we have to do is update A's adjacent nodes, which are **B** and **D**. Since there's only one known path to each, it shouldn't be too hard to see why we need to update the values below. One thing to note is that the priority queue **sorts the vertices by the distance it takes to get there.**\n\n![](\u003c../../img/assets/image (93).png\u003e)\n\nNow, we have a choice to move on to either **B** or **D**. Since B has a **shorter distance,** we'll move on to that first. When we move on, we have to **remove that value from the priority queue** and **update all of its neighbors.** Here, we see that going from **B to D** is **shorter** than **A to D**, so we have to **update distTo AND edgeTo of D** to reflect this new, shorter path. **This process** (updating each adjacent node) **is called relaxing the edges of a node.**\n\n![](\u003c../../img/assets/image (94).png\u003e)\n\nNow, let's move onto **D** since it has the next shortest path. Again, we **remove D from the priority queue** and **relax C** since we found a shorter path.\n\n![](\u003c../../img/assets/image (95).png\u003e)\n\nFinally, we'll move onto **C** as that has the next shortest path in the priority queue. This will reveal our final node, **E**.\n\n![](\u003c../../img/assets/image (96).png\u003e)\n\nSince **the priority queue is now empty,** our search is done! 😄 Here's what the final solution looks like **in a tree form**:\n\n![Dijkstra's Algorithm ALWAYS produces a solution in a tree format.](\u003c../../img/assets/image (98).png\u003e)\n\nIt's a very spindly tree indeed, but hopefully it demonstrates that the result is **acyclic**.\n\n## Properties of Dijkstra's Algorithm\n\n**Dijkstra's Algorithm has some invariants (things that must always be true):**\n\n1. edgeTo\\[v] always contains best known predecessor for v\n2. distTo\\[v] contains best known distance from source to v\n3. PQ contains all unvisited vertices in order of distTo\n\n**Additionally, there are some properties that are good to know:**\n\n* always visits vertices **in order of total distance from source**\n* relaxation always **fails on edges to visited vertices**\n* guarantees to work optimally **as long as** **edges are all non-negative**\n* solution always creates a **tree form.**\n* can think of as **union of shortest paths to all vertices**\n* **edges in solution tree always has V-1 edges**, where V = the number of vertices. This is because every vertex in the tree except the root should have **exactly one input.**\n\n## Pseudocode\n\n```java\npublic Class Djikstra() {\n\n    public Djikstra() {\n        PQ = new PriorityQueue\u003c\u003e();\n        distTo = new Distance[numVertices];\n        edgeTo = new Edge[numVertices];\n    }\n\n    public void doDijkstras(Vertex sourceVertex) {\n        PQ.add(sourceVertex, 0);\n        for(v : allOtherVertices) {\n            PQ.add(v, INFINITY);\n        }\n        while (!PQ.isEmpty()) {\n            Vertex p = PQ.removeSmallest();\n            relax(p);\n        }\n    }\n    // Relaxes all edges of p\n    void relax(Vertex p) {\n        for (q : p.neighbors()) {\n            if (distTo[p] + q.edgeWeight \u003c distTo[q]) {\n                distTo[q] = distTo[p] + q.edgeWeight;\n                edgeTo[q] = p;\n                PQ.changePriority(q, distTo[q]);\n            }\n        }\n    }\n}\n```\n\n## Runtime Analysis\n\n**Unsimplified:**\n\n$\n\\theta(V * log(V) + V * log(V) + E * log(V))\n$\n\n**Simplified:**\n\n$\n\\theta(E * log(V))\n$\n\n**Explanation:**\n\n* each add operation to PQ takes log(V), and perform this V times\n* each removeFirst operation to PQ takes log(V) and perform this V times\n* each change priority operation to PQ takes log(V), perform this at most as many times as there are edges\n* everything else = O(1)\n* usually, there are more or equal edges compared to the number of vertices.\n","lastmodified":"2023-01-08T06:32:50.487930192Z","tags":null},"/cs61b/algorithms/sorting":{"title":"Sorting","content":"\n\u003e [!important] Sorting Guide\n\u003e\n\u003e For more information about specific sorting algorithms covered in 61B, see my [guide on sorting](https://docs.google.com/document/d/1dUfzdh5V3okrwFbB9o0PgtEBaLHyCqJFwpQWyQ53IeU/edit) that covers all of the sorts in far greater detail 🙂\n\n## Why sort?\n\n* It makes searching for a specific value much faster (e.g. binary search). Typically, searching through an unsorted list requires a full scan ($\\Theta(N)$​ runtime).\n* It's easy to see if two items in list are equal: just compare to see if any neighboring values are the same.\n\n## Properties of a Sorting Algorithm\n\nA sorting algorithm changes a sequence based on a **total order.** A total order is:\n\n* **Total:** All items can be compared with one another\n* **Reflexive:** An item can be compared to itself\n* **Antisymmetric:** x \u003c= y AND y \u003c= x IFF y == x\n* **Transitive:** If x \u003c= y and y \u003c= z, then x must be \u003c= z\n\nA sorting algorithm could be **stable** if it does not change relative order of equivalent entries. For example, if Bob and I both owned Toyota Corollas, and the list of cars were sorted by model, if Bob's car came before mine originally it must also come before mine in the sorted list after a stable sort.\n\n\n\n## Sorting Algorithm Classifications\n\n* **Internal sort:** Keeps all data in primary memory\n* vs. **External sort:** Processes data in batches, then merges them together at the end\n* **Comparison-based sort:** The only thing we know about keys are their relative orders\n* **Radix sort:** Uses information other than keys\n* **Insertion sort:** Insert items at their appropriate positions one at a time\n* **Selection sort:** Chooses items and places them in order\n\n## Sorting in Java\n\nJava automatically chooses the best sorting algorithm for a given list if you call the `Arrays.sort` method.\n\n```java\nString[] x = new String[] {\"Vat\", \"Bat\", \"Cat\"};\n\nArrays.sort(x); // mutates x into Bat, Cat, Vat\nArrays.sort(x, Collections.reverseOrder()); // mutates x into Vat, Cat, Bat\nArrays.sort(x, 0, 2) // sorts the first two elements, leaving the rest unchanged (Cat, Vat, Bat)\n```\n\n## Inversions\n\nInversions are used as a measure for how sorted a list is. For every two elements that are swapped compared to a sorted list, we add one inversion.\n\n* As an example, if `1 2 3 4 5` is a sorted list, `1 4 3 2 5` would have one inversion (`4` and `2` are swapped).\n* 0 inversions mean a list is perfectly sorted.\n* In the worst case, a reversed list will have $(N \\cdot (N-1))/2$ inversions.\n\n## The Guide to Sorting Algorithms\n\n[A comprehensive guide to sorting algorithms, now with memes!](https://docs.google.com/document/d/1dUfzdh5V3okrwFbB9o0PgtEBaLHyCqJFwpQWyQ53IeU/edit)\n","lastmodified":"2023-01-08T06:32:50.487930192Z","tags":null},"/cs61b/asymptotics/":{"title":"","content":"","lastmodified":"2023-01-08T06:32:50.487930192Z","tags":null},"/cs61b/asymptotics/amortization":{"title":"","content":"# Amortization\n\n\u003e [!info] Content Note\n\u003e\n\u003e Please read [Asymptotic Analysis Basics](asymptotics.md) first. If you don't, none of this will make any sense!\n\n**Amortization** means **spreading out.**\n\nSometimes, an operation takes different amounts of time for different values of $n$. Rather than having to report runtimes for each different case, we can instead average all of them out and report the **amortized runtime.**\n\nThis is especially good for functions where most actions have a low cost, but a few have a high cost. We'll see an example of this further down the page!\n\n## A Case Study: Resizing Arrays\n\nAs you probably know, normal Java arrays don't resize. If we create a `new int[5]` then that array will always have a length of 5.\n\nBut what if we wanted to make an array resize itself every time it reaches capacity? (Like a `List`!) Let's see what happens when we **add one to the array size:**\n\nFirst, we have to make a new array with a new size:\n\n![](\u003c../img/assets/image (16).png\u003e)\n\nThen, we have to copy over all of the old elements over:\n\n![](\u003c../img/assets/image (17).png\u003e)\n\nFinally, we can add in the new element!\n\n![](\u003c../img/assets/image (19).png\u003e)\n\n**Let's analyze the runtime of this operation.**\n\n* A single resizing will take $\\Theta(n)$ time.\n* Adding a single element will take $\\Theta(1)$ time.\n* Together, a single operation will take $\\Theta(n+1)$ time, which simplifies into  $\\Theta(n)$ .\n* Since we're doing a n-operation n times, **the end result is a resizing function that is**$\\Theta(n^2)$. **We can do better with the power of amortization!**\n\n### **What if we doubled the size instead of adding one?**\n\n* A single resizing will take $\\Theta(2n)$ time \\_\\*\\*\\_which simplifies into $\\Theta(n)$ time.\n  * We do this every time the array hits a power of 2 (2, 4, 8, 16, 32 ...).\n* Adding a single element will take $\\Theta(1)$ time.\n  * We do this every time we add a new element, so in all we add n elements. Therefore, this is an\n    * $\\Theta(n)$operation.\n\n**Therefore, the unsimplified function is:** $\\Theta(n + (2 + 4 + 8 ... +2^i))$ where $2^i$ is the largest power of two less than n. This might not seem clear on its own, so let's rewrite it:\n\n$\n\\theta(n + (\\frac{n}{2} + \\frac{n}{4} + ... + 8 + 4 + 2))\n$\n\nIntuitively, this looks like this:\n\n![](\u003c../img/assets/image (39).png\u003e)\n\nMathematically, it looks like this:\n\n$\nn + n\\sum_{n=1}^{n}(\\frac{1}{2})^n\n$\n\nWhich simplifies to $2n$if you recall your power series properties . **Therefore, this approach is** $\\Theta(n)$ **!!**\n\n![](\u003c../img/assets/image (116).png\u003e)\n\n\n\n\n\n","lastmodified":"2023-01-08T06:32:50.487930192Z","tags":null},"/cs61b/asymptotics/asymptotics":{"title":"Asymptotic Analysis Basics","content":"\n\u003e [!info] Content Note\n\u003e\n\u003e This concept is a big reason why a strong math background is helpful for computer science, even when it's not obvious that there are connections! Make sure you're comfortable with Calculus concepts up to [power series](http://tutorial.math.lamar.edu/Classes/CalcII/PowerSeries.aspx).\n\n\n## An Abstract Introduction to Asymptotic Analysis\n\nThe term **asymptotics,** or **asymptotic analysis,** refers to the idea of **analyzing functions when their inputs get really big.** This is like the **asymptotes** you might remember learning in math classes, where functions approach a value when they get very large inputs.\n\n![](\u003c../img/assets/image (13).png\u003e)\n\nHere, we can see that $y= \\dfrac{x^3}{x^2 + 1}$ looks basically identical to $y = x$ when x gets really big. Asymptotics is all about reducing functions to their eventual behaviors exactly like this!\n\n## That's cool, but how is it useful?\n\nGraphs and functions are great and all, but at this point it's still a mystery as to how we can use these concepts for more practical uses. Now, we'll see how we can **represent programs as mathematical functions** so that we can do cool things like:\n\n* **Figure out how much time or space a program will use**\n* **Objectively tell how one program is better than another program**\n* **Choose the optimal data structures for a specific purpose**\n\nAs you can see, this concept is **absolutely fundamental** to ensuring that you write **efficient algorithms** and choose the **correct data structures.** With the power of asymptotics, you can figure out if a program will take 100 seconds or 100 years to run without actually running it!\n\n## How to Measure Programs\n\nIn order to convert your `public static void Main(String[] args)` or whatever into `y = log(x)`, we need to figure out what `x` and `y` even represent!\n\n**TLDR:** It depends, but the three most common measurements are **time, space,** and **complexity**.\n\n**Time** is almost always useful to minimize because it could mean the difference between a program being able to run on a smartphone and needing a supercomputer. Time usually increases with the **number of operations** being run. Loops and recursion will increase this metric substantially. On Linux, the `time` command can be used for measuring this.\n\n**Space** is also often nice to reduce, but has become a smaller concern now that we can get terabytes (or even petabytes) of storage pretty easily! Usually, the things that take up lots of space are **big lists** and **a very large number of individual objects.** Reducing the size of lists to hold only what you need will be very helpful for this metric!\n\nThere is another common metric, which is known as **complexity** or **computational cost.** This is a less concrete concept compared to time or space, and cannot be measured easily; however, it is highly generalized and usually easier to think about. For complexity, we can simply assign basic operations (like println, adding, absolute value) a complexity of **1** and add up how many basic operation calls there are in a program.\n\n## Simplifying Functions\n\nSince we **only care about the general shape of the function,** we can keep things as simple as possible! Here are the main rules:\n\n* **Only keep the** **fastest growing term.** For example,  $log(n) + n$ can be simplified to just $n$since $n$ grows faster out of the two terms.\n* **Remove all constants.** For example,  $5log(3n)$ can just be simplified to $log(n)$since constants don't change the overall shape of a function.\n* **Remove all other variables.** If a function is really $log(n + m)$ but we only care about n, then we can simply it into  $log(n)$.\n\nThere are two cases where we can't remove other variables and constants though, and they are:\n\n* A polynomial term $n^c$(because $n^2$grows slower than $n^3$, for example), and\n* An exponential term $c^n$(because $2^n$grows slower than $3^n$, for example).\n\n## The Big Bounds\n\nThere are **three** important types of runtime bounds that can be used to describe functions. These bounds put restrictions on how slow or fast we can expect that function to grow!\n\n**Big O** is an **upper bound** for a function growth rate. That means that **the function grows slower or the same rate as the Big O function.** For example, a valid Big O bound for $log(n) + n$ is $O(n^2)$ since $n^2$ grows at a faster rate.\n\n**Big Omega** is a **lower bound** for a function growth rate. That means that **the function grows faster or the same rate as the Big Omega function.** For example, a valid Big Omega bound for  $log(n) + n$ is $\\Omega(1)$ since $1$ (a constant) grows at a slower rate.\n\n**Big Theta** is a **middle ground** that describes the function that grows at the **same rate** as the actual function. **Big Theta only exists if there is a valid Big O that is equal to a valid Big Omega.** For example, a valid Big Theta bound for  $log(n) + n$ is $\\Theta(n)$ since $n$ grows at the same rate (log n is much slower so it adds an insignificant amount).\n\n\n\n![A comparison of the three bounds.](\u003c../img/assets/image (15).png\u003e)\n\n## Orders of Growth\n\nThere are some **common functions** that many runtimes will simply into. Here they are, from fastest to slowest:\n\n| Function              | Name        | Examples                                               |\n| --------------------- | ----------- | ------------------------------------------------------ |\n| $\\Theta(1)$         | Constant    | System.out.println, +, array accessing                 |\n| $\\Theta(\\log(n))$   | Log         | Binary search                                          |\n| $\\Theta(n)$         | Linear      | Iterating through each element of a list               |\n| $\\Theta(n\\log(n))$  | nlogn 😅    | Quicksort, merge sort                                  |\n| $\\Theta(n^2)$       | Quadratic   | Bubble sort, nested for loops                          |\n| $\\Theta(2^n)$       | Exponential | Finding all possible subsets of a list, tree recursion |\n| $\\Theta(n!)$        | Factorial   | Bogo sort, getting all permutations of a list          |\n| $\\Theta(n^n)$       | n^n 😅😅    | [Tetration](https://en.wikipedia.org/wiki/Tetration)   |\n\nDon't worry about the examples you aren't familiar with- I will go into much more detail on their respective pages.\n\n![Source: bigocheatsheat.com. Check it out, it's great!](\u003c../img/assets/image (14).png\u003e)\n\n## Asymptotic Analysis: Step by Step\n\n1. Identify the function that needs to be analyzed.\n2. Identify the parameter to use as $n$.\n3. Identify the measurement that needs to be taken. (Time, space, etc.)\n4. Generate a function that represents the complexity. If you need help with this step, [try some problems!](asymptotics-practice.md)\n5. [Simplify](asymptotics.md#simplifying-functions) the function (remove constants, smaller terms, and other variables).\n6. Select the correct bounds (O, Omega, Theta) for particular cases (best, worst, overall).\n","lastmodified":"2023-01-08T06:32:50.487930192Z","tags":null},"/cs61b/asymptotics/asymptotics-practice":{"title":"","content":"# Asymptotics Practice\n\n\u003e [!info] Content Note\n\u003e\n\u003e Make sure to review [Asymptotic Analysis Basics](asymptotics.md) before proceeding with these problems.\n\n## Introduction\n\nAsymptotics is a very intuition-based concept that often doesn't have a set algorithm for computing. The best way to get good at analyzing programs is to practice!\n\nWith that said, here are some problems of increasing difficulty for you to enjoy 😊\n\n\u003e [!hint] Read before you do the problems!\n\u003e\n\u003e For all of the below problems, assume that all undefined functions have a constant O(1) complexity.\n\n## Loops\n\n{{\u003c tabs \"q1\" \u003e}}\n{{\u003c tab \"Question 1\" \u003e}}\nWhat runtime does this function have?\n\n```java\nvoid throwHalfOfMyItems(int n) {\n    for (int i = 0; i \u003c n; i += 1) {\n        if (i % 2 == 0)\n            throwItem(n);\n    }\n}\n```\n{{\u003c /tab \u003e}}\n\n{{\u003c tab \"Q1 Answer\" \u003e}}\n$\n\\Theta(n)\n$\n\n**Explanation:** The method `throwItem()` runs `n/2` times. Using the simplification rules, we can extract the constant `1/2` to simply get `n`.\n\n![Keep the change, ya filthy animal.](\u003c../img/assets/image (40).png\u003e)\n{{\u003c /tab \u003e}}\n{{\u003c /tabs \u003e}}\n\n{{\u003c tabs \"q2a\" \u003e}}\n{{\u003c tab \"Question 2a\" \u003e}}\nWhat runtime does this function have?\n\n```java\nvoid lootShulkerBoxes(int n) {\n    for (int box = n; box \u003e 0; box -= 1) {\n        for (int stack = 0; stack \u003c n; stack += 1) {\n            for (int i = 0; i \u003c 64; i += 1) {\n                lootItem(i * stack * box);\n            }\n        }\n    }       \n}\n```\n{{\u003c /tab \u003e}}\n\n{{\u003c tab \"Q2a Answer\" \u003e}}\n$\n\\Theta(n^2)\n$\n\n**Explanation:** There are **three** nested loops in this problem. Whenever there are nested loops whose runtimes are independent of each other, we need to **multiply** the runtimes in each loop.\\\nSo, we get: $\\Theta(n * n * 64)$ which simplifies into `n^2`\n\n![That's a lot of items to loot...](\u003c../img/assets/image (41).png\u003e)\n{{\u003c /tab \u003e}}\n{{\u003c /tabs \u003e}}\n\n{{\u003c tabs \"q2b\" \u003e}}\n{{\u003c tab \"Question 2b\" \u003e}}\nI've tweaked the previous problem a little 😁 Try to spot the difference and see if it changes the runtime at all!\n\n```java\nvoid lootShulkerBoxes(int n, int stacksToLoot) {\n    for (int box = n; box \u003e 0; box -= 1) {\n        for (int stack = stacksToLoot; stack \u003e 0; stack -= 1) {\n            for (int i = 0; i \u003c 64; i += 1) {\n                lootItem(i * stack * box);\n            }\n        }\n    }       \n}\n```\n{{\u003c /tab \u003e}}\n\n{{\u003c tab \"Q2b Answer\" \u003e}}\n$\n\\Theta(n)\n$\n\n**Explanation:** Even though `stacksToLoot` is a user input, we're only concerned about finding the runtime for `n` so `stacksToLoot` can be treated like a constant! Therefore, we now have $\\Theta(n * s* 64)$ where `s = stacksToLoot` which simplifies into `n`.\n\n![ok now this is getting a bit overboard](\u003c../img/assets/image (42).png\u003e)\n{{\u003c /tab \u003e}}\n{{\u003c /tabs \u003e}}\n\n## Recursion\n\nThe following two problems are inspired by [this worksheet](https://inst.eecs.berkeley.edu/\\~cs61b/sp20/materials/disc/discussion8.pdf).\n\n{{\u003c tabs \"q3\" \u003e}}\n{{\u003c tab \"Question 3\" \u003e}}\nTREEEEEEEEE recursion 🌳🌲🌴\n\n```java\nvoid plantJungleSaplings(int n) {\n    if (n \u003e 0) {\n        for (int i = 0; i \u003c 4; i += 1) {\n            plantJungleSaplings(n - 1);\n        }\n    }\n}\n```\n{{\u003c /tab \u003e}}\n\n{{\u003c tab \"Q3 Answer\" \u003e}}\n$\n\\Theta(4^n)\n$\n\n**Explanation:** This tree recursion creates a tree with `n` layers. Each layer you go down, the number of calls multiplies by 4!\n\n![Tree diagram for method calls.](\u003c../img/assets/image (48).png\u003e)\n\nThis means that the number of calls in total will look like this:\n\n$\n\\sum_{i=1}^{n}4^i\n$\n\nAnd if you remember your power series, you'll know that this sum is equal to $4^{n+1}-1$ which simplifies into the final answer.\n\n![an image that makes you long for TreeCapacitator](\u003c../img/assets/image (49).png\u003e)\n{{\u003c /tab \u003e}}\n{{\u003c /tabs \u003e}}\n\n{{\u003c tabs \"q4\" \u003e}}\n{{\u003c tab \"Question 4\" \u003e}}\nLet's replace the 4 in the previous problem with **n** and see what insanity ensues.\n\n```java\nvoid plantCrazySaplings(int n) {\n    if (n \u003e 0) {\n        for (int i = 0; i \u003c n; i += 1) { // This line changed\n            plantCrazySaplings(n - 1);\n        }\n    }\n}\n```\n{{\u003c /tab \u003e}}\n\n{{\u003c tab \"Q4 Answer\" \u003e}}\n$\n\\Theta(n!)\n$\n\n**Explanation:** This tree recursion creates a tree with `n` layers. Each layer you go down, the number of calls multiplies by `n-1`...\n\n![What a mess (!)](\u003c../img/assets/image (51).png\u003e)\n\nThis means that the number of calls in total will look like this:\n\n$\n\\sum_{i=1}^{n} \\frac{n!}{i!}\n$\n\nSince `n!` is not dependent on `i` it can be factored out of the sum to produce this:\n\n$\nn!\\sum_{i=1}^{n} \\frac{1}{i!}\n$\n\nHey, that looks a lot like the Taylor series for $e$! Since `e` is a constant, it simply reduces to `n!`.\n{{\u003c /tab \u003e}}\n{{\u003c /tabs \u003e}}\n\n## Best and Worst Case Runtimes\n\n{{\u003c tabs \"q5\" \u003e}}\n{{\u003c tab \"Question 5\" \u003e}}\nHere's a case where the best case and worst case runtimes are different. Can you figure out what they are? (Let `n = items.length`).\n\n```java\nItem[] hopperSort(Item[] items) {\n    int n = arr.length; \n    for (int i = 1; i \u003c n; ++i) { \n        int key = items[i]; \n        int j = i - 1; \n        while (j \u003e= 0 \u0026\u0026 items[j].compareTo(key) \u003e 0) { \n            items[j + 1] = items[j]; \n            j = j - 1; \n        } \n        items[j + 1] = key; \n    } \n}\n```\n{{\u003c /tab \u003e}}\n\n{{\u003c tab \"Q5 Answer\" \u003e}}\n**Best Case:** $\\Theta(n)$ if the array is nearly sorted except for a couple values. In this case, the `while` loop will only run a small number of times, so the only loop left is the for loop.\n\n**Worst Case:** $\\Theta(n^2)$if the array is totally reversed. This will cause the `while` loop to run on the order of `O(n)` times, resulting in a nested loop.\n\n**Note:** HopperSort is literally just Insertion Sort 😎🤣\n\n![hoppers rate 64/64](\u003c../img/assets/image (45).png\u003e)\n{{\u003c /tab \u003e}}\n{{\u003c /tabs \u003e}}\n\n{{\u003c tabs \"q6\" \u003e}}\n{{\u003c tab \"Question 6\" \u003e}}\nHere's a mutual recursion problem! What are the best and worst cases for `explodeTNT(n)`? What are their runtimes?\n\n```java\nvoid explodeTNT(int n) {\n    if (n % 2 == 0) {\n        digDirts(n - 1, true);\n        digDirts(n - 2, false);\n    } else {\n        digDirts(n / 2, true);\n    }\n}\n\nvoid digDirts(int n, boolean isTNT) {\n    if (isTNT) {\n        explodeTNT(n / 2);\n    }\n    removeDirt(); // not implemented, assume O(1) runtime\n}\n```\n{{\u003c /tab \u003e}}\n\n{{\u003c tab \"Q6 Answer\" \u003e}}\n**Best Case:** $\\Theta(\\log(n))$ if n is even. This will result in n being halved every function call.\n\n**Worst Case:** $\\Theta(n)$if n is odd. See the tree below for an illustration of what happens in this case- hopefully the diagram will make it clearer as to why it's O(n).\n\n![A diagram of what happens in the worst and best cases.](\u003c../img/assets/image (46).png\u003e)\n\n![don't play with tnt, kids](\u003c../img/assets/image (47).png\u003e)\n{{\u003c /tab \u003e}}\n{{\u003c /tabs \u003e}}\n\n## Challenge Problems\n\nThese problems are quite difficult. Don't be concerned if you don't feel confident in solving them (I certainly don't).\n\n{{\u003c tabs \"q7\" \u003e}}\n{{\u003c tab \"Question 7\" \u003e}}\nA huge disaster :ooo\n\n```java\n//initially start is 0, end is arr.length.\npublic int PNH(char[] arr, int start, int end) {\n    if (end \u003c= start) {\n        return 1;\n    }\n    int counter = 0; \n    int result = 0;\n    for (int i = start; i \u003c end; i += 1) {\n        if (arr[i] == 'a') {\n            counter += 1;\n        }\n    }\n    for (int i = 0; i \u003c counter; i += 1) {\n        result += PNH(arr, start + 1, end);\n    }\n    int mid = start + (end - start) / 2;\n    return PNH(arr, start, mid) + PNH(arr, mid + 1, end);\n}\n```\n{{\u003c /tab \u003e}}\n\n{{\u003c tab \"Q7 Answer\" \u003e}}\n**Best Case:** $\\Theta(n\\log(n))$ If none of the characters in char\\[] is 'a', then each call to PNH does $\\Theta(n)$ work. Total work per layer is always N, with logN layers total.\n\n**Worst Case:** $\\Theta(n!)$ All of characters in char\\[] is 'a'. In this case, the for loop recursive calls will dominate the runtime, because it'll be the main part of the recursive tree. The interval start to end is decreased by one in the for loop recursive calls, while that interval is halved in the return statement recursive calls. This means the return statement recursive calls will reach the base case in logn levels, while the recursive calls in the for loop will take n levels. Thus, we can proceed with the same analysis as question 4.\n{{\u003c /tab \u003e}}\n{{\u003c /tabs \u003e}}\n\n{{\u003c tabs \"q8\" \u003e}}\n{{\u003c tab \"Question 8\" \u003e}}\nCongrats for making it this far! By now you should be an expert in asymptotic analysis :P Here's one last problem:\n\n```java\n//initially start is 0, end is arr.length.\npublic int lastOne(char[] arr, int start, int end) {\n    if (end \u003c= start) {\n        return 1;\n    }\n    else if (arr[start] \u003c= arr[end]) {\n        return lastOne(arr, start + 1, end - 1);\n    } else {\n        int temp = arr[start];\n        arr[start] = arr[end];\n        arr[end] = temp;\n\n        return lastOne(arr, start + 1, end) \n        + lastOne(arr, start, end - 1) \n        + lastOne(arr, start + 1, end - 1);\n    }\n}\n```\n{{\u003c /tab \u003e}}\n\n{{\u003c tab \"Q8 Answer\" \u003e}}\n**Best Case:** $\\Theta(n)$ if the else if case is always true. This will produce a tree with height n/2, where each height does constant work.\n\n**Worst Case:** $\\Theta(3^n)$ if else if case is never true. Sorry no diagram yet :((\n{{\u003c /tab \u003e}}\n{{\u003c /tabs \u003e}}\n","lastmodified":"2023-01-08T06:32:50.487930192Z","tags":null},"/cs61b/misc-topics/":{"title":"","content":"","lastmodified":"2023-01-08T06:32:50.603932863Z","tags":null},"/cs61b/misc-topics/exceptions":{"title":"","content":"# Exceptions\n\n## Basics\n\nAn **exception** occurs when something unintended occurs and the interpreter must exit.\n\nWhile this might sound like a bad thing, we can often throw our own exceptions to handle known errors or edge cases more gracefully.\n\n### Exceptions in Java\n\nIn Java, there are two types of exceptions: **checked** and **unchecked.**\n\n**Checked** exceptions are handled during compile time, and are included in the method declaration. As an example:\n\n```java\npublic void openFile() throws IOException {\n    ...\n}\n```\n\n* All children that override this method must also throw the same exceptions.\n\n**Unchecked** exceptions are not handled during compile time, and thus are thrown during runtime. All `Error` or `RuntimeException` types are unchecked; all other exceptions are checked. Some examples of unchecked exceptions are dividing by zero (`ArithmeticException`), or accessing an index that doesn't exist (`IndexOutOfBoundsException`).\n\n![Some of the more common Exception types in Java.](\u003c../img/assets/image (4).png\u003e)\n\n## Creating Custom Exceptions\n\nWe can use the `throw` keyword to create exceptions with custom error messages as follows:\n\n```java\npublic void divide(int a, int b) {\n    if (b == 0) {\n        throw new Exception(\"Error Message\");\n    } else {\n        return a / b;\n    }\n}\n```\n\nThis is often used within a `try catch` block, as such:\n\n```java\npublic void divide2() {\n    int a = 0;\n    try {\n        return 10 / 0;\n    } catch(Exception e) {\n        System.out.println(\"oops!\");\n    }\n }\n```\n\nAn alternate to custom exceptions is to simply handle exception cases. For example, we can add a check to make sure a number is not zero before running a division operation.\n\n## Try/Catch/Finally Example\n\nLet's check your understanding of exception handling!\n\n```java\nstatic String tryCatchFinally() {\n        try {\n            System.out.println(\"trying\");\n            throw new Exception();\n        } catch (Exception e) {\n            System.out.println(\"catching\");\n            return \"done catch\";\n        } finally {\n            System.out.println(\"finally\");\n        }\n    }\n```\n\n{% tabs %}\n{% tab title=\"Q1\" %}\nWhat will be printed (and in what order) when `tryCatchFinally()` is run?\n{% endtab %}\n\n{% tab title=\"Q1 Answer\" %}\nFirst, `trying` will be printed.\n\nSince an Exception is thrown, the catch block will run next, so `catching` is printed next.\n\nSince finally blocks _always_ run regardless of result, `finally` is printed last.\n{% endtab %}\n{% endtabs %}\n\n{% tabs %}\n{% tab title=\"Q2\" %}\nSuppose the same code were run, but without the `catch` block. What would this code do?\n\n```java\nstatic String tryFinally() {\n        try {\n            System.out.println(\"trying\");\n            throw new Exception();\n        } finally {\n            System.out.println(\"finally\");\n        }\n}\n```\n{% endtab %}\n\n{% tab title=\"Q2 Answer\" %}\nIf the try block throws an uncaught Exception (i.e. if catch block does not exist or catch block does not handle the type of Exception that is thrown in the try block), Java halts execution of the try block, **executes the finally block**, then raises a runtime error.\\\n\\\nSo, the following sequence would occur:\\\n1\\. `trying` is printed.\\\n2\\. `finally` is printed.\\\n3\\. The program exits with a `RuntimeException`.\n{% endtab %}\n{% endtabs %}\n","lastmodified":"2023-01-08T06:32:50.603932863Z","tags":null},"/cs61b/misc-topics/modular-arithmetic":{"title":"","content":"# Modular Arithmetic and Bit Manipulation\n\n{% hint style=\"warning\" %}\nMake sure you're comfortable working with binary numbers (adding, subtracting, converting to decimal) before continuing.\n{% endhint %}\n\n## Integer Types\n\nThis is an excerpt from the chart in [Java Objects](../oop/objects.md). Go there to review primitive types first!\n\n| Type  | Bits | Signed | Literals                      |\n| ----- | ---- | ------ | ----------------------------- |\n| byte  | 8    | yes    | 3, (int)17                    |\n| short | 16   | yes    | None - must cast from int     |\n| char  | 16   | no     | 'a', '\\n'                     |\n| int   | 32   | yes    | 123, 0100 (octal), 0xff (hex) |\n| long  | 64   | yes    | 123L, 0100L, 0xffL            |\n\n## Signed Numbers\n\nA type is **signed** if it can be **positive** **or** **negative.** Unsigned types can _only_ be positive.\n\nIn signed types, the **first bit** is reserved for determining the sign of the number (0 is positive, 1 is negative). This means that there is one fewer bit for the actual number. For example, ints only have **31** bits for the number.\n\n### Reading negative numbers\n\nLet's say you are given a number like `10100`and want to convert it to decimal. We know that the 1 in the front means it's a negative number! However, we can't just discard that 1 and read the rest like a positive number. Instead, we have to **flip all the bits** and then **add one** to the result. So, `10100` flipped will become `01011`. Adding one will result in `01100`, which is the correct answer (12).\n\n**Why do we have to do this?** Read on to the next section to find out!\n\n## Two's Complement\n\n**Two's Complement** is a a method of storing negative numbers in a way that supports proper arithmetic. Here's how it works:\n\n1. Start with a binary number we want to negate, like `0101`, which is 5.\n2. Flip all the bits to make `1010`.\n3. Add one to make `1011`.\n\nAlthough it makes negative numbers harder to read, the benefits are much more significant- it allows addition and subtraction to work between positive and negative numbers.\n\nIf you want to see firsthand why simply flipping the signed bit doesn't work, try out some problems in [this worksheet](https://d1b10bmlvqabco.cloudfront.net/attach/k5eevxebzpj25b/jcaul3qcivh6kh/k8g51ayfl9ui/GuerillaSection2.pdf) ([solutions](https://d1b10bmlvqabco.cloudfront.net/attach/k5eevxebzpj25b/jcaul3qcivh6kh/k8g53zthgevk/GuerillaSection2Sols.pdf)).\n\n## Modular Arithmetic\n\nSince primitive types have a fixed number of bits, it is possible to **overflow** them if we add numbers that are too large. For example, if we add `01000000`(a byte) with itself, we'd need 9 bits to store the result!\n\nThis will cause lots of issues, so we use **modular arithmetic** to **wrap around to the largest negative version** and keep the number in bounds. For example, `(byte)128 == (byte)(127+1) == (byte)(-128)`**.**\n\n## Bit Operations\n\n**Mask: \u0026**\n\n* `A \u0026 B` will only keep the bits that are 1 in A **AND** B\n* Example: `00101100 \u0026 10100111 == 00100100`\n\n**Set: |**\n\n* `A | B` will keep the bits that are 1 in A **OR** B\n* Example: `00101100 | 10100111 == 10101111`\n\n**Flip: ^**\n\n* `A ^ B` will keep the bits that are 1 in A **XOR** B\n* In other words, 1 if bits are unequal in A and B, 0 otherwise\n* Example: `00101100 ^ 10100111 == 10001011`\n\n**Flip all: \\~**\n\n* `~A` will flip all the bits from 1 to 0 or 0 to 1 in A\n* Example: `~10100111 == 01011000`\n\n**Shift Left: \u003c\u003c**\n\n* `A \u003c\u003c n` will shift all bits left n places\n* All newly introduced bits are 0\n* Example: `10101101 \u003c\u003c 3 == 01001000`\n* `x \u003c\u003c n` is equal to x \\* 2^n\n\n**Arithmetic Right: \u003e\u003e**\n\n* `A \u003e\u003e n` will shift all bits **except for the signed bit** right n times\n* Newly introduced bits are the same as the signed bit\n* Example: `10101101 \u003e\u003e 3 == 11110101`\n\n**Logical Right: \u003e\u003e\u003e**\n\n* `A \u003e\u003e\u003e n` will shift ALL bits right n times\n* Newly introduced bits are 0\n* Example: `10101101 \u003e\u003e\u003e 3 == 00010101`\n* Another example: `(-1) \u003e\u003e\u003e 29 == 7` because it leaves 3 1-bits- ints are 32 bits\n\n## Why is this useful?\n\nJust looking at these obscure operations, it may be unclear as to why we need to use these at all.\n\nWell, [here's a massive list of bit twiddling hacks](https://graphics.stanford.edu/\\~seander/bithacks.html) that should demonstrate plenty of ways to use these simple operations to do some things really efficiently.\n\nThese operations are also the **building blocks for almost all operations done by a computer.** You'll see firsthand how these are used to construct ALU's in [61C](https://cs61c.org/).\n","lastmodified":"2023-01-08T06:32:50.603932863Z","tags":null},"/cs61b/misc-topics/more-resources":{"title":"","content":"# More Resources\n\nHere are some more cool things to look at!\n\n* [Big O Cheat Sheet ](https://www.bigocheatsheet.com/)- complexities of sorting and common data structure operations\n* [Toptal Sorting Algorithm Animations ](https://www.toptal.com/developers/sorting-algorithms)- animations, pseudocode, and property summaries\n* [Josh Hug's 61B Playlist](https://www.youtube.com/channel/UC7FzTMO4rKvlqIyU5vwzFKQ/playlists) - concise video lectures for most 61B topics\n* [Balanced Search Demos](https://inst.eecs.berkeley.edu/\\~cs61b/sp20/materials/lectures/lect29/) - play around with balanced search structures and see how they work\n","lastmodified":"2023-01-08T06:32:50.603932863Z","tags":null},"/cs61b/oop/":{"title":"","content":"","lastmodified":"2023-01-08T06:32:50.603932863Z","tags":null},"/cs61b/oop/access-control":{"title":"","content":"\n## What is Access Control?\n\nIn Java, we can specify the **level of access** certain variables and methods have. With this power, we can show or hide these variables to other classes and references on demand!\n\nThere are **4** modifier levels that get progressively more open:\n\n* **Private:** Only this class can see it.\n* **Package Protected (the default level):** All classes in the **same package** can see it.\n* **Protected: Subclasses** (that inherit from the parent) can also see it.\n* **Public:** All classes in the program can see it.\n\n![A chart comparing the different access modifiers. The black bar is the default (\"package protected\").](\u003c../img/assets/image (5).png\u003e)\n\n## Why do we need access control?\n\nAccess control works really well with other OOP concepts to help structure programs better and make them easier to understand. Here are some of the major benefits:\n\n* Access control is **self documenting.** Usually, there's a reason for making certain variables private and others public, and no more needs to be said for that to be understood.\n* **It's safe to change private methods without worrying about breaking things.** If a method is private, we know that the only references are within the same class, so we can edit them however we want without making other classes error as well.\n* **Private/protected variables don't need to be understood by users.** If someone needs to use your program, they don't need to learn how to use any private methods since those will be hidden to them.\n\n## Practice\n\nLet's see how access control can be used to hide variables in different situations:\n\n```java\npackage P;\npublic class A {\n    int def; // Variable with default access\n    protected int prot; // Variable with protected access\n    private int priv; // Variable with private access\n    \n    static class NestedA { ... }\n}\n\npublic class B extends A { ... }\n\n===================\npackage Q;\npublic class C extends P.A { ... }\n\n```\n\n{{\u003c tabs \"q1\" \u003e}}\n{{\u003c tab \"Question 1\" \u003e}}\nWhich variables can be accessed in B?\n{{\u003c /tab \u003e}}\n\n{{\u003c tab \"Answer\" \u003e}}\n`def` and `prot` since B is in the same package as A.\n{{\u003c /tab \u003e}}\n{{\u003c /tabs \u003e}}\n\n{{\u003c tabs \"q2\" \u003e}}\n{{\u003c tab \"Question 2\" \u003e}}\nWhich variables can be accessed in C?\n{{\u003c /tab \u003e}}\n\n{{\u003c tab \"Answer\" \u003e}}\n`prot` only, since C is in a different package but extends A.\n{{\u003c /tab \u003e}}\n{{\u003c /tabs \u003e}}\n\n{{\u003c tabs \"q3\" \u003e}}\n{{\u003c tab \"Question 3\" \u003e}}\nWhich variables can be accessed in NestedA?\n{{\u003c /tab \u003e}}\n\n{{\u003c tab \"Answer\" \u003e}}\nNone of them, because NestedA is static and cannot reference any non-static variables.\n{{\u003c /tab \u003e}}\n{{\u003c /tabs \u003e}}\n","lastmodified":"2023-01-08T06:32:50.603932863Z","tags":null},"/cs61b/oop/dynamic-method-selection":{"title":"","content":"# Dynamic Method Selection\n\n\u003e [!warning] Content Note\n\u003e\n\u003e This is a **very tricky topic**. Make sure you are comfortable with [inheritance](inheritance.md) and [access control](access-control.md)before proceeding!\n\nInheritance is great and all, but it does have some issues. One of the biggest issues lies in overriding: **if two methods have exactly the same name and signature, which one do we call?**\n\nIn a standard use case, this is a pretty simple answer: whichever one is in the class we want! Let's look at some basic examples.\n\n```java\npublic class Dog {\n    public void eat() { ... } // A\n}\n\npublic class Shiba extends Dog {\n    @Override\n    public void eat() { ... } // C\n}\n```\n\n{{\u003c tabs \"q1\" \u003e}}\n{{\u003c tab \"Question 1\" \u003e}}\nWhich method is called when we run:\n\n```java\nDog rarePupper = new Dog();\nrarePupper.eat();\n```\n{{\u003c /tab \u003e}}\n\n{{\u003c tab \"Q1 Answer\" \u003e}}\nIt's **A** 🐕 Dog doesn't know anything about `Shiba` or any other classes, so we can just look at the Dog.\n{{\u003c /tab \u003e}}\n{{\u003c /tabs \u003e}}\n\n{{\u003c tabs \"q2\" \u003e}}\n{{\u003c tab \"Question 2\" \u003e}}\nWhat about when we call:\n\n```java\nShiba doge = new Shiba();\nrarePupper.eat();\n```\n{{\u003c /tab \u003e}}\n\n{{\u003c tab \"Q2 Answer\" \u003e}}\nThis calls **C**! This works intuitively because `Shiba` overrides `Dog` so all `Shibas` will use C instead of A.\n\n![](\u003c../img/assets/image (8).png\u003e)\n{{\u003c /tab \u003e}}\n{{\u003c /tabs \u003e}}\n\n## Things Get Wonky: Mismatched Types\n\nThere's an interesting case that actually works in Java:\n\n```java\nDog confuzzled = new Shiba();\n```\n\nWhat??? Shouldn't this error because `Dog` is incompatible with `Shiba`?\n\nIt turns out that **subclasses can be assigned to superclasses.** In other words, `Parent p = new Child()` works fine. This is really useful for things like [Interfaces](inheritance.md#interfaces) and generic [Collections](../abstract-data-types/collections/) because we might only care about using generic methods, and not the specific implementation that users chose to provide.\n\nHowever, **it is important to note that it doesn't work the other way.** `Child c = new Parent()` will error because the child might have new methods that don't exist in the parent.\n\n**Let's see how this makes inheritance really tricky:**\n\n```java\n/** The following problems are inspired by Spring 2020 Exam Prep 5. */\n\npublic class Dog {\n    public void playWith(Dog d) { ... } // D\n}\n\npublic class Shiba extends Dog {\n    @Override\n    public void playWith(Dog d) { ... } // E\n    public void playWith(Shiba s) { ... } // F\n}\n```\n\n{{\u003c tabs \"q3\" \u003e}}\n{{\u003c tab \"Question 3\" \u003e}}\nWhich method(s) run when we call:\n\n```java\nDog rarePupper = new Shiba();\nrarePupper.playWith(rarePupper); // aww rarePupper is lonely :(\n```\n{{\u003c /tab \u003e}}\n\n{{\u003c tab \"Q3 Answer\" \u003e}}\n**E** is called! What happens is that the **dynamic type** is chosen to **select the method from,** but the **static type** is used to **select the parameters.** `rarePupper`'s **** dynamic type is `Shiba` but its static type is `Dog` so `Shiba.playWith(Dog)` is chosen as the method.\n\n![rarePupper in action](\u003c../img/assets/image (10).png\u003e)\n{{\u003c /tab \u003e}}\n{{\u003c /tabs \u003e}}\n\n{{\u003c tabs \"q4\" \u003e}}\n{{\u003c tab \"Question 4\" \u003e}}\nWhich is called when we run**:**\n\n```java\nDog rarePupper = new Shiba();\nShiba doge = new Shiba();\nrarePupper.playWith(doge); // rarePupper is happy :) borks all around\n```\n{{\u003c /tab \u003e}}\n\n{{\u003c tab \"Q4 Answer\" \u003e}}\n**E** is called again! Bet ya didn't see that coming 😎\n\n**Why is it not F? I thought doge and rarePupper were both** `Shiba`**?**\\\n****When the compiler chooses a method, it **always** starts at the **static method.** Then, it keeps going down the inheritance tree until it hits the **dynamic method.** Since F has a **different signature** than D, it isn't an **overriding method** and thus the compiler won't see it. But E is (since it has the same signature as D), so that is why it is chosen instead.\n\n![bork bork bork :DDD](\u003c../img/assets/image (11).png\u003e)\n{{\u003c /tab \u003e}}\n{{\u003c /tabs \u003e}}\n\n## Adding more insanity: Static vs. Dynamic\n\nBy now, you should have a pretty good understanding of the **method selection** part of DMS. But why is it **dynamic?**\n\nYou may have noticed that there are **two** type specifiers in an instantiation. For example, `Dog s = new Shiba()` has type `Dog` on the left and `Shiba` on the right.\n\nHere, `Dog` is the **static type** of `s`: it's what the compiler believes the type should be when the program is compiled. Since the program hasn't run yet, Java doesn't know what exactly it is- it just knows it has to be some type of `Dog`.\n\nConversely, `Shiba` is the **dynamic type:** it gets assigned during runtime.\n\n### The type rules\n\nJust remember: **like chooses like.** If a method is **static**, then choose the method from the **static type.** Likewise, if a method is **not static,** choose the corresponding method from the **dynamic type.**\n\nLet's try some examples!\n\n```java\npublic class Dog {\n    public static String getType() {\n        return \"cute doggo\";\n \n    @Override // Remember, all objects extend Object class!   \n    public String toString() {\n        return getType();\n    }\n}\n\npublic class Shiba extends Dog {\n    public static String getType() {\n        return \"shiba inu\";\n    }\n}\n```\n\n{{\u003c tabs \"q5\" \u003e}}\n{{\u003c tab \"Question 5\" \u003e}}\nWhat prints out when we run:\n\n```java\nDog d = new Shiba();\nSystem.out.println(d.getType());\n```\n{{\u003c /tab \u003e}}\n\n{{\u003c tab \"Q5 Answer\" \u003e}}\n`cute doggo` gets printed because `getType()` is a static method! Therefore, Java looks at the **static type** of `d`, which is `Dog`. \\\n(If `getType()` weren't static, then `shiba inu` would have been printed as usual.)\n{{\u003c /tab \u003e}}\n{{\u003c /tabs \u003e}}\n\n{{\u003c tabs \"q6\" \u003e}}\n{{\u003c tab \"Question 6\" \u003e}}\nWhat prints out when we run:\n\n```java\nShiba s = new Shiba();\nSystem.out.println(s);\n```\n{{\u003c /tab \u003e}}\n\n{{\u003c tab \"Q6 Answer\" \u003e}}\n`cute doggo` also gets printed!! This is because static methods **cannot be overridden.** When `toString()` is called in `Dog`, it doesn't choose `Shiba`'s `getType()` because `getType()` is static and the static type is `Dog`.\n{{\u003c /tab \u003e}}\n{{\u003c /tabs \u003e}}\n\n{{\u003c tabs \"q7\" \u003e}}\n{{\u003c tab \"Question 7\" \u003e}}\nWhat prints out when we run:\n\n```java\nDog d = new Shiba();\nSystem.out.println(((Shiba)d).getType());\n```\n{{\u003c /tab \u003e}}\n\n{{\u003c tab \"Q7 Answer\" \u003e}}\nThis time, `shiba inu` gets printed. This is because casting temporarily changes the **static type:** since the static type of `d` is `Shiba` in line 2, it chooses the `getType()` from `Shiba`.\n{{\u003c /tab \u003e}}\n{{\u003c /tabs \u003e}}\n\n## That's all, folks!\n\nIf you want some **even harder** problems, [check this out](https://inst.eecs.berkeley.edu/\\~cs61b/sp20/materials/disc/examprep5.pdf) and also [this](https://inst.eecs.berkeley.edu/\\~cs61b/sp20/materials/disc/examprep6.pdf).\n\n![bai bai!](\u003c../img/assets/image (12).png\u003e)\n","lastmodified":"2023-01-08T06:32:50.603932863Z","tags":null},"/cs61b/oop/generics":{"title":"Generic Types","content":"\nSometimes, we want things to support **any type**, including user defined types that we don't know about! For example, it would make sense that we don't care what type we make a `List` out of, since it's just a whole bunch of objects put together.\n\nThe Java solution is **generics!** Generic types are denoted by a `\u003c\u003e` and can be appended to **methods and classes.** Here's an example with classes:\n\n```java\n/**\n  * Creates a type SomeClass that takes * in a generic SomeType. SomeType can * be named anything.\n*/\npublic class SomeClass\u003cSomeType\u003e {\n    private SomeType someThing;\n\n    public void someMethod(SomeType stuff) {\n        doStuff(stuff);\n    }\n}\n\n...\n/** Creates a new instance of SomeClass, setting SomeType to String.\n    We don't need to put the type on the right since it's already\n    defined on the left. */\nSomeClass\u003cString\u003e aClass = new SomeClass\u003c\u003e();\n```\n\nIn this example, `SomeType` is a **Generic Type Variable** that is not a real type, but can still be used inside the class as normal.\n\nOn the other hand, `String` is an **Actual Type Argument** that replaces `SomeType` during runtime. Now, every time `SomeType` is used in `SomeClass` Java treats it exactly like a `String`.\n\n## Generic Subtypes\n\nLike in [Dynamic Method Selection](dynamic-method-selection.md), adding inheritance makes things tricky! Let's look at an example:\n\n```java\nList\u003cString\u003e LS = new ArrayList\u003cString\u003e();\nList\u003cObject\u003e LO = LS; // Line 3\nLO.add(42); // Line 4\nString s = LS.get(0); // Line 5\n```\n{{\u003c tabs \"q1\" \u003e}}\n{{\u003c tab \"Question 1\" \u003e}}\nWill **line 3** error?\n{{\u003c /tab \u003e}}\n{{\u003c tab \"Q1 Answer\" \u003e}}\n**No**, line 3 is valid and will not error! This is because Object is a **superclass** of String. Generics work in a very similar way to the [inheritance rules](inheritance.md).\n{{\u003c /tab \u003e}}\n{{\u003c /tabs \u003e}}\n\n{{\u003c tabs \"q2\" \u003e}}\n{{\u003c tab \"Question 2\" \u003e}}\nWill **line 4** error?\n{{\u003c /tab \u003e}}\n\n{{\u003c tab \"Q2 Answer\" \u003e}}\n**No**, line 4 is valid and will not error! This is because LO is a **list of Objects** and integers are a **subtype** of Object, as all things are.\n{{\u003c /tab \u003e}}\n{{\u003c /tabs \u003e}}\n\n{{\u003c tabs \"q3\" \u003e}}\n{{\u003c tab \"Question 3\" \u003e}}\nWill **line 5** error?\n{{\u003c /tab \u003e}}\n\n{{\u003c tab \"Q3 Answer\" \u003e}}\n**Yes,** line 5 will error! This is because we put 42 into LO, which is an integer. Since LO is pointing to the same object as LS, 42 is also in LS! That means we are trying to assign a String equal to an integer.\n{{\u003c /tab \u003e}}\n{{\u003c /tabs \u003e}}\n\n\u003e [!info] Content Note\n\u003e\n\u003e Arrays have slightly different behavior than this and will throw an `ArrayStoreException` if types are mismatched in any way.\n\n## Type Bounds\n\nSometimes, we want to **put constraints** on what kinds of types can be passed into a generic type.\n\nOne way of doing is is to specify that a generic type must fit within a **type bound**: here, T must be some subtype of a specified type `Number`.\n\nWe can also do it the other way and specify that a type can be a **supertype** of a specified type. Both of these examples are shown below:\n\n```java\nclass SomeClass\u003cT extends Number\u003e {\n    // A method that takes a type parameter T and takes any SUPERCLASS\n    // of T as a list generic type.\n    static \u003cT\u003e void doSomething(List\u003c? super T\u003e L) { ... }\n}\n```\n\n## Limitations of Generic Types\n\nThe biggest limitation is that **primitive types cannot be used as generic types.** For example, `List\u003cint\u003e` is invalid and will not work!\n\nOne workaround to this is to use the reference-type counterparts to primitives, such as `Integer`, `Boolean`, `Character` and so on. However, converting between these types and primitive types, which is called **autoboxing,** has significant performance penalties that must be taken into consideration.\n\nAnother limitation is that **instanceof** does not work properly with generic types. For instance, `new List\u003cX\u003e() instanceof List\u003cY\u003e` will always be true regardless of what types X and Y are.\n","lastmodified":"2023-01-08T06:32:50.603932863Z","tags":null},"/cs61b/oop/inheritance":{"title":"","content":"\n## What is inheritance?\n\nEssentially, it's a way of putting similar objects together to **generalize behavior.** Inheritance is best used with relating **subtypes** to larger categories. For example, an :tangerine:orange **is a** fruit (so it's a **subtype** of fruit).\n\nLet's say that a supermarket named _Jrader Toe's_ asks us to simulate fruits for them in an online system. We could do it like this:\n\n![The naive approach.](../img/assets/image.png)\n\nNow, every fruit would need some of the same properties- like cost, weight, and name! So we would need to do something like:\n\n```java\npublic class Orange {\n    private String name = \"Orange\";\n    private int cost;\n    ...\n    public Orange(int cost, ...) {\n        this.cost = cost;\n        ...\n    }\n    // lots of methods\n    public String getName() { ...\n```\n\nThis would be _really annoying_ to do for every single fruit. And they're all the same properties for every fruit so it would also be incredibly inefficient code-wise. **Inheritance gives a much better solution!**\n\nLet's make a **Fruit** class and have all of our fruits **inherit from** that class.\n\n![uwu inheritance is cool and good](\u003c../img/assets/image (1).png\u003e)\n\nThis does amazing things because we can just create one single Fruit class that has all of the properties we need, and simply make our specific fruits inherit those properties. (Side note: making multiple things inherit from one generic interface like this is called **polymorphism.**)\n\n```java\npublic class Fruit {\n    private String name;\n    private int cost;\n    ...\n    public Orange(String name, int cost, ...) {\n        this.name = name;\n        this.cost = cost;\n        ...\n    }\n    // lots of methods\n    public String getName() { ...\n}\n\n// Now for a very simple Orange method!\npublic class Orange extends Fruit {\n    public Orange(int cost,...) {\n        super(\"Orange\", cost, ...);\n    }\n}\n```\n\nWith only those 4 lines, :tangerine:Orange now has all of the same methods and properties that Fruit has!\n\n## Implementation Inheritance (\"Extends\")\n\nYou may have noticed the `extends` keyword being used to specify that an object **inherits** from another object. This is called **implementation inheritance** since an object takes all of the behaviors from its parent and can use them like its own.\n\nWhen `extends` is used, these are the things that are inherited:\n\n* All instance and static variables that are **not private** (see [Access Control](access-control.md) for more information)\n* All non-private methods\n* All nested classes\n\nThese are **not** inherited:\n\n* Any **private** variables and methods\n* All constructors\n\n\n\u003e [!info] **Quick sidenote!**\n\u003e \n\u003e All objects automatically extend the `Object` class whether you like it or not. See [References, Objects, and Types in Java](objects.md) for more about this behavior.\n\n### Constructor magic 🏗\n\nWhen an object `extends` another object, its constructor will **automatically call the parent's constructor.** However, this does have some limitations:\n\n* It will only call the **default** (no-argument) constructor in the parent.\n* Calling the constructor is the **first thing that is done** in the child constructor.\n\nBut what if we want to call another constructor? That's where the `super` keyword comes in! When `super` is called, Java will know to **not** call the default constructor anymore. Here's an example:\n\n```java\npublic class Parent {\n    public Parent() {\n        System.out.println(\"Default constructor\");\n    }\n    public Parent(String say) {\n        System.out.println(say);\n    }\n    void doStuff() { ... }\n}\n\n// Child inherits doStuff(), but not the constructors.\npublic class Child extends Parent {\n    public Child() {\n        System.out.println(\"Child\")\n    }\n    public Child(String say) {\n        super(say);\n    }\n}\n    \npublic static void Main(String[] args) {\n    Child c1 = new Child(); // will print \"Default constructor\" then \"Child\" !!!\n    Child c2 = new Child(\"Hi\"); // will print \"Hi\"\n}\n```\n\n## Method Overriding\n\nLet's say that _Jrader Toe's_ is running a promotion for 🍐pears and wants to make them 20% off normal pears! This poses a problem because **we want to inherit everything that normal pears have, but change only one behavior** (getPrice). Well I've got the solution for you!!! And it's called **overriding.**\n\n```java\npublic class PromoPear extends Pear {\n    public PromoPear(int cost, ...) {\n        super(cost, ...);\n    }\n    \n    // Overriding the getPrice to have a new behavior only for PromoPears!\n    @Override\n    public int getPrice() {\n        return super.getPrice() * 0.8;\n    }\n    ...\n}   \n```\n\nThe `@Override` tag is technically optional, but it's highly suggested because it makes sure that you are indeed overriding something and not just making a new method! (Remember, it has to have the **same name and parameters as a method in one of its parents**.)\n\n## **Method Overloading**\n\nSometimes, you want to take in **different parameters** into the **same method.** For instance, what if we wanted to create a method `getCount(Fruit fruit)` that counts how many fruits of that type we have? We might also want to allow users to pass in the name of the fruit to do the same thing- `getCount(String fruit)`. Java will allow us to make **both** of these methods in the same class!\n\nHowever, this has some major downsides that should be considered.\n\n* It's repetitive.\n* It requires maintaining more code- changing one overload won't change the others!\n* You can't handle any data types other than the ones you explicitly specify will work.\n\nWe'll discuss better solutions further down the page as well as in the [Generic Types](generics.md) page!\n\n### How is overriding different from overloading?\n\nThey have very similar names but pretty different uses!\n\nOverriding is for methods of the same name, **same parameters**, and **different classes.** If you can remember when you use the `@Override` tag, you can relate it back to this concept!\n\nOverloading is for methods of the same name, **different parameters**, in the **same class**.\n\n## Interfaces\n\nInterfaces are like **blueprints 📘** for objects- they tell you what an object needs, but not how to implement them.\n\nThey are very similar to normal classes except for some major differences:\n\n* **All variables are constants** (public static final).\n* **Methods have no body**- just a signature (like `void doStuff();`)\n* **Classes can inherit from multiple interfaces.**\n\nTypically, interfaces will not have any implemented methods whatsoever. This limitation can technically be removed using the [default keyword](https://stackoverflow.com/questions/31578427/what-is-the-purpose-of-the-default-keyword-in-java/31579210), but this is **not recommended** because abstract classes handle this much better.\n\nHere's an example of interfaces in the wild:\n\n```java\npublic interface AnInterface\u003cItem\u003e {\n  public void doStuff(Item x);\n  public Item getItem();\n  ...\n}\n\npublic class Something implements AnInterface\u003cItem\u003e { // Note the IMPLEMENTS!\n @Override\n public void doStuff(Item x) {\n     // implement method\n }\n\n @Override\n public void getItem() {\n     // implement method\n }\n}\n\npublic class MainClass {\n  public static void main(String[] args) {\n      AnInterface\u003cString\u003e smth = new AnInterface\u003c\u003e(); // ERROR!!\n      // (new can't be used with interfaces.)\n      AnInterface\u003cString\u003e smthElse = new Something\u003cString\u003e(); // Will not error!\n      smth.getItem();\n      ...\n  }\n}\n```\n\n## Abstract Classes\n\nAbstract classes live in the place **in between** interfaces and concrete classes. In a way, they get the best of both worlds- you can implement whichever methods you want, and leave the rest as **abstract** methods (same behavior as interface methods)!\n\nHere are some properties:\n\n* **Variables behave just like a concrete class.**\n* **Normal methods can be created like any other concrete class.**\n* **Abstract methods** (`abstract void doSomething()`) **behave just like methods in interfaces.**\n* Classes can only inherit from **one** abstract class.\n\nHere's the same example from the interfaces section, but implemented using an abstract class.\n\n```java\npublic abstract class AnAbstract\u003cItem\u003e {\n  public abstract void doStuff(Item x);\n  public abstract Item getItem();\n  ...\n}\n\npublic class Something extends AnAbstract\u003cItem\u003e { // EXTENDS, not implements!\n @Override\n public void doStuff(Item x) {\n     // implement method\n }\n\n @Override\n public void getItem() {\n     // implement method\n }\n}\n\npublic class MainClass {\n  public static void main(String[] args) {\n      AnAbstract\u003cString\u003e smth = new AnAbstract\u003c\u003e(); // ERROR!!\n      // (new can't be used with abstract classes, just like interfaces.)\n      AnAbstract\u003cString\u003e smthElse = new Something\u003cString\u003e(); // Will not error!\n      smth.getItem();\n      ...\n  }\n}\n```\n\n![A chart comparing the differences between the types of classes.](\u003c../img/assets/image (2).png\u003e)\n\n## Still not satisfied?\n\nWatch [Josh Hug's video lecture](https://www.youtube.com/watch?v=IaEq\\_fogI08\\\u0026list=PL8FaHk7qbOD6km6LlaHLWgRl9SbhlTHk2) about inheritance.\n\nOr, move onto an advanced application of inheritance concepts, [Dynamic Method Selection](dynamic-method-selection.md).\n","lastmodified":"2023-01-08T06:32:50.603932863Z","tags":null},"/cs61b/oop/objects":{"title":"Java Objects","content":"\nThere are two main categories of objects in Java: **Primitive Types** and **Reference Types.** This page will give a brief overview of both, and close off with some info about the mystical **Object** class.\n\n## Primitive Types\n\n**Primitive types** are built in to Java and have **fixed memory sizes.** Different types require different amounts of memory.\n\nIf you remember [environment diagrams](http://albertwu.org/cs61a/notes/environments), you may recall that some variables are put straight into the boxes, while others have an arrow pointing to them. The reason for this is that it actually denotes primitive vs. reference types! **Primitive types go straight in the box** because they aren't mutable (i.e. you can't change the objects contained in the box since they're just constant literals like numbers).\n\n**There are 8 primitive types in Java.** Here's a table of their properties! (If you don't know what \"signed\" means, go to [Modular Arithmetic and Bit Manipulation](../misc-topics/modular-arithmetic.md).)\n\n| Type    | Bits | Signed | Default | Examples                      |\n| ------- | ---- | ------ | ------- | ----------------------------- |\n| boolean | 1    | no     | false   | true, false                   |\n| byte    | 8    | yes    | 0       | 3, (int)17                    |\n| short   | 16   | yes    | 0       | None - must cast from int     |\n| char    | 16   | no     | \\u0000  | 'a', '\\n'                     |\n| int     | 32   | yes    | 0       | 123, 0100 (octal), 0xff (hex) |\n| long    | 64   | yes    | 0       | 123L, 0100L, 0xffL            |\n| float   | 32   | yes    | 0.0     | 1.23f, -1.23e10f, .001f       |\n| double  | 64   | yes    | 0.0     | 1.23e256d, 1e1d, 1.2e-10d     |\n\n{% hint style=\"info\" %}\n**A quick aside on Strings 🧵**\\\nYou may have noticed that strings are not on this list. That is because unlike in Python, they aren't a primitive type! Under the hood, Strings are a reference type that are very similar to a char array.\n{% endhint %}\n\n## Type Conversion\n\nJava will automatically convert between primitive types if **no information is lost** (\nfrom byte to int).\n\nConversion in the other direction (from a larger to smaller container) requires an explicit cast (e.g., `(char) int`). The compiler will treat a cast object as though its static type is the cast type, but this will only work if the cast type is the same as or a parent of the dynamic type. However, relative to the assigned static type, the cast type could be a child of the static type or a parent of the static type.\n\n**Assignment statements are an exception to this**: `aByte = 10` is fine even though 10 is an int literal. This is because arithmetic operations (+, \\*, ...) automatically promote operands (e.g., `'A' + 2` is equivalent to `(int)'A' + 2`)\n\nHowever, **this doesn't work if you are trying to add a larger type to a smaller type** (e.g., `aByte = aByte + 1` since operands become an int type which cannot be set equal to a byte type. **But += works**!\n\n## Reference Types\n\nA **reference type** refers to basically anything that's not primitive 😅\n\nThis includes **user-defined objects** as well as many common Java built-in types such as **arrays, strings, and** [**collections**](../abstract-data-types/collections/)**.**\n\nHere are some major differences that set them apart from primitive types:\n\n* Reference types can take an **arbitrary amount of memory.** Unlike primitives which have a fixed memory for each type, objects like arrays can expand to hold lots of things inside it.\n* Reference types are referred to using **addresses.** When you say something like `int[] arr = new int[5]`, `arr` only stores a 64-bit **memory address** which **points** to the real object, a 5-length integer array. Again, think back to the arrow in environment diagrams, and how those work.\n* By default, reference types can be set to **null** which is represented as an **address of all zeroes.** Or, the **new** keyword can be used to set it to a specific address.\n* Reference objects can be **lost** if all pointers to it are reassigned. For example, if I now enter `arr = null;`, the original 5-length array still exists, but just has nothing to refer to it.\n\n## The Equals Sign\n\nThe assignment operator (`=`) has **different behaviors** for primitive types and references types.\n\nFor **primitive types,** `y = x` means \"**copy** **the bits** from y into a new location, then call them x\". Here, the **entire object** is copied- this means that changing y will NOT change x even though they are set \"equal\".\n\nFor **reference types,** `obj1 = obj2` means \"**copy the address** stored in obj1 to obj2\". Here, `obj1` and `obj2` are referring to the **exact same object,** and mutating one will change the other.\n\n{% hint style=\"info\" %}\n**A clarification on reference type assignment**\n\nBy mutating, I mean changing the **internals** of an object (for example, accessing an array index or doing something like `obj1.value = 1`. If you change the actual **address** of `obj2`, as in `obj2 = obj3`, this does **not** change `obj1` because `obj2` is now referring to a completely different object!\n{% endhint %}\n\n## The Object Class\n\nIn Java, **all objects inherit from the master Object class.** Here are some important properties of Object that will be useful to know:\n\n* `String toString()`: By default, this prints out the class name followed by the memory address (e.g., `Object@192c38f`). This can be overridden to make more user-friendly names for objects.\n* `boolean equals(Object obj)`: By default, this checks if the two objects are actually the same object (same memory address). This can be overridden to check if specific contents of objects are the same, rather than checking if they are literally the same object. (Like `\"foo\"` should equal `new String(\"foo\")`)\n* `int hashCode()`: Returns a numeric hash code for the object that should differentiate it from other objects. **This should be overridden if** **`equals()` is overridden** since `x.hashCode()` should equal `y.hashCode()` if `x.equals(y)` is true!\n* `Class\u003c?\u003e getClass()`: Returns the class of this object.\n\nObject has plenty of other methods and properties as well, but these aren't as important. If you want to learn about them, feel free to refer to the [Java documentation](https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html).\n","lastmodified":"2023-01-08T06:32:50.603932863Z","tags":null}}