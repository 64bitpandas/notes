<!doctype html><html lang=en dir=ltr><head><meta property="og:title" content><meta property="og:description" content="What is a Markov Decision Process? #  A Markov Decision Process is a Markov model that solves nondeterministic search problems (where an action can result in multiple possible successor states).
A MDP is defined by:
 A set of states $s$ A set of actions $a$ A transition model $T(s, a, s’)$ that represents the probability $P(s’ | s, a)$ - that the action $a$ taken at state $s$ will lead to a new state $s’$."><meta property="og:type" content="article"><meta property="og:url" content="https://notes.bencuan.me/data102/Markov-Decision-Processes/"><meta property="article:section" content="data102"><meta property="article:modified_time" content="2023-01-12T00:50:22+00:00"><meta property="og:site_name" content="🗒️ Ben's Notes"><title>Markov Decision Processes | 🗒️ Ben's Notes</title><link rel=manifest href=/manifest.json><link rel=icon href=/favicon.png type=image/x-icon><link rel=stylesheet href=/book.min.9571c7adb6029c898bf9b839bbb08bbb02cf98ea0b2512213e6dad0a0b116159.css integrity="sha256-lXHHrbYCnImL+bg5u7CLuwLPmOoLJRIhPm2tCgsRYVk=" crossorigin=anonymous><meta charset=utf-8><meta name=description content="What is a Markov Decision Process? #  A Markov Decision Process is a Markov model that solves nondeterministic search problems (where an action can result in multiple possible successor states).
A MDP is defined by:
 A set of states $s$ A set of actions $a$ A transition model $T(s, a, s’)$ that represents the probability $P(s’ | s, a)$ - that the action $a$ taken at state $s$ will lead to a new state $s’$."><title>🗒️ Ben's Notes</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=https://notes.bencuan.me//favicon.png><link href=https://notes.bencuan.me/styles.e08ceb33360cec132feb69cfb982e2a4.min.css rel=stylesheet><link href=https://notes.bencuan.me/quartz/styles/_light_syntax.86a48a52faebeaaf42158b72922b1c90.min.css rel=stylesheet id=theme-link><link rel=preload href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css as=style onload='this.onload=null,this.rel="stylesheet"' integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/copy-tex.min.js integrity=sha384-ww/583aHhxWkz5DEVn6OKtNiIaLi2iBRNZXfJRiY1Ai7tnJ9UXpEsyvOITVpTl4A crossorigin=anonymous></script>
<script src=https://notes.bencuan.me/quartz/js/darkmode.9d3e1c1ebec61822893ee54cff5423d0.min.js></script>
<script src=https://notes.bencuan.me/quartz/js/util.c69e233dc1cc331a30b0f670e657b425.min.js></script>
<script async src=https://unpkg.com/@floating-ui/core@0.7.3></script>
<script async src=https://unpkg.com/@floating-ui/dom@0.5.4></script>
<script src=https://notes.bencuan.me/quartz/js/popover.f03552ccb84d99ca615d1cfb9abde59e.min.js></script>
<script defer src=https://notes.bencuan.me/quartz/js/code-title.ce4a43f09239a9efb48fee342e8ef2df.min.js></script>
<script defer src=https://notes.bencuan.me/quartz/js/clipboard.2913da76d3cb21c5deaa4bae7da38c9f.min.js></script>
<script defer src=https://notes.bencuan.me/quartz/js/callouts.7723cac461d613d118ee8bb8216b9838.min.js></script>
<script>const SEARCH_ENABLED=!1,LATEX_ENABLED=!0,PRODUCTION=!0,BASE_URL="https://notes.bencuan.me/",fetchData=Promise.all([fetch("https://notes.bencuan.me/indices/linkIndex.68587926b15a523610470d276acdfef2.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://notes.bencuan.me/indices/contentIndex.dfd7854505f6353a83875393f9d06bac.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),render=()=>{const e=new URL(BASE_URL),t=e.pathname,n=window.location.pathname,i=t==n;addCopyButtons(),addCollapsibleCallouts(),initPopover("https://notes.bencuan.me",!0);const s=document.getElementById("footer");if(s){const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(render);e.textContent="";const t=!0;drawGraph("https://notes.bencuan.me",t,[{"/cs61b":"#4388cc"},{"/data102":"#ba0af0"},{"/cs70":"#70fa70"},{"/cs61a":"#fbff22"},{"/cs186":"#ffaf1c"},{"/cs168":"#ff86a1"},{"/cs162":"#ffdfdd"}],t?{centerForce:1,depth:-1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.5,linkDistance:1,opacityScale:3,repelForce:1,scale:1.4}:{centerForce:1,depth:1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.6,linkDistance:1,opacityScale:3,repelForce:2,scale:1.2})}var o=document.getElementsByClassName("mermaid");o.length>0&&import("https://unpkg.com/mermaid@9/dist/mermaid.esm.min.mjs").then(e=>{e.default.init()})},init=(e=document)=>{addCopyButtons(),addTitleToCodeBlocks(),renderMathInElement(e.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],macros:{'’':"'"},throwOnError:!1})}</script><script>window.Million={navigate:e=>window.location.href=e,prefetch:()=>{}},window.addEventListener("DOMContentLoaded",()=>{init(),render()})</script></head><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://notes.bencuan.me/quartz/js/full-text-search.e6e2e0c213187ca0c703d6e2c7a77fcd.min.js></script><header class=book-header><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu></label><h1 id=page-title><a href=https://notes.bencuan.me/>🗒️ Ben's Notes</a></h1><div class=spacer></div><div id=search-icon class=quartz-search><p>Search</p><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control><main class=flex><aside class=book-menu><div class=book-menu-content><nav><div class=menu-search><div id=search-icon class=quartz-search><p>Search</p><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></div></div><ul><li><input type=checkbox id=section-da99de074169576d9f77860d41691a68 class=toggle>
<label for=section-da99de074169576d9f77860d41691a68 class="flex justify-between"><a href=/cs61b/ class=book-collapse-toggle>CS 61B: Data Structures</a></label><ul><li><span class=book-menu-title>Object Oriented Programming</span><ul><li><a href=/cs61b/oop/inheritance/>Inheritance</a></li><li><a href=/cs61b/oop/access-control/>Access Control</a></li><li><a href=/cs61b/oop/dynamic-method-selection/>Dynamic Method Selection</a></li><li><a href=/cs61b/oop/objects/>Java Objects</a></li><li><a href=/cs61b/oop/generics/>Generic Types</a></li></ul></li><li><span class=book-menu-title>Asymptotics</span><ul><li><a href=/cs61b/asymptotics/asymptotics/>Asymptotic Analysis Basics</a></li><li><a href=/cs61b/asymptotics/amortization/>Amortization</a></li><li><a href=/cs61b/asymptotics/asymptotics-practice/>Asymptotics Practice</a></li></ul></li><li><span class=book-menu-title>Abstract Data Types</span><ul><li><input type=checkbox id=section-70dba6a254ce0b2bcdb77dd58dbf6cdc class=toggle>
<label for=section-70dba6a254ce0b2bcdb77dd58dbf6cdc class="flex justify-between"><a href=/cs61b/abstract-data-types/collections/ class=book-collapse-toggle>Collections</a></label><ul><li><a href=/cs61b/abstract-data-types/collections/arrays/>Arrays</a></li><li><a href=/cs61b/abstract-data-types/collections/linked-lists/>Linked Lists</a></li><li><a href=/cs61b/abstract-data-types/collections/sets/>Sets</a></li><li><a href=/cs61b/abstract-data-types/collections/stacks-and-queues/>Stacks and Queues</a></li></ul></li><li><input type=checkbox id=section-5eb2d5b415a82a45c7861774887e844b class=toggle>
<label for=section-5eb2d5b415a82a45c7861774887e844b class="flex justify-between"><a href=/cs61b/abstract-data-types/binary-trees/ class=book-collapse-toggle>Binary Trees</a></label><ul><li><a href=/cs61b/abstract-data-types/binary-trees/balanced-search-structures/>Balanced Search Structures</a></li><li><a href=/cs61b/abstract-data-types/binary-trees/heaps/>Heaps</a></li><li><a href=/cs61b/abstract-data-types/binary-trees/tries/>Tries</a></li></ul></li><li><a href=/cs61b/abstract-data-types/graphs/>Graphs</a></li><li><a href=/cs61b/abstract-data-types/hashing/>Hashing and Hash Tables</a></li><li><a href=/cs61b/abstract-data-types/union-find-disjoint-sets/>Union Find (Disjoint Sets)</a></li><li><a href=/cs61b/abstract-data-types/comparables-and-comparators/>Comparables and Comparators</a></li></ul></li><li><span class=book-menu-title>Algorithms</span><ul><li><input type=checkbox id=section-26838f44377a3dfc0925929b8b82de69 class=toggle>
<label for=section-26838f44377a3dfc0925929b8b82de69 class="flex justify-between"><a href=/cs61b/algorithms/searching/ class=book-collapse-toggle>Searching</a></label><ul><li><a href=/cs61b/algorithms/searching/binary-search/>Binary Search</a></li><li><a href=/cs61b/algorithms/searching/breadth-first-search-bfs/>Breadth First Search (DFS)</a></li><li><a href=/cs61b/algorithms/searching/depth-first-search-dfs/>Depth First Search (DFS)</a></li></ul></li><li><input type=checkbox id=section-fd5238a874e42ff4e1ab0e65c20a9824 class=toggle>
<label for=section-fd5238a874e42ff4e1ab0e65c20a9824 class="flex justify-between"><a href=/cs61b/algorithms/shortest-paths/ class=book-collapse-toggle>Shortest Paths</a></label><ul><li><a href=/cs61b/algorithms/shortest-paths/dijkstras-algorithm/>Dijkstra's Algorithm</a></li><li><a href=/cs61b/algorithms/shortest-paths/a-search/>A* Search</a></li></ul></li><li><input type=checkbox id=section-434cf8cf80558e0886666bff24d740d1 class=toggle>
<label for=section-434cf8cf80558e0886666bff24d740d1 class="flex justify-between"><a href=/cs61b/algorithms/minimum-spanning-trees/ class=book-collapse-toggle>Minimum Spanning Trees</a></label><ul><li><a href=/cs61b/algorithms/minimum-spanning-trees/kruskals-algorithm/>Kruskal's Algorithm</a></li><li><a href=/cs61b/algorithms/minimum-spanning-trees/prims-algorithm/>Prim's Algorithm</a></li></ul></li><li><a href=/cs61b/algorithms/sorting/>Sorting</a></li><li><a href=/cs61b/algorithms/minimax/>Minimax Algorithm</a></li></ul></li><li><span class=book-menu-title>Misc. Topics</span><ul><li><a href=/cs61b/misc-topics/exceptions/>Exceptions</a></li><li><a href=/cs61b/misc-topics/modular-arithmetic/>Modular Arithmetic</a></li><li><a href=/cs61b/misc-topics/more-resources/>More Resources</a></li></ul></li></ul></li><li><input type=checkbox id=section-ee67b82ee54e976a9f81e3e41661ae13 class=toggle>
<label for=section-ee67b82ee54e976a9f81e3e41661ae13 class="flex justify-between"><a href=/cs70/ class=book-collapse-toggle>CS 70: Discrete Math</a></label><ul><li><a href=/cs70/latex-reference/>LaTeX Reference</a></li><li><span class=book-menu-title>Discrete Math</span><ul><li><a href=/cs70/discrete-math/overview/>Discrete Math Overview</a></li><li><a href=/cs70/discrete-math/propositional-logic/>Propositional Logic</a></li><li><a href=/cs70/discrete-math/proofs/>Proofs</a></li><li><a href=/cs70/discrete-math/stable-matching/>Stable Matching</a></li><li><a href=/cs70/discrete-math/graphs/>Graphs</a></li><li><a href=/cs70/discrete-math/modular-arithmetic/>Modular Arithmetic</a></li><li><a href=/cs70/discrete-math/rsa-cryptography/>RSA Cryptography</a></li><li><a href=/cs70/discrete-math/polynomials/>Polynomials</a></li><li><a href=/cs70/discrete-math/countability/>Countability</a></li><li><a href=/cs70/discrete-math/computability/>Computability</a></li></ul></li><li><span class=book-menu-title>Probability</span><ul><li><a href=/cs70/probability/probability-overview/>Probability Overview</a></li><li><a href=/cs70/probability/counting/>Counting</a></li><li><a href=/cs70/probability/discrete-probability/>Discrete Probability</a></li><li><a href=/cs70/probability/hashing-and-the-union-bound/>Hashing and the Union Bound</a></li><li><a href=/cs70/probability/expectation-and-variance/>Expectation and Variance</a></li><li><a href=/cs70/probability/concentration-inequalities/>Concentration Inequalities</a></li><li><a href=/cs70/probability/continuous-probability/>Continuous Probability</a></li><li><a href=/cs70/probability/markov-chains/>Markov Chains</a></li><li><a href=/cs70/probability/the-beta-family/>The Beta Family</a></li><li><a href=/cs70/probability/conditional-expectation-and-variance/>Conditional Expectation and Variance</a></li></ul></li></ul></li><li><input type=checkbox id=section-6d80b867b0e7b10bf4afdb6cb30147b7 class=toggle>
<label for=section-6d80b867b0e7b10bf4afdb6cb30147b7 class="flex justify-between"><a href=/cs186/ class=book-collapse-toggle>CS 186: Databases</a></label><ul><li><a href=/cs186/io/>What is an I/O and why should I care?</a></li><li><a href=/cs186/00-SQL-Basics/>SQL Basics</a></li><li><a href=/cs186/01-Disks-Buffers-Files/>Disks, Buffers, and Files</a></li><li><a href=/cs186/02-B+-Trees/>B+ Trees</a></li><li><a href=/cs186/03-Buffer-Management/>Buffer Management</a></li><li><a href=/cs186/04-Sorting-and-Hashing/>Sorting and Hashing</a></li><li><a href=/cs186/05-Iterators-and-Joins/>Iterators and Joins</a></li><li><a href=/cs186/06-Relational-Algebra/>Relational Algebra</a></li><li><a href=/cs186/07-Query-Optimization/>Query Optimization</a></li><li><a href=/cs186/08-Transactions/>Transactions and ACID</a></li><li><a href=/cs186/09-Parallel-Query-Processing/>Parallel Query Processing</a></li><li><a href=/cs186/10-Recovery/>Recovery</a></li><li><a href=/cs186/11-Distributed-Transactions/>Distributed Transactions</a></li><li><a href=/cs186/12-ER-Diagrams/>E-R Diagrams</a></li></ul></li><li><input type=checkbox id=section-a5f138e4453c6deda66a767c13d8a673 class=toggle>
<label for=section-a5f138e4453c6deda66a767c13d8a673 class="flex justify-between"><a href=/cs162/ class=book-collapse-toggle>CS 162: Operating Systems</a></label><ul><li><a href=/cs162/Chapter-1-OS-Basics/>Chapter 1: OS Basics</a></li><li><a href=/cs162/Chapter-2-Processes/>Chapter 2: Processes</a></li><li><a href=/cs162/Chapter-3-Threads/>Chapter 3: Threads</a></li><li><a href=/cs162/Chapter-4-I-O/>Chapter 4: I/O</a></li><li><a href=/cs162/Chapter-5-Synchronization/>Chapter 5: Synchronization</a></li><li><a href=/cs162/Chapter-6-Scheduling/>Chapter 6: S cheduling</a></li><li><a href=/cs162/Chapter-7-Address-Translation/>Chapter 7: Address Translation</a></li><li><a href=/cs162/Chapter-8-Caching/>Chapter 8: Caching</a></li><li><a href=/cs162/Chapter-9-File-Systems/>Chapter 9: File Systems</a></li><li><a href=/cs162/Appendix-A-GDB-foobars/>GDB Reference</a></li></ul></li><li><input type=checkbox id=section-d6662b440d117f766a2c3bdd162dce8f class=toggle>
<label for=section-d6662b440d117f766a2c3bdd162dce8f class="flex justify-between"><a href=/cs168/ class=book-collapse-toggle>CS 168: The Internet</a></label><ul><li><a href=/cs168/intro-to-the-internet/>Intro to the Internet</a></li><li><a href=/cs168/cli/>CLI Tools</a></li><li><a href=/cs168/intradomain-routing/>Introduction to Routing</a></li><li><a href=/cs168/measuring-link-performance/>Measuring Link Performance</a></li><li><a href=/cs168/resource-sharing-packet-and-circuit-switching/>Resource Sharing</a></li><li><a href=/cs168/internet-organization-and-layers/>Internet Organization</a></li><li><a href=/cs168/sockets-and-ports/>Sockets and Ports</a></li><li><a href=/cs168/addressing-ip/>Addressing</a></li><li><a href=/cs168/interdomain-routing-bgp/>Interdomain Routing (BGP)</a></li><li><a href=/cs168/TCP/>TCP</a></li><li><a href=/cs168/reliability/>Reliability</a></li><li><a href=/cs168/congestion-control/>Congestion Control</a></li><li><a href=/cs168/dns/>DNS</a></li><li><a href=/cs168/web/>Web</a></li><li><a href=/cs168/ethernet/>Ethernet</a></li><li><a href=/cs168/end-to-end-operation/>End to End Operation</a></li><li><a href=/cs168/final-review/>Final Review</a></li></ul></li><li><input type=checkbox id=section-0ab56f4e325729fc5ee23c1204712a3e class=toggle>
<label for=section-0ab56f4e325729fc5ee23c1204712a3e class="flex justify-between"><a href=/cs61a/ class=book-collapse-toggle>CS 61A: Computer Programs</a></label><ul><li><a href=/cs61a/resources/>Resources</a></li><li><a href=/cs61a/midterm-tips/>Midterm Tips</a></li></ul></li><li><input type=checkbox id=section-7a47eedeadc37bd22193fe5cd002031e class=toggle checked>
<label for=section-7a47eedeadc37bd22193fe5cd002031e class="flex justify-between"><a href=/data102/ class=book-collapse-toggle>Data 102: Inference</a></label><ul><li><a href=/data102/binary-decision-making/>Binary Decision Making</a></li><li><a href=/data102/hypothesis-testing/>Hypothesis Testing</a></li><li><a href=/data102/decision-theory/>Decision Theory</a></li><li><a href=/data102/parameter-estimation/>Parameter Estimation</a></li><li><a href=/data102/sampling/>Sampling</a></li><li><a href=/data102/regression-and-glms/>Regression and GLMs</a></li><li><a href=/data102/nonparametric-methods/>Nonparametric Methods</a></li><li><a href=/data102/interpretability/>Interpretability</a></li><li><a href=/data102/causality/>Causality</a></li><li><a href=/data102/concentration-inequalities/>Concentration Inequalities</a></li><li><a href=/data102/bandits/>Bandits</a></li><li><a href=/data102/Markov-Decision-Processes/ class=active>Markov Decision Processes</a></li><li><a href=/data102/Reinforcement-Learning/>Reinforcement Learning</a></li></ul></li></ul><ul><li><a href=/contributing target=_blank rel=noopener>Contribute</a></li><li><a href=https://github.com/64bitpandas/notes/issues target=_blank rel=noopener>Feedback</a></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class="book-page container"><article class=markdown><h1 class=title>Markov Decision Processes</h1><h2 id=what-is-a-markov-decision-process>What is a Markov Decision Process?
<a class=anchor href=#what-is-a-markov-decision-process>#</a></h2><p>A Markov Decision Process is a Markov model that solves <strong>nondeterministic search problems</strong> (where an action can result in multiple possible successor states).</p><p>A MDP is defined by:</p><ul><li>A set of states $s$</li><li>A set of actions $a$</li><li>A transition model $T(s, a, s’)$ that represents the probability $P(s’ | s, a)$ - that the action $a$ taken at state $s$ will lead to a new state $s’$. (Allowed by memoryless property)</li><li>A reward function $R(s, a, s’)$ per transition</li><li>Discount factor $\gamma \in [0, 1]$</li><li>A start state</li><li>A terminal (absorbing state)</li></ul><p>The utility function of an MDP can be calculated as follows:</p><p>$$
U([s_0, a_0, s_1, a_1, s_2, \cdots]) = R(s_0, a_0, s_1) + \gamma R(s_1, a_1, s_2) + \gamma^2 R(s_2, a_2, s_3) + \cdots
$$</p><p>Intuitively, the discount factor causes an exponential decay over time, so if an agent takes too long to reach a state it is automatically terminated. This also solves the problem of infinite reward streams. (<strong>enforce a finite horizon)</strong></p><ul><li>The reward is bounded by the value $\frac{R_{\max}}{1-\gamma}$.</li></ul><p><strong>The primary goal for an MDP is to find an optimal policy</strong> $\pi^*$ that gives us an action for every state that results in the maximum expected utility.</p><ul><li>The expected utility of a policy $U^{\pi}(s_0)$ is equal to the sum over all possible state sequences multiplied by the probability of them occurring.</li></ul><h1 id=solving-mdps>Solving MDPs
<a class=anchor href=#solving-mdps>#</a></h1><h2 id=the-bellman-equation>The Bellman Equation
<a class=anchor href=#the-bellman-equation>#</a></h2><p>Some values:</p><ul><li>The utility of a state $s$ is equal to the expected utility when starting at $s$ and acting according to the optimal policy.</li><li>The utility of a Q-state $Q^*(s,a)$ is equal to the expected utility of taken action $a$ in state $s$, then acting optimally.</li></ul><p>The Bellman Equation is as follows:</p><p>$$
U^<em>(s) = \max_a \sum_{s&rsquo;} T(s, a, s&rsquo;)\times(R(s, a,s&rsquo;) + \gamma U^</em>(s&rsquo;)) = \max_a Q^*(s, a)
$$</p><ul><li>The equation finds the optimal value of the state $s$ by multiplying the transition probability to the next state by the reward for that transition plus the discounted utility of the next state.</li><li>The inner sum term is equivalent to the utility of the Q-state.</li><li>The equation creates a dynamic programming problem where the subproblem $U^*(s’)$ is used to calculate the current state’s utility.</li></ul><h2 id=value-iteration>Value Iteration
<a class=anchor href=#value-iteration>#</a></h2><p>Used for computing the optimal values of states, by iterative updates until convergence.</p><p>In order to compute the value of a state. we can use the following algorithm:</p><ol><li>For all states $s \in S$, initialize their starting value $U_0(s) = 0$.</li><li>Until convergence (i.e. $U_{k+1} = U_k$), run the following update formula:<ol><li>$\forall s \in S, U_{k+1}(s) = \max_a \sum_{s’} T(s, a, s’)\times (R(s, a, s’) + \gamma U_k(s’))$</li><li>Unlike the Bellman equation, which tests for optimality, this equation changes the value iteratively using dynamic programming.</li><li>In other words, this equation takes the max of the values of alll neighboring states, and multiplies it by the transition probability (if nondeterministic).</li><li>$U^*(s)$ for any terminal state must be $0$ (since no actions can be taken from these states).</li></ol></li></ol><p>Properties:</p><ul><li>Value iteration is guaranteed to converge for discounts less than 1.</li><li>Value iterations will converge to the same $U^*$ values regardless of initial values.</li><li>The runtime of value iteration is $O(|S|^2 |A|)$ since for every action, we need to compute each action’s Q-value which requires iterating over all states.</li></ul><p><img src="/data102/img/Pasted image 20221115161107.png" width=auto></p><p><strong>Q-value iteration</strong> is a similar update algorithm that computes Q-values instead:</p><p>$$
Q_{k+1}(s,a) = \sum_{s&rsquo;} T(s, a, s&rsquo;) \times (R(s, a, s&rsquo;) + \gamma \max_{a&rsquo;} Q_k (s&rsquo;, a&rsquo;))
$$</p><ul><li>This is different because for value iteration, we select an action before transitioning, whereas in Q-value iteration, we make the transition before choosing the new state.</li></ul><p><strong>Policy extraction:</strong> to determine the optimal policy from optimal state values (given a state value function), we can use the following equation:</p><p>$$ \forall s \in S, \pi^<em>(s) = \argmax_a Q^</em>(s, a) = \argmax_a \sum_{s&rsquo;} T(s, a, s&rsquo;) [R(s, a, s&rsquo;) + \gamma U^*(s&rsquo;)]$$</p><p>Policy extraction is only optimal if the state value function is optimal. Used either after value iteration (to compute optimal policies from optimal state values) or as a subroutine in policy iteration (to compute the best policy for estimated state values).</p><h2 id=policy-iteration>Policy Iteration
<a class=anchor href=#policy-iteration>#</a></h2><p>Policy iteration has the same optimality guarantees as value iteration, but has better performance. The algorithm is as follows:</p><ol><li>Define an initial policy. The closer the initial policy is to the optimal policy, the faster it will converge.</li><li>Compute $U^{\pi}(s) = \sum_{s’} T(s, \pi(s), s’) [R(s, \pi(s), s’) + \gamma U^{\pi}(s’)]$ for all states $s$ (the expected utility of starting in state $s$ when following the current policy $\pi$).</li><li>Generate the next iteration $i$ of policy values using the equation above for every state.</li><li>Use policy improvement to generate a better policy: $\pi_{i+1}(s) = \argmax_a \sum_{s&rsquo;} T(s, a, s&rsquo;)[R(s, a, s&rsquo;) + \gamma U^{\pi_i}(s&rsquo;)]$</li><li>Repeat steps 2-5 until $\pi_{i+1} = \pi_i = \pi^*$.</li></ol><hr><div class=page-end id=footer><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li>No backlinks found</li></ul></div><div><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://notes.bencuan.me/quartz/js/graph.cbd78cfa87df7d3e230d16fc24f06548.js></script></div></div><div id=contact_buttons><footer><p>Made with <a href=https://github.com/64bitpandas/amethyst>Amethyst</a>, © 2023 Ben Cuan</p><ul><li><a href=https://notes.bencuan.me/>Home</a></li><li><a href=https://github.com/64bitpandas/notes/issues>Feedback</a></li><li><a href=https://bencuan.me>Website</a></li></ul></footer></div></article><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><ul><li><a href=#what-is-a-markov-decision-process>What is a Markov Decision Process?</a></li></ul></li><li><a href=#solving-mdps>Solving MDPs</a><ul><li><a href=#the-bellman-equation>The Bellman Equation</a></li><li><a href=#value-iteration>Value Iteration</a></li><li><a href=#policy-iteration>Policy Iteration</a></li></ul></li></ul></nav></div></aside></main></body></html>